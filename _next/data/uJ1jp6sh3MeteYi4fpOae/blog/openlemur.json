{"pageProps":{"post":{"slug":"openlemur","content":"\nTLDR: üéâ Introducing Lemur-70B & Lemur-70B-Chat: üöÄOpen & SOTA Foundation Models for Language Agents! The closest open model to GPT-3.5 on ü§ñ15 agent tasksü§ñ!\n\nüìÑPaper: [http://arxiv.org/abs/2310.06830](http://arxiv.org/abs/2310.06830)\n\nü§óModel: [http://huggingface.co/OpenLemur](http://huggingface.co/OpenLemur)\n\nüë©‚ÄçüíªCode: [https://github.com/OpenLemur/Lemur](https://github.com/OpenLemur/Lemur)\n\n<hr class=\"solid\">\n\nWe are excited to announce Lemur, an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. Our preprint [Lemur: Harmonizing Natural Language and Code for Language Agents](https://arxiv.org/abs/2310.06830) shares details on this new model.\n\nAs language models continue to evolve from conversational chatbots to functional agents that can act in the real world, they need both strong language understanding and the ability to execute actions. Lemur balances natural language and coding skills to enable agents to follow instructions, reason about tasks, and take grounded actions.\n\nPlease find more detailed information here:\n\n- Paper: [Lemur: Harmonizing Natural Language and Code for Language Agents](https://arxiv.org/abs/2310.06830)\n- Code: [OpenLemur Repository ](https://github.com/OpenLemur/Lemur)\n- Model: [HuggingFace Hub](https://huggingface.co/OpenLemur)\n\n## Why Lemur?\nMost existing open source models specialize in either natural language or code. Lemur combines both strengths by:\n\n- Pretraining on a 90B token corpus with 10:1 ratio of code to text\n- Instruction tuning on 300K examples covering both text and code\n\nThis two-stage training produces state-of-the-art performance averaged across diverse language and coding benchmarks, surpassing other available open source models and narrowing the gap between open-source and commercial models on agent abilities.\n\n<figure style=\"text-align: center;\">  \n  <img src=\"/blog/lemur/pipeline.png\" height=20>  \n  <figcaption style=\"text-align: center;\">Training Procedure of Lemur Models</figcaption>  \n</figure>  \n\n## Testing Lemur's Abilities\nWe evaluated Lemur across:\n- 8 language and code datasets like MMLU, BBH, GSM8K, HumanEval, and Spider to validate balanced capabilities\n- 13 interactive agent datasets to test skills like tool usage, adapting to feedback from environment or human, and exploring partially observable digital or physical environments.\n\n<figure style=\"text-align: center;\">  \n  <img src=\"/blog/lemur/agent-skills.png\" height=20>  \n  <figcaption style=\"text-align: center;\"></figcaption>  \n</figure>  \n\n\nLemur significantly outperformed other models on agent benchmarks, showcasing its versatility and potential as a foundation for capable agents.\n\n<figure style=\"text-align: center;\">  \n  <img src=\"/blog/lemur/overall-performance.png\" height=20>  \n  <figcaption style=\"text-align: center;\">Comparison of the foundational and agent capabilities between Lemur and other models.</figcaption>  \n</figure>  \n\n<br>\n\n## Moving Research Forward\nBy open sourcing Lemur and our training corpora, we hope to empower more research into developing capable and controllable agents that can understand instructions and act appropriately. Testing Lemur across diverse agent scenarios gives insights into critical areas like:\n\n- Harmonizing natural language and programming abilities\n- Tool usage as an augmentation mechanism\n- Adapting behavior based on environment and human feedback\n- Reasoning and planning under partial observability in digital and physical environments\n\nThere is still much work to be done, but Lemur represents an important step towards open source models that can power the next generation of language agents. We look forward to seeing what the community builds!\n\nYou can find more details in our preprint: [Lemur: Harmonizing Natural Language and Code for Language Agents](https://arxiv.org/abs/2310.06830)\n\n## Acknowledgements\n\nThe Lemur project is a open collaborative research effort between [XLang Lab](https://xlang.ai) and [Salesforce Research](https://www.salesforceairesearch.com/). We would like to thank Salesforce Research, Google Research, and Amazon AWS for their gift support to this open-source effort!\n","title":"Introducing Lemur: Open Foundation Models for Language Agents","shortTitle":"Lemur Intro","date":"08 October 2023","author":"XLANG Lab","coverImage":"/blog/lemur/overview.jpeg","previewContent":"We are excited to announce Lemur, an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents.","onlineImage":"https://i.imgur.com/m4aI7hW.jpg","twitterLink":"https://twitter.com/taoyds/status/1712547761252610076","githubLink":"https://github.com/OpenLemur/Lemur"}},"__N_SSG":true}