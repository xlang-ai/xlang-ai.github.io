<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>XLANG Lab | Blogs</title><link rel="apple-touch-icon" sizes="180x180" href="/favicon/black-on-white/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/black-on-white/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/black-on-white/favicon-16x16.png"/><link rel="manifest" href="/favicon/black-on-white/site.webmanifest"/><meta name="next-head-count" content="7"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/d6f692ad1e46a8a8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d6f692ad1e46a8a8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-4d525326a36e0ad2.js" defer=""></script><script src="/_next/static/chunks/pages/_app-43f67b4afd5f2ce7.js" defer=""></script><script src="/_next/static/chunks/pages/blog-0d3d493f191c6fa2.js" defer=""></script><script src="/_next/static/f537HbUFpGIXc0lJ81QQQ/_buildManifest.js" defer=""></script><script src="/_next/static/f537HbUFpGIXc0lJ81QQQ/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&display=swap">@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtr6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtZ6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCu170w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCuM70w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next"><div class="fixed top-0 left-0 w-full h-14 md:h-20 bg-white py-4 z-10 navbar-shadow"><div class="page-x-width w-full flex justify-between items-center"><div class="sm:hidden w-fit h-fit cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" class="text-[#0156AC]" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><line x1="4" y1="6" x2="20" y2="6"></line><line x1="4" y1="12" x2="20" y2="12"></line><line x1="4" y1="18" x2="20" y2="18"></line></svg></div><a href="/"><div class="flex gap-2 items-center cursor-pointer text-brand-dark"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/logo.svg"/><div>XLANG Lab</div></div></a><ul class="gap-8 text-md text-text-brand-dark hidden sm:flex"><li class="font-[500] hover:underline text-brand-dark"><a href="/">about</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/team">team</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/publications">publications</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/blog">blogs</a></li></ul><div class="flex gap-4 items-center"><ul class="hidden lg:flex gap-3"><li><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github-black.svg"/></a></li><li><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter-black.svg"/></a></li></ul><div class="max-sm:text-sm border border-brand-primary2 border-2 text-brand-primary2 font-[500] rounded-xl py-1 px-3 cursor-pointer"><a href="https://forms.gle/3Ki9ectMB5D31F8g8" target="_blank" rel="noopener noreferrer">join us</a></div></div></div></div><div class="relative w-full h-full"><div class="absolute top-0 left-0 max-sm:hidden -mt-8 z-[-1]"><img alt="Wave" loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/background/wave.svg"/><img alt="Wave 2" loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/background/wave2.svg"/></div><div class="max-sm:py-20 pt-36 w-full bg-[#D9D9D9]/20 min-h-[95vh]"><div class="page-x-width pb-10"><h1 class="text-2xl font-[500] mb-4">Blog</h1><div class="flex flex-col gap-4"><div class="w-full grid grid-cols-5 gap-4"><div class="max-sm:pb-4 col-span-5 sm:col-span-3 flex flex-col max-sm:gap-2 gap-6 justify-center border-b border-black/30"><a class="col-span-2 relative w-full rounded-lg overflow-hidden sm:hidden h-[300px] " href="/blog/openlemur"><img alt="Introducing Lemur: Open Foundation Models for Language Agents" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center;color:transparent" src="/blog/lemur/overview.jpeg"/></a><div class="text-xs text-[#666666] font-bold max-sm:hidden">Oct 8, 2023</div><a class="text-[#0156AC] text-lg font-[500] tracking-wide leading-5 my-0 hover:underline" href="/blog/openlemur">Introducing Lemur: Open Foundation Models for Language Agents</a><div class="text-xs text-[#666666] font-bold sm:hidden">Oct 8, 2023</div><p class="text-sm tracking-wide leading-5 my-0 overflow-hidden h-36">We are excited to announce Lemur, an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents.</p></div><a class="col-span-2 relative w-full rounded-lg overflow-hidden max-sm:hidden h-full " href="/blog/openlemur"><img alt="Introducing Lemur: Open Foundation Models for Language Agents" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center;color:transparent" src="/blog/lemur/overview.jpeg"/></a></div><div class="w-full grid grid-cols-5 gap-4"><div class="max-sm:pb-4 col-span-5 sm:col-span-3 flex flex-col max-sm:gap-2 gap-6 justify-center border-b border-black/30"><a class="col-span-2 relative w-full rounded-lg overflow-hidden sm:hidden h-[300px] " href="/blog/xlang-intro"><img alt="Introducing XLang: An Open-Source Framework for Building Language Model Agents via Executable Language Grounding" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center;color:transparent" src="/blog/xlang_overview.png"/></a><div class="text-xs text-[#666666] font-bold max-sm:hidden">Aug 10, 2023</div><a class="text-[#0156AC] text-lg font-[500] tracking-wide leading-5 my-0 hover:underline" href="/blog/xlang-intro">Introducing XLang: An Open-Source Framework for Building Language Model Agents via Executable Language Grounding</a><div class="text-xs text-[#666666] font-bold sm:hidden">Aug 10, 2023</div><p class="text-sm tracking-wide leading-5 my-0 overflow-hidden h-36">Introducing XLang, an open-source platform that constructs language model agents through executable language grounding. Alongside this framework, we unveil demos of XLang Agents, encompassing Data, Plugins, and Web agents. Moving forward, we&#x27;re set to open-source multiple substantial projects, encompassing frameworks, models, demos, code, benchmarks, and beyond.</p></div><a class="col-span-2 relative w-full rounded-lg overflow-hidden max-sm:hidden h-full " href="/blog/xlang-intro"><img alt="Introducing XLang: An Open-Source Framework for Building Language Model Agents via Executable Language Grounding" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center;color:transparent" src="/blog/xlang_overview.png"/></a></div></div></div></div></div><div class="w-full bg-brand-offBlack p-4"><div class="page-x-width flex justify-center sm:justify-between flex-wrap gap-4"><div class="flex gap-2 items-center"><a href="/"><div class="relative"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/logo-white.svg"/></div></a><div class="text-white text-xs w-fit">¬© Copyright 2023 XLANG Lab. All right reserved.</div></div><nav><ul class="text-white flex gap-6"><li class="cursor-pointer"><a href="https://discord.com/invite/4Gnw7eTEZR"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/discord.svg"/></a></li><li class="cursor-pointer"><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github.svg"/></a></li><li class="cursor-pointer"><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter.svg"/></a></li></ul></nav></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"slug":"openlemur","content":"\nTLDR: üéâ Introducing Lemur-70B \u0026 Lemur-70B-Chat: üöÄOpen \u0026 SOTA Foundation Models for Language Agents! The closest open model to GPT-3.5 on ü§ñ15 agent tasksü§ñ!\n\nüìÑPaper: [http://arxiv.org/abs/2310.06830](http://arxiv.org/abs/2310.06830)\n\nü§óModel: [http://huggingface.co/OpenLemur](http://huggingface.co/OpenLemur)\n\nüë©‚ÄçüíªCode: [https://github.com/OpenLemur/Lemur](https://github.com/OpenLemur/Lemur)\n\n\u003chr class=\"solid\"\u003e\n\nWe are excited to announce Lemur, an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. Our preprint [Lemur: Harmonizing Natural Language and Code for Language Agents](https://arxiv.org/abs/2310.06830) shares details on this new model.\n\nAs language models continue to evolve from conversational chatbots to functional agents that can act in the real world, they need both strong language understanding and the ability to execute actions. Lemur balances natural language and coding skills to enable agents to follow instructions, reason about tasks, and take grounded actions.\n\nPlease find more detailed information here:\n\n- Paper: [Lemur: Harmonizing Natural Language and Code for Language Agents](https://arxiv.org/abs/2310.06830)\n- Code: [OpenLemur Repository ](https://github.com/OpenLemur/Lemur)\n- Model: [HuggingFace Hub](https://huggingface.co/OpenLemur)\n\n## Why Lemur?\nMost existing open source models specialize in either natural language or code. Lemur combines both strengths by:\n\n- Pretraining on a 90B token corpus with 10:1 ratio of code to text\n- Instruction tuning on 300K examples covering both text and code\n\nThis two-stage training produces state-of-the-art performance averaged across diverse language and coding benchmarks, surpassing other available open source models and narrowing the gap between open-source and commercial models on agent abilities.\n\n\u003cfigure style=\"text-align: center;\"\u003e  \n  \u003cimg src=\"/blog/lemur/pipeline.png\" height=20\u003e  \n  \u003cfigcaption style=\"text-align: center;\"\u003eTraining Procedure of Lemur Models\u003c/figcaption\u003e  \n\u003c/figure\u003e  \n\n## Testing Lemur's Abilities\nWe evaluated Lemur across:\n- 8 language and code datasets like MMLU, BBH, GSM8K, HumanEval, and Spider to validate balanced capabilities\n- 13 interactive agent datasets to test skills like tool usage, adapting to feedback from environment or human, and exploring partially observable digital or physical environments.\n\n\u003cfigure style=\"text-align: center;\"\u003e  \n  \u003cimg src=\"/blog/lemur/agent-skills.png\" height=20\u003e  \n  \u003cfigcaption style=\"text-align: center;\"\u003e\u003c/figcaption\u003e  \n\u003c/figure\u003e  \n\n\nLemur significantly outperformed other models on agent benchmarks, showcasing its versatility and potential as a foundation for capable agents.\n\n\u003cfigure style=\"text-align: center;\"\u003e  \n  \u003cimg src=\"/blog/lemur/overall-performance.png\" height=20\u003e  \n  \u003cfigcaption style=\"text-align: center;\"\u003eComparison of the foundational and agent capabilities between Lemur and other models.\u003c/figcaption\u003e  \n\u003c/figure\u003e  \n\n\u003cbr\u003e\n\n## Moving Research Forward\nBy open sourcing Lemur and our training corpora, we hope to empower more research into developing capable and controllable agents that can understand instructions and act appropriately. Testing Lemur across diverse agent scenarios gives insights into critical areas like:\n\n- Harmonizing natural language and programming abilities\n- Tool usage as an augmentation mechanism\n- Adapting behavior based on environment and human feedback\n- Reasoning and planning under partial observability in digital and physical environments\n\nThere is still much work to be done, but Lemur represents an important step towards open source models that can power the next generation of language agents. We look forward to seeing what the community builds!\n\nYou can find more details in our preprint: [Lemur: Harmonizing Natural Language and Code for Language Agents](https://arxiv.org/abs/2310.06830)\n\n## Acknowledgements\n\nThe Lemur project is a open collaborative research effort between [XLang Lab](https://xlang.ai) and [Salesforce Research](https://www.salesforceairesearch.com/). We would like to thank Salesforce Research, Google Research, and Amazon AWS for their gift support to this open-source effort!\n","title":"Introducing Lemur: Open Foundation Models for Language Agents","shortTitle":"Lemur Intro","date":"08 October 2023","author":"XLANG Lab","coverImage":"/blog/lemur/overview.jpeg","previewContent":"We are excited to announce Lemur, an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents.","onlineImage":"https://i.imgur.com/m4aI7hW.jpg","twitterLink":"https://twitter.com/taoyds/status/1712547761252610076","githubLink":"https://github.com/OpenLemur/Lemur"},{"slug":"xlang-intro","content":"\n\n\u003e *\"Many years later, as he faced the firing squad, Colonel Aureliano Buend√≠a was to remember that distant afternoon when his father took him to discover ice.\" ‚Äî‚Äî One Hundred Years of Solitude, Gabriel Garcia M√°rquez.*\n\u003e\n---\n\nWhen envisioning the future, people have always imagined an intelligent agent capable of following human commands and performing specific tasks, significantly enhancing productivity. In recent times, with the emergence of powerful language models, this vision is accelerating. We're getting a fresh perspective on what language model agents can truly achieve. The language model agent represents a crucial step in this burgeoning field, enabling powerful language models to take actions, use tools, execute various tasks and facilitate more intelligent interactions with humans. \n\nWith great excitement, our team, **XLang Lab**, introduces our recent efforts to exploring and advancing the realm of language model agents. In the upcoming sections, we will elaborate on our mission, workouts, and the outlook for future endeavors. Through our mission, we aspire to humbly provide insightful perspectives on the historical trajectory of this captivating vision, pushing forward the frontiers of what these agents can achieve. With this enthusiasm and determination, let us together embrace a new era of intelligent agents for the future.\n\n## **What is XLang?**\n\n**Executable Language Grounding (XLang)** refers to the process of converting natural language instructions into ***code or action sequences executable*** in some environments. It involves generating code or actions that can interact with the environment, perform specific operations, and produce tangible results. XLang thus serves as a bridge, transforming natural language instructions into code or actions executable within real-world environments. Such environments include but are not limited to databases, web applications, and the physical world navigated by robots.\n\n\u003e *\"Code never lies, comments sometimes do.\" ‚Äî‚Äî Ron Jeffries*\n\u003e\n\nImagine the process as the transmutation of human instructions and questions ‚Äî expressed in everyday language ‚Äî into machine-understandable actions and code. The machine then executes these within a specific environment, leading to changes in the state of that environment. This change is observed, results are analyzed, and a further cycle of interaction with humans is initiated.\n\nThis process expands the capabilities of the agents **far beyond those of a conventional chatbot**, allowing it to address and serve a much broader scope of tasks and applications. And we believe such process lies at the heart of AI/language agents that interact with various real-world environments via natural language and accomplish tangible tasks for us. Recent advances in Executable Language Grounding incorporate techniques such as LLM (Large Language Models) enhanced with neuro-symbolic tools, code generation or semantic parsing, and dialog or interactive systems.\n\nIn the current generation of LLMs, we can furnish our models with an assortment of neuro-symbolic tools to enhance their capabilities. Typical inputs to these models could include language from the user, a toolkit brimming with different tools, and a variety of environments. The outcome is an action or code sequence executable within the corresponding environment, often entailing the use of certain tool APIs.\n\nThe process of building such agents often demands considerable effort and collaboration from a dedicated team. This is where XLang comes into play. Our objective with XLang is to establish an open-source framework and ecosystem for building and evaluating these powerful LLM-powered agents.\n\n## **The Motivations and Challenges behind XLang**\n\nOur team's initial spark of excitement came when OpenAI announced the release of code interpreters and plugins. These tools represented live demonstrations of many years of our research in areas such as code generation, semantic parsing, natural language interfaces, and interactive dialog systems. Unfortunately, we couldn't access these tools to experiment with our ideas for improvements. This limitation prompted us to think ambitiously. What if we developed our own open-source code interpreters and plugins, or even a more general agent system and framework? This would not only benefit our team but also other research labs and companies worldwide. By sharing our work, we believe we can contribute to the growth of research and applications in this direction, allowing more people to perform exciting and interesting work on our open-source system. More specifically, by shifting the focus towards interactive and real-time demos, we can:\n\n- iteratively add and improve the agent‚Äôs design and working logics, such as integrating more useful tools\n- implement robust evaluation procedures for various LLMs (Large Language Models) in a neutral manner; while platforms like Vicuna Arena (chatbots) have served as valuable pioneers in this area, we strive to uncover evaluation metrics tailored specifically for language model agents (chatbots + grounding).\n- push forward the agentic model‚Äôs training and development; by incorporating comprehensive evaluations and continuously iterating the training process, we aim to uncover the shortcomings of LLMs and make iterative improvements.\n\nIn short, by setting the frameworks including a real-time demo, we can further advance the research and development of LLM-powered agents and demonstrate their potential.\n\nAll such promising directions requires a critical first step: establishing a unified system that real users can access. However, challenges are many. Setting up agent demos is not as straightforward as simply connecting a chatbox to an LLM. It requires considerable effort, both in terms of engineering and research. The agents need to interact with their corresponding environments and effect changes in those environments. The system must robustly handle all sorts of situations, appropriately manage different possible execution results, and present these outcomes in the correct manner. Ensuring the robustness and scalability of this interaction cycle is a significant challenge.\n\nHistorically, NLP has lacked practical system demonstrations like those found in robotics or databases, and has instead focused more on testing against static benchmarks. However, with the advent of large models, we believe the time has come to bridge this gap. Similar to the MineDojo framework in embodied AI and the ManiSkill for robotics, setting up such an agent framework necessitates long-term cooperation among many individuals. Our goal is to see the fruits of our research move step by step towards real-world applications that will, in the not-so-distant future, be used by millions.\n\nOur XLang team of about 15 researchers and developers from various backgrounds including **NLP** (Natural Language Processing), **ML** (Machine Learning), **HCI** (Human Computer Interaction), **VIS** (Visualization), **DB** (Database), **Full-stack** development, **UI design**, and **Robotics** have been working full-time on this project since the end of March. We've invested significant effort in addressing these challenges and minimizing the gap between research and the development of real-world interactive agents. We firmly believe in the value of our work and hope our open-source project will attract more researchers, developers, and designers to contribute to this exciting direction.\n\n## **Why Us? Our Journey**\n\nThe answer to this question is quite straightforward. Our team is deeply interested in this field, and we've always wished for agents that can help people analyze data without coding, and for more natural language-led interaction modes for webs/apps. This was the primary reason why, in March, our team decided within four days of discussing the concept, to congregate in WeWork Shenzhen and commence full-time work on the project. This dedication has continued for more than four months, and we are committed to the long-term development of this project.\n\nMany of our team members have been consistently drawn to research problems in this direction. Our team comprises professionals with backgrounds in HCI, DB, NLP, visualization, and ML. We have all conducted extensive work on executable language grounding for building LLM-powered AI agents, specifically natural language interfaces for data in databases and webs/apps. \n\nThroughout this process, we have maintained active collaborations with industry players like Salesforce, Microsoft, Amazon, Facebook, Google, especially in the realm of text-to-SQL and code generation. We were also among the pioneers in working with large language models for in-context learning ([**Selective Annotation**](https://github.com/HKUNLP/icl-selective-annotation)), LLM + tool use ([**Binder**](https://lm-code-binder.github.io/)), instruction tuning and retrieval embeddings for LLM ([**Instructor Embeddings**](https://instructor-embedding.github.io/)), code generation and semantic parsing ([**Spider**](https://yale-lily.github.io/spider), [**DS-1000**](https://ds1000-code-gen.github.io/),¬†[**Coder-Reviewer Reranking**](https://arxiv.org/abs/2211.16490), [**UnifiedSKG**](https://github.com/HKUNLP/UnifiedSKG)),and interactive dialog systems ([**ICL-DST**](https://github.com/Yushi-Hu/IC-DST), [**SParC**](https://yale-lily.github.io/sparc),¬†[**CoSQL**](https://yale-lily.github.io/cosql),¬†[**NL2Interface**](https://arxiv.org/abs/2209.08834)). More about our research can be found on our [project page](https://www.xlang.ai/project). \n\nWe are a dedicated research team profoundly invested and interested in XLang and language model agents, particularly those related to data and web/app agents. More XLang, code generation, LLM+tool use, and LLM+robotics paper collections can be found in our ACL tutorial on complex reasoning: [LLM+tool use](https://github.com/xlang-ai/xlang-paper-reading).\n\n## **XLang Agents: An Introduction**\n\nXLang Agents are language model agents developed by our team, aiming to utilize a range of tools to enhance their capabilities, serving as user-centric intelligent agents. Currently the XLang Agents supports three different agents focusing on different application scenarios, including:\n\n- **Data Agent**: This agent is skilled in data tools, allowing efficient data search, manipulation, and visualization. It excels in code execution for data-centric tasks.\n- **Plugins Agent**: With over 200 third-party plugins, this agent addresses diverse daily life needs, aiding in various tasks.\n- **Web Agent**: Utilizing a Chrome extension, this agent automates web navigation, streamlining browsing to find and access information.\n- **Robotic Agent**: comming soon\n    \nüí° We have make all three agents online, just visit üëâ[**XLang Agents**](https://chat.xlang.ai) and feel free to explore! For more details about XLang Agents, you can also check the official documents in üëâ[**XLang Docs**](https://docs.xlang.ai) !\n\n**Here are some interesting things XLang Agents can do!**\n\n---\n\n### Data Agent\n\n**Code generation + Data tools = Data Agent!**\n\nAfter selecting certain data tools, the agent can take your request and proactively take actions to fulfill your request. \n\nIn the following example, you will see how data agent help you search a dataset, draw an interactive line plot, and finally construct an ARIMA model to perform some prediction.\n\n\u003c!-- [https://www.youtube.com/watch?v=JabK4PiJJqs](https://www.youtube.com/watch?v=JabK4PiJJqs) --\u003e\n\u003ciframe src=\"https://www.youtube.com/embed/JabK4PiJJqs\" title=\"Data Agent Overview\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\n\n---\n\n### Plugins Agent\n\n**Unleash the power of hundreds of real-world applications through our intelligent Plugins system!**\n\nThe Agent uses a provided API YAML to intelligently determines the optimal timing and selection of plugins to invoke. Each plugin has been thoughtfully curated to fulfill various requirements across your everyday life situations.\n\nFor instance, when traveling to Toronto, it recommends attractions, handles currency conversion, provides weather updates, and suggests clothing, ensuring a hassle-free journey.\n\n\u003ciframe src=\"https://www.youtube.com/embed/UL7VEAQHYBE\" title=\"Plugins Agent Overview\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\n\n---\n\n### Web Agent\n\n**Effortlessly navigate the internet with the Web Agent, powering up your browsing experience.**\n\nThe Web Agent, utilizing a Chrome extension, automates website navigation to streamline browsing and enhance information retrieval. It simplifies the user's quest for pertinent details and desired resources. \n\nSpecifically in the following example, the agent extracts movie reviews from IMDb and assists in posting a thread on Twitter. Additionally, our interface facilitates multi-turn interactions, ensuring efficient task completion and enriched user engagement.\n\n\u003ciframe src=\"https://www.youtube.com/embed/yH31TXBfrKI\" title=\"Web Agent Overview\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\n\n---\n\nBy harnessing the power of large language models in conjunction with diverse tools, XLang Agents significantly expand the capabilities of conversational interfaces, offering intelligent assistance that revolves around the user. \n\n**Engage in a conversation with our XLang Agents to explore its wide-ranging capabilities further!**\n\n## **What's Next? The Future**\n\nOur aim is to build XLang, an open-source ecosystem and community for building and evaluating language model agents. This release will just be the beginning of our XLang open-source journey. In the following months, and beyond, we will be open-sourcing several significant projects, which will include all frameworks, models, demos, code, benchmarks, and more. We hope that in these particular times in NLP, we can enable more people, rather than just a few large companies or closed start-ups, to participate. We envision these initiatives as the starting point to establish a vivid LLM-powered agents, LLM + tool use, and language grounding community, encouraging more people to contribute, develop and perform exciting research based on our work.\n\n- Online demos of XLang Agents\n- Framework or toolkits, more sophisticated LangChain for building and evaluating langauge model agents\n- Agent demo frontend and backend repos for HCI/VIS + NLP research and developers\n- Pretraining actionable and agentic large language models (donation supports welcome!)\n- SOTA methods for code generation, general LLMs, and LLMs + tool use for building language model agents\n- ‚Ä¶‚Ä¶\n\n## **Acknowledgements**\n\nWe would like to express our gratitude towards Google Research, Amazon AWS, and Salesforce Research. The gift funds and necessary computational resources generously provided by these awards have given us the capability and resources to implement this project. We also appreciate the invaluable advice we received throughout the process.\n\n**Personal Acknowledgements by [Tao](https://taoyds.github.io/):**\n\nI feel fortunate for the year I spent at UWNLP, which is one of the world's top institutions for NLP research. During this time, I observed the nascent shift towards LLM in NLP. I would like to extend my thanks to Noah Smith, Luke Zettlemoyer, and Mari Ostendorf. The idea of XLang came about from a suggestion Luke made during a meeting in his office.\n\nI would also like to pay tribute to my late Ph.D. advisor, Dragomir Radev. Without him, it's very possible that none of what we are starting today would exist.\n","title":"Introducing XLang: An Open-Source Framework for Building Language Model Agents via Executable Language Grounding","shortTitle":"XLANG Intro","date":"10 August 2023","author":"XLANG Lab","coverImage":"/blog/xlang_overview.png","previewContent":"Introducing XLang, an open-source platform that constructs language model agents through executable language grounding. Alongside this framework, we unveil demos of XLang Agents, encompassing Data, Plugins, and Web agents. Moving forward, we're set to open-source multiple substantial projects, encompassing frameworks, models, demos, code, benchmarks, and beyond.","onlineImage":"https://i.imgur.com/sPqu2t7.png","twitterLink":"https://twitter.com/XLangNLP/status/1689723514134446081"}]},"__N_SSG":true},"page":"/blog","query":{},"buildId":"f537HbUFpGIXc0lJ81QQQ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>