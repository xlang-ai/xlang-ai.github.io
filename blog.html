<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>XLANG Lab | Blogs</title><link rel="apple-touch-icon" sizes="180x180" href="/favicon/black-on-white/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/black-on-white/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/black-on-white/favicon-16x16.png"/><link rel="manifest" href="/favicon/black-on-white/site.webmanifest"/><meta name="next-head-count" content="7"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/c7a1138c526c7fb2.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c7a1138c526c7fb2.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-4d525326a36e0ad2.js" defer=""></script><script src="/_next/static/chunks/pages/_app-43f67b4afd5f2ce7.js" defer=""></script><script src="/_next/static/chunks/pages/blog-0d3d493f191c6fa2.js" defer=""></script><script src="/_next/static/d5O8OIQN-5BBLJ4NpJJOB/_buildManifest.js" defer=""></script><script src="/_next/static/d5O8OIQN-5BBLJ4NpJJOB/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&display=swap">@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtr6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtZ6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCu170w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCuM70w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next"><div class="fixed top-0 left-0 w-full h-14 md:h-20 bg-white py-4 z-10 navbar-shadow"><div class="page-x-width w-full flex justify-between items-center"><div class="sm:hidden w-fit h-fit cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" class="text-[#0156AC]" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><line x1="4" y1="6" x2="20" y2="6"></line><line x1="4" y1="12" x2="20" y2="12"></line><line x1="4" y1="18" x2="20" y2="18"></line></svg></div><a href="/"><div class="flex gap-2 items-center cursor-pointer text-brand-dark"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/logo.svg"/><div>XLANG Lab</div></div></a><ul class="gap-8 text-md text-text-brand-dark hidden sm:flex"><li class="font-[500] hover:underline text-brand-dark"><a href="/">about</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/team">team</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/publications">publications</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/blog">blogs</a></li></ul><div class="flex gap-4 items-center"><ul class="hidden lg:flex gap-3"><li><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github-black.svg"/></a></li><li><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter-black.svg"/></a></li></ul><div class="max-sm:text-sm border border-brand-primary2 border-2 text-brand-primary2 font-[500] rounded-xl py-1 px-3 cursor-pointer"><a href="https://forms.gle/3Ki9ectMB5D31F8g8" target="_blank" rel="noopener noreferrer">join us</a></div></div></div></div><div class="relative w-full h-full"><div class="absolute top-0 left-0 max-sm:hidden -mt-8 z-[-1]"><img alt="Wave" loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/background/wave.svg"/><img alt="Wave 2" loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/background/wave2.svg"/></div><div class="max-sm:py-20 pt-36 w-full bg-[#D9D9D9]/20 min-h-[95vh]"><div class="page-x-width pb-10"><h1 class="text-2xl font-[500] mb-4">Blog</h1><div class="flex flex-col gap-4"><div class="w-full grid grid-cols-5 gap-4"><div class="max-sm:pb-4 col-span-5 sm:col-span-3 flex flex-col max-sm:gap-2 gap-6 justify-center border-b border-black/30"><a class="col-span-2 relative w-full rounded-lg overflow-hidden sm:hidden h-[300px] " href="/blog/osworld-verified"><img alt="Introducing OSWorld-Verified" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center;color:transparent" src="/blog/osworld-verified/overview.png"/></a><div class="text-xs text-[#666666] font-bold max-sm:hidden">Jul 28, 2025</div><a class="text-[#0156AC] text-lg font-[500] tracking-wide leading-5 my-0 hover:underline" href="/blog/osworld-verified">Introducing OSWorld-Verified</a><div class="text-xs text-[#666666] font-bold sm:hidden">Jul 28, 2025</div><p class="text-sm tracking-wide leading-5 my-0 overflow-hidden h-36">We&#x27;ve systematically addressed 300+ issues in OSWorld through a comprehensive refinement process. OSWorld-Verified delivers more reliable evaluation signals through improved infrastructure and enhanced task quality, providing the community with a more stable foundation for advancing computer use agent research.</p></div><a class="col-span-2 relative w-full rounded-lg overflow-hidden max-sm:hidden h-full " href="/blog/osworld-verified"><img alt="Introducing OSWorld-Verified" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center;color:transparent" src="/blog/osworld-verified/overview.png"/></a></div><div class="w-full grid grid-cols-5 gap-4"><div class="max-sm:pb-4 col-span-5 sm:col-span-3 flex flex-col max-sm:gap-2 gap-6 justify-center border-b border-black/30"><a class="col-span-2 relative w-full rounded-lg overflow-hidden sm:hidden h-[300px] " href="/blog/openlemur"><img alt="Introducing Lemur: Open Foundation Models for Language Agents" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center;color:transparent" src="/blog/lemur/overview.jpeg"/></a><div class="text-xs text-[#666666] font-bold max-sm:hidden">Oct 8, 2023</div><a class="text-[#0156AC] text-lg font-[500] tracking-wide leading-5 my-0 hover:underline" href="/blog/openlemur">Introducing Lemur: Open Foundation Models for Language Agents</a><div class="text-xs text-[#666666] font-bold sm:hidden">Oct 8, 2023</div><p class="text-sm tracking-wide leading-5 my-0 overflow-hidden h-36">We are excited to announce Lemur, an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents.</p></div><a class="col-span-2 relative w-full rounded-lg overflow-hidden max-sm:hidden h-full " href="/blog/openlemur"><img alt="Introducing Lemur: Open Foundation Models for Language Agents" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center;color:transparent" src="/blog/lemur/overview.jpeg"/></a></div><div class="w-full grid grid-cols-5 gap-4"><div class="max-sm:pb-4 col-span-5 sm:col-span-3 flex flex-col max-sm:gap-2 gap-6 justify-center border-b border-black/30"><a class="col-span-2 relative w-full rounded-lg overflow-hidden sm:hidden h-[300px] " href="/blog/xlang-intro"><img alt="Introducing XLang: An Open-Source Framework for Building Language Model Agents via Executable Language Grounding" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center;color:transparent" src="/blog/xlang_overview.png"/></a><div class="text-xs text-[#666666] font-bold max-sm:hidden">Aug 10, 2023</div><a class="text-[#0156AC] text-lg font-[500] tracking-wide leading-5 my-0 hover:underline" href="/blog/xlang-intro">Introducing XLang: An Open-Source Framework for Building Language Model Agents via Executable Language Grounding</a><div class="text-xs text-[#666666] font-bold sm:hidden">Aug 10, 2023</div><p class="text-sm tracking-wide leading-5 my-0 overflow-hidden h-36">Introducing XLang, an open-source platform that constructs language model agents through executable language grounding. Alongside this framework, we unveil demos of XLang Agents, encompassing Data, Plugins, and Web agents. Moving forward, we&#x27;re set to open-source multiple substantial projects, encompassing frameworks, models, demos, code, benchmarks, and beyond.</p></div><a class="col-span-2 relative w-full rounded-lg overflow-hidden max-sm:hidden h-full " href="/blog/xlang-intro"><img alt="Introducing XLang: An Open-Source Framework for Building Language Model Agents via Executable Language Grounding" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center;color:transparent" src="/blog/xlang_overview.png"/></a></div></div></div></div></div><div class="w-full bg-brand-offBlack p-4"><div class="page-x-width flex justify-center sm:justify-between flex-wrap gap-4"><div class="flex gap-2 items-center"><a href="/"><div class="relative"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/logo-white.svg"/></div></a><div class="text-white text-xs w-fit">© Copyright 2023 XLANG Lab. All right reserved.</div></div><nav><ul class="text-white flex gap-6"><li class="cursor-pointer"><a href="https://discord.com/invite/4Gnw7eTEZR"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/discord.svg"/></a></li><li class="cursor-pointer"><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github.svg"/></a></li><li class="cursor-pointer"><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter.svg"/></a></li></ul></nav></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"slug":"osworld-verified","content":"\n📝Blog: [https://xlang.ai/blog/osworld-verified](https://xlang.ai/blog/osworld-verified)\n\n📊Data: [https://github.com/xlang-ai/OSWorld/tree/main/evaluation_examples](https://github.com/xlang-ai/OSWorld/tree/main/evaluation_examples) (same place as before)\n\n👩‍💻Code: [https://github.com/xlang-ai/OSWorld](https://github.com/xlang-ai/OSWorld) (same place as before)\n\nIn April 2024, OSWorld's initial version [[1]](#ref1) was released for the first controllable environment to benchmark computer use agents. Over the past year and more, we have been delighted and pleasantly surprised to witness the benchmark and environment's driving impact on the community, the emergence of multiple related works, new directions and testing initiatives from tech giants like Anthropic [[2]](#ref2)[[3]](#ref3) and OpenAI [[4]](#ref4), and the birth of new startups [[5]](#ref5)[[6]](#ref6)[[7]](#ref7), applications and possibilities.\n\nThroughout these 15 months, we have continuously invested in supporting features like Docker, parallelization and developing system images for more platforms, while collaborating with the community to address issues on an ongoing basis. \n\nAfter 15 months, we are announcing a major improvement and refinement initiative. We have collected, verified, validated, and fixed 300+ pieces of feedback, involving approximately two months of dedicated effort from a ~10-person team. We are now launching OSWorld-Verified - an enhanced version of OSWorld with comprehensive upgrades and refined examples, providing more authentic signals for evaluation and learning based on this foundation.\n\nWhat follows are our insights from this refinement process, re-evaluation results and current state analysis, a retrospective on computer-use agent evaluation over the past year, and our outlook for the future of CUA evaluation.\n\n**TL;DR**: Major Upgrade! OSWorld has been enhanced and is now OSWorld-Verified with comprehensive improvements. We've systematically addressed 300+ issues in OSWorld through a comprehensive refinement process. OSWorld-Verified delivers more reliable evaluation signals through improved infrastructure (VMware/Docker → AWS, 50x parallelization) and enhanced task quality (fixed web changes, ambiguity, evaluation robustness). The benchmark remains challenging with significant room for improvement toward human-level performance.\n\n**Key Updates**:\n- **Infrastructure**: Migrated to AWS cloud for massive parallelization (10+ hours → minutes)\n- **Task Quality**: Fixed 300+ issues including web structure changes, instruction ambiguity, and evaluation robustness\n- **Evaluation**: Established public evaluation platform for verified apple-to-apple comparisons\n- **Performance**: Current leading models show success primarily stems from extensive human trajectory data\n\n**Impact**: OSWorld-Verified provides the community with a more stable, scalable foundation for advancing computer use agent research and development.\n\n\u003chr class=\"solid\"\u003e\n\n## Acknowledgement\n\nSpecial thanks to the following institutions that provided feedback and participated in the fixes (as well as institutions that provided feedback during the process): [MoonShot AI](https://www.moonshot.ai/)，[Human Data](https://www.hud.so/), [OpenAI](https://openai.com/), [ByteDance Seed TARS](https://seed-tars.com/), [Anthropic](https://www.anthropic.com/), [Simular](https://www.simular.ai/), [HKU Data Intelligence Lab](https://sites.google.com/view/chaoh)\n\nSpecial thanks to the following students who participated in the specific fixes: [Mengqi Yuan](https://yuanmengqi.github.io/), [Danyang Zhang](https://zdy023.github.io/), [Xinzhuang Xiong](https://thisisxxz.com/),  [Zhennan Shen](https://scholar.google.com/citations?user=JPwg5MwAAAAJ\u0026hl=en), [Zilong Zhou](https://github.com/adlsdztony), Yanxu Chen, [Jiaqi Deng](https://millank0817.github.io/), [Tianbao Xie](https://tianbaoxie.com/), Junda Chen, [Jixuan Chen](https://chenjix.github.io/), [Haoyuan Wu](https://www.linkedin.com/in/haoyuan-wu-240878291/).\n\nSpecial thanks to the following students who participated in running the re-evaluation: [Mengqi Yuan](https://yuanmengqi.github.io/), [Zilong Zhou](https://github.com/adlsdztony), [Xinyuan Wang](https://xinyuanwangcs.github.io/), [Bowen Wang](https://bowenbryanwang.github.io/).\n\n\u003chr class=\"solid\"\u003e\n\n## Why an Update?\n\nAlthough OSWorld's early infrastructure using virtual machines and Docker theoretically already provided a completely realistic and almost reproducible environment for decentralized evaluation distribution, and we dedicated over 400 man-hours for four rounds of checks before the first release in April 2024, with continuous maintenance investing hundreds of additional hours to ensure quality, we still collected approximately 300 issues pointed out by several institutions. \n\nThis made us deeply realize a lesson: **providing reliable rewards consumes more human resources than we imagined**. \n\nNext, we'll discuss the uncontrollable factors we discovered in actual operations that potentially cause this benchmark's signal to gradually weaken and become chaotic over time:\n\n### Unexpected Uncontrollable Uncertainty in (Certain) Environments\n\n#### Environment Instability and Changes\n\n- **Anti-crawling mechanisms and CAPTCHAs**: Google search, shopping websites showing \"blocked by the website as robot\", verification challenges\n\n\u003cdiv style=\"display: flex; justify-content: center; align-items: center; gap: 20px; margin: 25px auto; max-width: 800px; flex-wrap: wrap;\"\u003e\n  \u003cfigure style=\"flex: 1; min-width: 300px; max-width: 380px; text-align: center; margin: 0;\"\u003e  \n    \u003cimg src=\"/blog/osworld-verified/access_denied.png\" style=\"max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;\"\u003e  \n    \u003cfigcaption style=\"text-align: center; font-size: 14px; color: #666; margin-top: 8px;\"\u003eAccess Denied - Websites blocking automated agents\u003c/figcaption\u003e  \n  \u003c/figure\u003e\n  \u003cfigure style=\"flex: 1; min-width: 300px; max-width: 380px; text-align: center; margin: 0;\"\u003e  \n    \u003cimg src=\"/blog/osworld-verified/amazon_captcha.png\" style=\"max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;\"\u003e  \n    \u003cfigcaption style=\"text-align: center; font-size: 14px; color: #666; margin-top: 8px;\"\u003eCAPTCHA Challenge - Human verification requirements\u003c/figcaption\u003e  \n  \u003c/figure\u003e\n\u003c/div\u003e\n\n- **Network access restrictions**: 403 IP blocking issues (Steam connection timeout, NBA.com geo-restrictions)\n- **Dynamic content changes**: Website UI overhauls causing DOM structure changes\n  - e.g., Apple comparison page URL parameter changes, Budget.com introducing CAPTCHAs from some point\n  - e.g., Car rental sites (Rentalcars.com) implementing lazy loading and encoding changes\n- **Web page structure modifications**: speedtest.net CSV export exists before while now be deleted, so the task is actually modified\n- **URL and content evolution**: Chrome settings URLs changing, airline sites modifying search interfaces\n\n#### Fragile Dependencies with Timing Issues\n\nMany tasks exhibit complex temporal dependencies where proper initialization requires precise sequential execution. Software applications and web pages often require significant loading and response times, creating timing-sensitive scenarios that can lead to task failures.\n\n\u003cdiv style=\"display: flex; justify-content: center; align-items: center; gap: 20px; margin: 25px auto; max-width: 800px; flex-wrap: wrap;\"\u003e\n  \u003cfigure style=\"flex: 1; min-width: 300px; max-width: 380px; text-align: center; margin: 0;\"\u003e  \n    \u003cimg src=\"/blog/osworld-verified/loading.png\" style=\"max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;\"\u003e  \n    \u003cfigcaption style=\"text-align: center; font-size: 14px; color: #666; margin-top: 8px;\"\u003eLoading Delays - Applications requiring extended initialization time\u003c/figcaption\u003e  \n  \u003c/figure\u003e\n  \u003cfigure style=\"flex: 1; min-width: 300px; max-width: 380px; text-align: center; margin: 0;\"\u003e  \n    \u003cimg src=\"/blog/osworld-verified/stucked_open.png\" style=\"max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;\"\u003e  \n    \u003cfigcaption style=\"text-align: center; font-size: 14px; color: #666; margin-top: 8px;\"\u003eBlocking Operation Dependencies - Ensuring software initialization before action execution\u003c/figcaption\u003e  \n  \u003c/figure\u003e\n\u003c/div\u003e\n\n**Specific Example**: Tasks involving document operations, such as \"Copy the screenshot 1.png from the desktop to where my cursor is located,\" require complex initialization sequences. Our configuration uses `pyautogui.press(\"down\", presses=8, interval=0.01)` to move the cursor to the 9th line, which demands that LibreOffice Writer be fully loaded with the document open and cursor positioned at the first line before executing this command. These fragile dependencies previously couldn't guarantee sequential execution, causing initial setup failures in multiple tasks.\n\n### Incompleteness of Initial Tasks Annotation\n\n#### Task Ambiguity\n\n**Subjective interpretation requirements**:\n- e.g., \"Purple/Red/Green background\" having multiple valid interpretations (light purple, dark purple, etc.)\n- e.g., \"Fill all blank cells\" unclear whether referring to all empty cells or specific ranges\n\n**Unclear scope definitions**:\n- e.g., \"Resize image\" vs \"resize layer\" confusion in GIMP tasks\n- e.g., \"Download some image\" vs \"download some image in GIMP\" - the former is feasible while the latter is infeasible, but some tasks failed to clarify this scope\n- e.g., When a user says 'Switch off Bluetooth' while Bluetooth is already unavailable (meaning the Bluetooth button cannot be turned on), agents can be confused about whether this should be considered a success or deemed infeasible\n\n#### Multiple Solution Approaches\n\n**Alternative valid methods**: Models finding different but correct paths to achieve goals\n- e.g., Task instruction is to concatenate two .csv files without specifying how to handle header rows during concatenation, so we must consider every valid concatenation approach and award full points if any method works.\n\n**Creative problem-solving**: Models using extensions or workarounds not anticipated in ground truth\n- e.g., VS Code background customization via extensions when built-in options are insufficient, while we didn't think about this option and marked the task as infeasible.\n- e.g., Multi-app workflows when single-app solutions are expected, while we didn't think about this option and marked the task as infeasible.\n\n**False negatives from limited ground truth**:\n- e.g., \"Change the first two paragraphs to double line spacing\" - the empty line between the two paragraphs can either be set to double spacing or left unchanged; both approaches should be considered correct.\n\n\u003cdiv style=\"display: flex; justify-content: center; align-items: center; gap: 20px; margin: 25px auto; max-width: 800px; flex-wrap: wrap;\"\u003e\n  \u003cfigure style=\"flex: 1; min-width: 300px; max-width: 380px; text-align: center; margin: 0;\"\u003e  \n    \u003cimg src=\"/blog/osworld-verified/double_linespace_line_by_line.png\" style=\"max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;\"\u003e  \n    \u003cfigcaption style=\"text-align: center; font-size: 14px; color: #666; margin-top: 8px;\"\u003eMethod 1: Sequential paragraph selection - selecting paragraphs individually\u003c/figcaption\u003e  \n  \u003c/figure\u003e\n  \u003cfigure style=\"flex: 1; min-width: 300px; max-width: 380px; text-align: center; margin: 0;\"\u003e  \n    \u003cimg src=\"/blog/osworld-verified/double_linespace_all_together.png\" style=\"max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;\"\u003e  \n    \u003cfigcaption style=\"text-align: center; font-size: 14px; color: #666; margin-top: 8px;\"\u003eMethod 2: Bulk selection - selecting both paragraphs together\u003c/figcaption\u003e  \n  \u003c/figure\u003e\n\u003c/div\u003e\n\n- e.g., Different but functionally equivalent spreadsheet formulas marked incorrect\n\n### Decentralized Evaluation Reduces Motivation to Contribute Error Discovery\n\n**Lack of motivation to provide feedback**: After discovering problems, most people have no motivation to provide feedback to OSWorld for us to make corrections. This leads to gradual evolution where everyone has their own environment, or even modifies tasks for evaluation to achieve higher scores, making scores increasingly opaque and incomparable.\n\n**No benefit from submitting modifications**: We also see that people hold similar attitudes toward other benchmarks. This is an inherent problem with decentralized benchmarks.\n\nIn summary, to advance the development of this field, we hope everyone can achieve apple-to-apple comparisons. Therefore, we hope the community will actively participate in OSWorld-Verified public evaluation while conducting their own tests, running agents on a unified platform for verification. We have made improvements in both infrastructure and tasks.\n\n\u003chr class=\"solid\"\u003e\n\n## Infrastructure — From VMware to AWS: The Journey of CUA Environment Infrastructure\n\nThe distribution methods for benchmark datasets (similarly, training data) have continuously evolved over time - from being shared through academic papers and word-of-mouth, to university-hosted servers (like Penn Treebank), to GitHub repositories, and now to platforms like Hugging Face datasets as the primary distribution medium. \n\nComputer use agent development environments present unique complexities (after all, we're enabling agents to operate on computers). Under these conditions, achieving apple-to-apple comparisons becomes increasingly challenging, leading to seemingly absurd situations where machine performance correlates with evaluation results.\n\nWe have been continuously searching for the technical optimal solution. Our initial attempt involved building controlled environments using VMware, where we distributed `.vmdk` files and `.vmx` configuration files, saving images during first-time execution on users' computers. However, this approach had significant drawbacks: despite being theoretically parallelizable, the software consumed substantial personal computer resources with excessive runtime. Additionally, VMware became increasingly closed and cumbersome to download after Broadcom's acquisition.\n\nInspired by the [dokur project series](https://github.com/dockur/windows), we integrated Docker containerization technology 2024 Summer, utilizing an open-source VMware-like service - QEMU - running within Docker containers to execute virtual machines. This approach enabled us to achieve multi-environment parallelization on a single server, running 8 or even 16 environments simultaneously for experiments, though still constrained by server performance. We also implemented AWS support during this period but didn't pursue it extensively.\n\nLater, WindowsAgentArena [[8]](#ref8) (whose leading authors founded the c/ua company) left a profound impression on us by leveraging cloud services for parallelization, compressing evaluation time from 10+ hours to just 20 minutes. We think it is the right direction, while enhancement can always be done. So we actually followed WindowsAgentArena's approach by leveraging AWS as cloud services and extended this feature in OSWorld infrastructure, which enables us to run up to 50 environments simultaneously and shorten evaluation time to minutes while ensuring comparability across evaluations.\n\n\u003cfigure style=\"text-align: center;\"\u003e  \n  \u003cimg src=\"/blog/osworld-verified/infra_evolution.svg\" height=20\u003e  \n  \u003cfigcaption style=\"text-align: center;\"\u003eFigure 1. The Journey of CUA Environment Infrastructure.\u003c/figcaption\u003e  \n\u003c/figure\u003e  \n\n\n\u003chr class=\"solid\"\u003e\n\n## Tasks — Practice for Repairing Task Signals Manually at Scale: Embracing Change and Ambiguity\n\nOver the past month we reviewed and manually fixed nearly 300 feedback items collected from every accessible community channel—Moonshot AI, HUD, OpenAI, ByteDance, Anthropic, and Simular AI, etc. We've compiled the detailed checks and modifications into the following spreadsheet [OSWorld fix log](https://docs.google.com/spreadsheets/d/19GSOVCtYM7j3V84Zl5QiaeiEtgK_NIvqtDVXEoVb4U0/edit?gid=0#gid=0).\n\nFor tasks we identified as genuinely problematic, we primarily modified only the evaluators to minimize changes to the tasks themselves and maintain score continuity. We only adjusted task descriptions when absolutely necessary.\n\n### Major Categories of Issues Addressed\n\n#### Web Structure and Content Changes\n\n**Problem**: Websites frequently change their HTML structure, CSS classes, and content, causing evaluation functions to fail or become unreliable.\n\n**Solutions Implemented**:\n- **Updated HTML parsing functions**: Modified `get_active_tab_html_parse` and similar functions to work with current website structures\n- **URL-based validation**: Changed from HTML structure validation to URL field validation (e.g., modifying `get_rule_relativeTime` function)\n- **Fuzzy matching**: Added `is_expected_active_tab_approximate` function for approximate URL comparisons\n- **Environment change marks**: Added `possibility_of_env_change` field to flag volatile tasks\n\n#### Anti-Crawling and Access Issues\n\n**Problem**: Websites blocking automated access through CAPTCHA, IP restrictions, or bot detection.\n\n**Solutions Deployed**:\n- **Proxy infrastructure**: Added `proxy` field support for websites with aggressive anti-crawling\n- **Alternative website selection**: For heavily protected sites (e.g., SeatGeek → Ticketek, TripAdvisor proxy issues), switched to functionally equivalent alternatives\n- **Fixed IP allocation**: Implemented single-IP assignment for tasks requiring human verification\n- **Simulated environments (WIP)**: Created controlled web pages for tasks affected by external dependencies\n\n#### Time-Sensitive and Booking Tasks\n\n**Problem**: Tasks involving future dates, reservations, or time-sensitive content become invalid over time.\n\n**Solutions Implemented**:\n- **Extended booking windows**: Changed booking periods from 4 months to 8+ months for availability\n- **Updated target locations**: Modified unavailable venues (e.g., Albion Basin → Diamond for reservations)\n- **Instruction clarification**: Added specific time qualifications and date ranges\n- **Dynamic date handling**: Improved relative time calculations in evaluation functions\n\n#### Instruction Ambiguity and Clarity\n\n**Problem**: Vague or ambiguous instructions leading to multiple valid interpretations and evaluation inconsistencies.\n\n**Specific Improvements**:\n- **File format specifications**: Added explicit format requirements (e.g., \".png format using only GIMP's built-in features\")\n- **Path clarifications**: Specified exact file paths and naming conventions\n- **Action sequences**: Clarified step-by-step requirements (e.g., \"batch process all images\" vs \"adjust individually\")\n- **Scope limitations**: Added restrictions like \"without launching separate web browser\" or \"using only built-in features\"\n- **Quantitative specifications**: Added precise measurements, percentages, and ranges\n- **Consider more options**: Instead of modifying instruction. Add more options into gold solutions, and give full marks once one is satisfied\n\n#### Evaluation Function Robustness\n\n**Problem**: Overly strict or inadequate evaluation functions causing incorrect scoring.\n\n**Major Enhancements**:\n\n##### File and Content Comparison\n- **Fuzzy matching for documents**: Added `compare_docx_files` with 0-1 scoring instead of binary\n- **Image similarity improvements**: Implemented perceptual hashing algorithms for `compare_pdf_images` to ignore minor visual differences\n- **PDF content validation**: Enhanced PDF comparison with margin and formatting tolerance\n- **EPUB accuracy calculation**: Fixed evaluation logic using averaging instead of problematic multiplication\n\n##### Spreadsheet and Data Tasks\n- **Cell format tolerance**: Ignored formatting differences that don't affect content (e.g., \"$\" signs in currency fields)\n- **Range specifications**: Added precise cell range definitions for operations\n- **Conditional formatting**: Improved detection of styling changes vs content changes\n- **Formula vs value comparison**: Enhanced logic to handle both calculated and manually entered data\n\n##### GUI and Application Tasks\n- **Color tolerance**: Implemented acceptable color variation ranges for visual elements\n- **Font and formatting flexibility**: Reduced strict requirements on default formatting while maintaining core functionality\n- **Unit conversion handling**: Added proper handling for measurement units (cm, px, etc.)\n- **Case sensitivity management**: Made appropriate distinctions between case-sensitive and case-insensitive comparisons\n\n#### System and Environment Stability\n\n**Problem**: Tasks failing due to system-level issues, timing problems, or environment inconsistencies.\n\n**Infrastructure Improvements**:\n- **Blocking vs non-blocking operations**: Converted `open_file` functions from fire-and-forget to synchronous blocking operations\n- **Sleep timing optimization**: Iteratively adjusted inter-action delays for different machine types (`t3.medium`, `t3.large`)\n- **VM Image Optimization**: We compressed the virtual machine images (from 50GB down to 25GB) and adjusted the appropriate IOPS ratios to address the slow first-boot performance caused by lazy loading during AWS AMI/EBS snapshot transfers (LibreOffice taking 1 minute to open on first launch was unacceptable), while also reducing costs\n- **Font installation**: Install all fonts for LibreOffice Impress and Writer tasks to avoid formatting issues caused by missing font files.\n- **Audio/video handling**: Added proper audio device configuration (`-aout=dummy` for VLC) and format validation\n- **Hosting Location**: We move all files (including init state files used to build the environment's initial state and gold files for result validation) from Google Drive to Hugging Face. Using Google Drive for file hosting proved to be a poor choice 🙃- whenever download counts became too high, it would temporarily block all downloads for everyone, causing evaluation issues. We have migrated all files to Hugging Face datasets for smooth distribution\n\n### Takeaways for Future Benchmark Construction\n\n#### What We Would Do Differently\n- **Proactive monitoring**: Implement webhook-like systems for continuous task health monitoring\n- **Simulated environments**: Build more controlled environments for tasks prone to external changes\n- **Dual-track evaluation**: Maintain both decentralized and centralized evaluation systems\n- **Resource planning**: Budget significantly more time and human resources for ongoing maintenance. Even very daily tasks can contain significant amounts of possible approaches than we can expect at the beginning\n\n#### Best Practices Established\n- **Minimal invasive changes**: Prioritize evaluation fixes over task modifications\n- **Comprehensive documentation**: Maintain detailed logs of all issues and resolutions\n- **Community integration**: Establish clear channels for issue reporting and collaborative fixes\n- **Continuous improvement**: Regular review cycles with stakeholder feedback integration\n\nThis comprehensive repair effort has significantly improved the reliability and validity of the OSWorld benchmark while establishing processes for ongoing maintenance and improvement.\n\n\u003chr class=\"solid\"\u003e\n\n## Based on the Rerun Results, How Far Are We From Completing OSWorld?\n\n### Our Implementation\n\nFor Agent implementation, we have added implementations of Operator, Claude, UI-TARS [[9]](#ref9), Qwen2.5-VL [[10]](#ref10), Seed-1.5-VL [[11]](#ref11), OpenCUA [[12]](#ref12) and other models based on the existing agents implementation. \nWe have also integrated advanced agent implementations based on agentic frameworks, such as Agent S[[13]](#ref13), Jedi[[14]](#ref14), GTA1[[15]](#ref15), and others.\nAll agent implementations are placed under [mm_agents folder of OSWorld repo](https://github.com/xlang-ai/OSWorld/tree/main/mm_agents).\n\nIn the implementation, since many of the above models are not publicly available, for Operator and Claude models, we extensively referenced the implementation of Computer Agent Arena [[16]](#ref16), and referenced some usage examples from these providers [[17]](#ref17)[[18]](#ref18)[[19]](#ref19). \nWe also conducted repeated validation and calibration of agent performance through preliminary experiments for confirmation.\n\n### Key Insights from OSWorld-Verified Results\n\nWe ran experiments for each model under different step settings. Here are some takeaways:\n\n**OSWorld remains far from saturated with substantial headroom to human-level performance.** \nDespite remarkable breakthroughs, the benchmark continues to present significant challenges. With human performance estimated at ~72% from our original study, the best current systems have now reached 84.4% of human capability (CoACT-1 at 60.76%). \nThe performance distribution reveals distinct tiers with substantial improvements: agentic frameworks now leading at 45-61%, advanced foundation models achieving 35-44%, and specialized models reaching 25-40%. \nWhile the gaps between tiers remain significant, the dramatic upward shift across all categories demonstrates accelerating progress. \nThis indicates that OSWorld continues to provide meaningful developmental signal, particularly highlighting the effectiveness of reasoning-enhanced agentic approaches while revealing remaining challenges in areas requiring complex multi-step reasoning, robust error recovery, and dynamic adaptation to interface changes.\n\n\u003cfigure style=\"text-align: center; margin: 30px auto; max-width: 100%;\"\u003e  \n  \u003cimg src=\"/blog/osworld-verified/human_gap_svg.svg\" height=400 style=\"display: block; margin: 0 auto;\"\u003e  \n  \u003cfigcaption style=\"text-align: center;\"\u003eFigure 2. Gap to Human Performance - Current best models still show significant gaps compared to human performance.\u003c/figcaption\u003e  \n\u003c/figure\u003e\n\n**Agentic frameworks with reasoning models dominate the leaderboard.** Agentic frameworks powered by reasoning models like o3 have achieved breakthrough performance.CoACT-1 leads with 60.76% success rate, followed closely by Agent S2.5 w/ o3 (56.0%) and GTA1 w/ o3 (53.1%). \nThis demonstrates that sophisticated orchestration layers can dramatically amplify the capabilities of reasoning models, even when those models weren't specifically trained for computer use tasks. \nInterestingly, highlighting the importance of computational resources in achieving strong performance.\n\n**General models' capability improvements and reasoning enhancements show exceptional progress in computer use capabilities.** \nAmong foundation models, Claude 4 Sonnet stands out with 43.9% performance, significantly outperforming other general-purpose models and even approaching specialized computer use models like UI-TARS (40.0%). o3's performance varies drastically with step budget (9.1% to 23.0%), compared with 5% of GPT-4o, not to mention further integration with grounding model improvements. \nThis all suggests that better pretraining and post-training bringing general scaling capabilities and reasoning abilities (even without any computer-use specific purpose) will potentially help improve computer use agents.\n\n\n\nBased on these results, we updated the leaderboard by adding a verified section and setting it as the default display, while also adding links to computer agent arena scores. For future model submissions, we will continue to serve as a verification platform, consistently open-sourcing agent implementations, execution code, reproducible results, and generated trajectories to help the community gain further insights into current capability boundaries and continuously provide reliable evaluation signals.\n\n\u003chr class=\"solid\"\u003e\n\n## How to Use and Submit New Results on the Leaderboard\n\nOSWorld-Verified is an in-place upgrade of OSWorld with enhanced infrastructure and improved task quality. You can always still use providers like VMware and Docker with the new task suites. Please compare your OSWorld results with the new benchmark results when running the latest version.\n\nMeanwhile, we have provided a detailed guide on using our AWS-based [Public Evaluation platform](https://github.com/xlang-ai/OSWorld/blob/main/PUBLIC_EVALUATION_GUIDELINE.md). You can set up and run your OSWorld-Verified tests on this more controllable platform. \n\nIf you want your results to be verified and displayed on the verified leaderboard, you need to schedule a meeting with us (current maintainers: tianbaoxiexxx@gmail.com, yuanmengqi732@gmail.com) to run your agent code on our side and have us report the results. \n\nYou need to upload and allow us to disclose your agent implementation under the OSWorld framework (you may choose not to expose your model API to the public), along with a report that allows the public to understand what's happening behind the scenes.\n\nAlternatively, if you are from a trusted institution, you can share your monitoring data and trajectories with us. \n\nPlease carefully follow the [Public Evaluation Guideline](https://github.com/xlang-ai/OSWorld/blob/main/PUBLIC_EVALUATION_GUIDELINE.md) to get results.\n\n\u003chr class=\"solid\"\u003e\n\n## Key Insights for the Community: Next Steps Forward\n\n### Scaling Computer Use Data Collection and Training\n\nOur analysis of UI-TARS and OpenCUA, currently the most successful models with publicly available technical details, reveals that their success stems primarily from extensive human computer use trajectory data. \nThe existing publicly available trajectory datasets remain insufficient in scale, predominantly focusing on Mobile and Web Agent domains with limited action spaces, indicating that research in this area is still in its early stages [[17]](#ref17)[[18]](#ref18)[[19]](#ref19).\n\nThere is substantial room for exploration in data collection methodologies (including collection tools, annotation frameworks, and potentially viable commercial models), synthetic data generation approaches, efficient utilization strategies, and optimal integration practices across different training phases. \nCurrent evidence suggests that diverse, high-quality computer use data, combined with existing model architectures, is sufficient to enable models to develop robust computer use capabilities. \nFurther scaling could involve collecting large volumes of unlabeled trajectories for pretraining phases, while providing more effective signals during post-training to enhance instruction-following capabilities in computer use scenarios. This approach is particularly suitable for broad and diverse tasks, especially in cases where building reproducible environments is challenging, serving as open-loop training that can capture the general patterns of computer interactions across varied contexts and applications.\n\n### Scaling Post-Training Signal Enhancement\n\nJason Wei's blog post on verification asymmetry and its implications for reinforcement learning [[20]](#ref20) provides valuable insights that align with OSWorld's approach to leveraging verification asymmetry for signal generation. For broader-scope computer use domains, we encompass both test case verification (similar to SWE-bench) and verification capabilities found in competition mathematics and open-domain QA (such as AIME and GAIA). We collectively term this \"verifiable code\" or \"(dense) reward code,\" which can often be automatically generated [[21]](#ref21)[[22]](#ref22).\n\nCurrent models lack robust capabilities for processing trajectories, particularly multimodal ones, making correctness assessment challenging (though human evaluation of computer use actions remains relatively straightforward). \nIf humans establish appropriate scaffolding with well-designed verification code, combined with human-generated operational results for additional validation support, the difficulty of assessment could be significantly reduced or even eliminated. \nHowever, building such scaffolding requires domain expertise, and iterating through multiple problem-solving approaches demands practical experience. \nOur goal should be to advance models to a position where they can autonomously construct this scaffolding—a process requiring iteration but representing our intended direction.\nGiven current cost considerations, this post-training environment construction process should be narrow and deep, targeting the most critical areas. For example, we could build a comprehensive Photoshop-related scaffolding codebase and then massively expand image editing tasks, or we could annotate a complete spreadsheet scaffolding codebase and then annotate a large number of business data analysis tasks. Rather than broadly annotating many scaffolding codebases and then, after consuming significant costs, annotating shallow tasks that waste our investment in this area.\n\nThe overall approach is that we propose leveraging the gap in all domains where max(human verification capability, model verification capability) \u003e model generation capability to reinforce or filter synthetically generated data for capability enhancement. This process can be substantially automated or semi-automated [[23]](#ref23)[[24]](#ref24), with existing academic research offering significant opportunities for industrial-scale implementation. Additionally, for existing verifiable rewards, we can build upon current infrastructure to consider process-based approaches for denser reward provision, thereby improving learning efficiency for suitable tasks where critical milestones can be clearly defined.\n\n### More Realistic and Controllable Evaluation\n\nIn OSWorld's first iteration, we focused on defining task frameworks for computer use agents, standardizing action states, and establishing evaluation environment infrastructure. Through engineering practice, we explored virtualization-based environment construction and backend-driven action execution methodologies. Due to time and resource constraints, we selected commonly used software applications and avoided over-annotating tasks requiring extended completion times, deep professional software expertise, video understanding components, real-time intensive operations, and asynchronous completion scenarios.\n\nOur team is actively working toward releasing an updated version before the end of this fall, addressing these limitations and expanding the evaluation scope.\n\nLooking forward, we envision building the future of digital intelligent agents, providing novel interfaces that liberate human productivity and transform how we interact with computing environments. The convergence of scaled trajectory data, enhanced verification mechanisms, and more comprehensive evaluation frameworks will be instrumental in realizing this vision of autonomous computer use capabilities.\n\nThanks for getting this far.\n\n\u003chr class=\"solid\"\u003e\n\n## References\n\n\u003ca id=\"ref1\"\u003e\u003c/a\u003e[1] Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., ... \u0026 Yu, T. (2024). Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37, 52040-52094.\n\n\u003ca id=\"ref2\"\u003e\u003c/a\u003e[2] Anthopic, P. B. C. (2024, October). Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku. https://www.anthropic.com/news/3-5-models-and-computer-use\n\n\u003ca id=\"ref3\"\u003e\u003c/a\u003e[3] Anthropic, C. (2025). 3.7 sonnet and claude code. https://www.anthropic.com/news/claude-3-7-sonnet\n\n\u003ca id=\"ref4\"\u003e\u003c/a\u003e[4] OpenAI Operator Team, Computer-Using Agent: Powering Operator with Computer-Using Agent, a universal interface for AI to interact with the digital world. https://openai.com/index/computer-using-agent/\n\n\u003ca id=\"ref5\"\u003e\u003c/a\u003e[5] Get started with C/ua, https://www.trycua.com/\n\n\u003ca id=\"ref6\"\u003e\u003c/a\u003e[6] The evaluation platform for computer use agents, https://www.hud.so/\n\n\u003ca id=\"ref7\"\u003e\u003c/a\u003e[7] A computer for your AI, https://scrapybara.com/\n\n\u003ca id=\"ref8\"\u003e\u003c/a\u003e[8] Bonatti, R., Zhao, D., Bonacci, F., Dupont, D., Abdali, S., Li, Y., ... \u0026 Hui, Z. (2024). Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264.\n\n\u003ca id=\"ref9\"\u003e\u003c/a\u003e[9] Qin, Y., Ye, Y., Fang, J., Wang, H., Liang, S., Tian, S., ... \u0026 Shi, G. (2025). Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326.\n\n\u003ca id=\"ref10\"\u003e\u003c/a\u003e[10] Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., ... \u0026 Lin, J. (2025). Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923.\n\n\u003ca id=\"ref11\"\u003e\u003c/a\u003e[11] Guo, D., Wu, F., Zhu, F., Leng, F., Shi, G., Chen, H., ... \u0026 Wang, W. (2025). Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062.\n\n\u003ca id=\"ref12\"\u003e\u003c/a\u003e[12] Wang, X., Wang, B., Lu, D., Yang, J., Xie, T., Wang, J., Deng, J., Guo, X., Xu, Y., Wu, C. H., Shen, Z., Li, Z., Li, R., Li, X., Chen, J., Zheng, B., Li, P., Lei, F., Cao, R., Fu, Y., Shin, D., Shin, M., Hu, J., Wang, Y., Chen, J., Ye, Y., Zhang, D., Wang, Y., Wang, H., Yang, D., Zhong, V., Charles, Y., Yang, Z., \u0026 Yu, T. (2025). OpenCUA: Open foundations for computer-use agents. arXiv preprint. https://opencua.xlang.ai/\n\n\u003ca id=\"ref13\"\u003e\u003c/a\u003e[13] Agashe, S., Wong, K., Tu, V., Yang, J., Li, A., \u0026 Wang, X. E. (2025). Agent s2: A compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906.\n\n\u003ca id=\"ref14\"\u003e\u003c/a\u003e[14] Xie, T., Deng, J., Li, X., Yang, J., Wu, H., Chen, J., ... \u0026 Xiong, C. (2025). Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis. arXiv preprint arXiv:2505.13227.\n\n\u003ca id=\"ref15\"\u003e\u003c/a\u003e[15] Yang, Y., Li, D., Dai, Y., Yang, Y., Luo, Z., Zhao, Z., ... \u0026 Li, J. (2025). GTA1: GUI Test-time Scaling Agent. arXiv preprint arXiv:2507.05791.\n\n\u003ca id=\"ref16\"\u003e\u003c/a\u003e[16] Wang, B., Wang, X., Deng, J., Zhong, V., Yu, T., Xie, T., Li, R., Zhang, Y., Li, G., Toh, J. H., Wang, Z., Zhang, Y., Su, Y., \u0026 Yang, D. (2025). Computer Agent Arena: An open-source online platform evaluating computer agents in real computer environments. XLANG Lab. https://arena.xlang.ai/\n\n\u003ca id=\"ref17\"\u003e\u003c/a\u003e[17] OpenAI CUA Sample App: OpenAI. (2025). OpenAI CUA sample app [Computer software]. GitHub. https://github.com/openai/openai-cua-sample-app\n\n\u003ca id=\"ref18\"\u003e\u003c/a\u003e[18] OpenAI Computer Use Documentation: OpenAI. (2025). Computer use. In OpenAI Platform Documentation. https://platform.openai.com/docs/guides/tools-computer-use\n\n\u003ca id=\"ref19\"\u003e\u003c/a\u003e[19] Anthropic Computer Use Demo: Anthropic. (2025). Computer use demo [Computer software]. GitHub. https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo\n\n\u003ca id=\"ref20\"\u003e\u003c/a\u003e[20] Xu, Y., Wang, Z., Wang, J., Lu, D., Xie, T., Saha, A., ... \u0026 Xiong, C. (2024). Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454.\n\n\u003ca id=\"ref21\"\u003e\u003c/a\u003e[21] Xu, Y., Lu, D., Shen, Z., Wang, J., Wang, Z., Mao, Y., ... \u0026 Yu, T. (2024). Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. arXiv preprint arXiv:2412.09605.\n\n\u003ca id=\"ref22\"\u003e\u003c/a\u003e[22] Xie, J., Xu, D., Zhao, X., \u0026 Song, D. (2025). AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents. arXiv preprint arXiv:2506.14205.\n\n\u003ca id=\"ref23\"\u003e\u003c/a\u003e[23] Jason Wei, Asymmetry of verification and verifier's law, 2025, https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law\n\n\u003ca id=\"ref24\"\u003e\u003c/a\u003e[24] Xie, T., Zhao, S., Wu, C. H., Liu, Y., Luo, Q., Zhong, V., ... \u0026 Yu, T. (2023). Text2reward: Reward shaping with language models for reinforcement learning. arXiv preprint arXiv:2309.11489.\n\n\u003ca id=\"ref25\"\u003e\u003c/a\u003e[25] Ma, Y. J., Liang, W., Wang, G., Huang, D. A., Bastani, O., Jayaraman, D., ... \u0026 Anandkumar, A. (2023). Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931.\n\n\u003ca id=\"ref26\"\u003e\u003c/a\u003e[26] Zhong, R., Snell, C., Klein, D., \u0026 Eisner, J. (2022). Non-programmers can label programs indirectly via active examples: A case study with text-to-SQL. arXiv preprint arXiv:2205.12422.\n\n\u003ca id=\"ref27\"\u003e\u003c/a\u003e[27] Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., ... \u0026 Wu, J. (2023). Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390.\n\n\u003chr class=\"solid\"\u003e\n\n## Citation\n\nIf you think this blog post and the content involved are helpful to you, please cite:\n```\n@article{osworld_verified,\n  title = {Introducing OSWorld-Verified},\n  author = {Tianbao Xie and Mengqi Yuan and Danyang Zhang and Xinzhuang Xiong and Zhennan Shen and Zilong Zhou and Xinyuan Wang and Yanxu Chen and Jiaqi Deng and Junda Chen and Bowen Wang and Haoyuan Wu and Jixuan Chen and Junli Wang and Dunjie Lu and Hao Hu and Tao Yu},\n  journal = {xlang.ai},\n  year = {2025},\n  month = {July},\n  url = \"https://xlang.ai/blog/osworld-verified\"\n}\n```\n\n\n\n\n","title":"Introducing OSWorld-Verified","shortTitle":"OSWorld-Verified","date":"28 July 2025","author":"XLANG Lab","coverImage":"/blog/osworld-verified/overview.png","previewContent":"We've systematically addressed 300+ issues in OSWorld through a comprehensive refinement process. OSWorld-Verified delivers more reliable evaluation signals through improved infrastructure and enhanced task quality, providing the community with a more stable foundation for advancing computer use agent research.","onlineImage":"https://imgur.com/a/KEfC6ce","githubLink":"https://github.com/xlang-ai/OSWorld"},{"slug":"openlemur","content":"\nTLDR: 🎉 Introducing Lemur-70B \u0026 Lemur-70B-Chat: 🚀Open \u0026 SOTA Foundation Models for Language Agents! The closest open model to GPT-3.5 on 🤖15 agent tasks🤖!\n\n📄Paper: [http://arxiv.org/abs/2310.06830](http://arxiv.org/abs/2310.06830)\n\n🤗Model: [http://huggingface.co/OpenLemur](http://huggingface.co/OpenLemur)\n\n👩‍💻Code: [https://github.com/OpenLemur/Lemur](https://github.com/OpenLemur/Lemur)\n\n\u003chr class=\"solid\"\u003e\n\nWe are excited to announce Lemur, an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. Our preprint [Lemur: Harmonizing Natural Language and Code for Language Agents](https://arxiv.org/abs/2310.06830) shares details on this new model.\n\nAs language models continue to evolve from conversational chatbots to functional agents that can act in the real world, they need both strong language understanding and the ability to execute actions. Lemur balances natural language and coding skills to enable agents to follow instructions, reason about tasks, and take grounded actions.\n\nPlease find more detailed information here:\n\n- Paper: [Lemur: Harmonizing Natural Language and Code for Language Agents](https://arxiv.org/abs/2310.06830)\n- Code: [OpenLemur Repository ](https://github.com/OpenLemur/Lemur)\n- Model: [HuggingFace Hub](https://huggingface.co/OpenLemur)\n\n## Why Lemur?\nMost existing open source models specialize in either natural language or code. Lemur combines both strengths by:\n\n- Pretraining on a 90B token corpus with 10:1 ratio of code to text\n- Instruction tuning on 300K examples covering both text and code\n\nThis two-stage training produces state-of-the-art performance averaged across diverse language and coding benchmarks, surpassing other available open source models and narrowing the gap between open-source and commercial models on agent abilities.\n\n\u003cfigure style=\"text-align: center;\"\u003e  \n  \u003cimg src=\"/blog/lemur/pipeline.png\" height=20\u003e  \n  \u003cfigcaption style=\"text-align: center;\"\u003eTraining Procedure of Lemur Models\u003c/figcaption\u003e  \n\u003c/figure\u003e  \n\n## Testing Lemur's Abilities\nWe evaluated Lemur across:\n- 8 language and code datasets like MMLU, BBH, GSM8K, HumanEval, and Spider to validate balanced capabilities\n- 13 interactive agent datasets to test skills like tool usage, adapting to feedback from environment or human, and exploring partially observable digital or physical environments.\n\n\u003cfigure style=\"text-align: center;\"\u003e  \n  \u003cimg src=\"/blog/lemur/agent-skills.png\" height=20\u003e  \n  \u003cfigcaption style=\"text-align: center;\"\u003e\u003c/figcaption\u003e  \n\u003c/figure\u003e  \n\n\nLemur significantly outperformed other models on agent benchmarks, showcasing its versatility and potential as a foundation for capable agents.\n\n\u003cfigure style=\"text-align: center;\"\u003e  \n  \u003cimg src=\"/blog/lemur/overall-performance.png\" height=20\u003e  \n  \u003cfigcaption style=\"text-align: center;\"\u003eComparison of the foundational and agent capabilities between Lemur and other models.\u003c/figcaption\u003e  \n\u003c/figure\u003e  \n\n\u003cbr\u003e\n\n## Moving Research Forward\nBy open sourcing Lemur and our training corpora, we hope to empower more research into developing capable and controllable agents that can understand instructions and act appropriately. Testing Lemur across diverse agent scenarios gives insights into critical areas like:\n\n- Harmonizing natural language and programming abilities\n- Tool usage as an augmentation mechanism\n- Adapting behavior based on environment and human feedback\n- Reasoning and planning under partial observability in digital and physical environments\n\nThere is still much work to be done, but Lemur represents an important step towards open source models that can power the next generation of language agents. We look forward to seeing what the community builds!\n\nYou can find more details in our preprint: [Lemur: Harmonizing Natural Language and Code for Language Agents](https://arxiv.org/abs/2310.06830)\n\n## Acknowledgements\n\nThe Lemur project is a open collaborative research effort between [XLang Lab](https://xlang.ai) and [Salesforce Research](https://www.salesforceairesearch.com/). We would like to thank Salesforce Research, Google Research, and Amazon AWS for their gift support to this open-source effort!\n","title":"Introducing Lemur: Open Foundation Models for Language Agents","shortTitle":"Lemur Intro","date":"08 October 2023","author":"XLANG Lab","coverImage":"/blog/lemur/overview.jpeg","previewContent":"We are excited to announce Lemur, an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents.","onlineImage":"https://i.imgur.com/m4aI7hW.jpg","twitterLink":"https://twitter.com/taoyds/status/1712547761252610076","githubLink":"https://github.com/OpenLemur/Lemur"},{"slug":"xlang-intro","content":"\n\n\u003e *\"Many years later, as he faced the firing squad, Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice.\" —— One Hundred Years of Solitude, Gabriel Garcia Márquez.*\n\u003e\n---\n\nWhen envisioning the future, people have always imagined an intelligent agent capable of following human commands and performing specific tasks, significantly enhancing productivity. In recent times, with the emergence of powerful language models, this vision is accelerating. We're getting a fresh perspective on what language model agents can truly achieve. The language model agent represents a crucial step in this burgeoning field, enabling powerful language models to take actions, use tools, execute various tasks and facilitate more intelligent interactions with humans. \n\nWith great excitement, our team, **XLang Lab**, introduces our recent efforts to exploring and advancing the realm of language model agents. In the upcoming sections, we will elaborate on our mission, workouts, and the outlook for future endeavors. Through our mission, we aspire to humbly provide insightful perspectives on the historical trajectory of this captivating vision, pushing forward the frontiers of what these agents can achieve. With this enthusiasm and determination, let us together embrace a new era of intelligent agents for the future.\n\n## **What is XLang?**\n\n**Executable Language Grounding (XLang)** refers to the process of converting natural language instructions into ***code or action sequences executable*** in some environments. It involves generating code or actions that can interact with the environment, perform specific operations, and produce tangible results. XLang thus serves as a bridge, transforming natural language instructions into code or actions executable within real-world environments. Such environments include but are not limited to databases, web applications, and the physical world navigated by robots.\n\n\u003e *\"Code never lies, comments sometimes do.\" —— Ron Jeffries*\n\u003e\n\nImagine the process as the transmutation of human instructions and questions — expressed in everyday language — into machine-understandable actions and code. The machine then executes these within a specific environment, leading to changes in the state of that environment. This change is observed, results are analyzed, and a further cycle of interaction with humans is initiated.\n\nThis process expands the capabilities of the agents **far beyond those of a conventional chatbot**, allowing it to address and serve a much broader scope of tasks and applications. And we believe such process lies at the heart of AI/language agents that interact with various real-world environments via natural language and accomplish tangible tasks for us. Recent advances in Executable Language Grounding incorporate techniques such as LLM (Large Language Models) enhanced with neuro-symbolic tools, code generation or semantic parsing, and dialog or interactive systems.\n\nIn the current generation of LLMs, we can furnish our models with an assortment of neuro-symbolic tools to enhance their capabilities. Typical inputs to these models could include language from the user, a toolkit brimming with different tools, and a variety of environments. The outcome is an action or code sequence executable within the corresponding environment, often entailing the use of certain tool APIs.\n\nThe process of building such agents often demands considerable effort and collaboration from a dedicated team. This is where XLang comes into play. Our objective with XLang is to establish an open-source framework and ecosystem for building and evaluating these powerful LLM-powered agents.\n\n## **The Motivations and Challenges behind XLang**\n\nOur team's initial spark of excitement came when OpenAI announced the release of code interpreters and plugins. These tools represented live demonstrations of many years of our research in areas such as code generation, semantic parsing, natural language interfaces, and interactive dialog systems. Unfortunately, we couldn't access these tools to experiment with our ideas for improvements. This limitation prompted us to think ambitiously. What if we developed our own open-source code interpreters and plugins, or even a more general agent system and framework? This would not only benefit our team but also other research labs and companies worldwide. By sharing our work, we believe we can contribute to the growth of research and applications in this direction, allowing more people to perform exciting and interesting work on our open-source system. More specifically, by shifting the focus towards interactive and real-time demos, we can:\n\n- iteratively add and improve the agent’s design and working logics, such as integrating more useful tools\n- implement robust evaluation procedures for various LLMs (Large Language Models) in a neutral manner; while platforms like Vicuna Arena (chatbots) have served as valuable pioneers in this area, we strive to uncover evaluation metrics tailored specifically for language model agents (chatbots + grounding).\n- push forward the agentic model’s training and development; by incorporating comprehensive evaluations and continuously iterating the training process, we aim to uncover the shortcomings of LLMs and make iterative improvements.\n\nIn short, by setting the frameworks including a real-time demo, we can further advance the research and development of LLM-powered agents and demonstrate their potential.\n\nAll such promising directions requires a critical first step: establishing a unified system that real users can access. However, challenges are many. Setting up agent demos is not as straightforward as simply connecting a chatbox to an LLM. It requires considerable effort, both in terms of engineering and research. The agents need to interact with their corresponding environments and effect changes in those environments. The system must robustly handle all sorts of situations, appropriately manage different possible execution results, and present these outcomes in the correct manner. Ensuring the robustness and scalability of this interaction cycle is a significant challenge.\n\nHistorically, NLP has lacked practical system demonstrations like those found in robotics or databases, and has instead focused more on testing against static benchmarks. However, with the advent of large models, we believe the time has come to bridge this gap. Similar to the MineDojo framework in embodied AI and the ManiSkill for robotics, setting up such an agent framework necessitates long-term cooperation among many individuals. Our goal is to see the fruits of our research move step by step towards real-world applications that will, in the not-so-distant future, be used by millions.\n\nOur XLang team of about 15 researchers and developers from various backgrounds including **NLP** (Natural Language Processing), **ML** (Machine Learning), **HCI** (Human Computer Interaction), **VIS** (Visualization), **DB** (Database), **Full-stack** development, **UI design**, and **Robotics** have been working full-time on this project since the end of March. We've invested significant effort in addressing these challenges and minimizing the gap between research and the development of real-world interactive agents. We firmly believe in the value of our work and hope our open-source project will attract more researchers, developers, and designers to contribute to this exciting direction.\n\n## **Why Us? Our Journey**\n\nThe answer to this question is quite straightforward. Our team is deeply interested in this field, and we've always wished for agents that can help people analyze data without coding, and for more natural language-led interaction modes for webs/apps. This was the primary reason why, in March, our team decided within four days of discussing the concept, to congregate in WeWork Shenzhen and commence full-time work on the project. This dedication has continued for more than four months, and we are committed to the long-term development of this project.\n\nMany of our team members have been consistently drawn to research problems in this direction. Our team comprises professionals with backgrounds in HCI, DB, NLP, visualization, and ML. We have all conducted extensive work on executable language grounding for building LLM-powered AI agents, specifically natural language interfaces for data in databases and webs/apps. \n\nThroughout this process, we have maintained active collaborations with industry players like Salesforce, Microsoft, Amazon, Facebook, Google, especially in the realm of text-to-SQL and code generation. We were also among the pioneers in working with large language models for in-context learning ([**Selective Annotation**](https://github.com/HKUNLP/icl-selective-annotation)), LLM + tool use ([**Binder**](https://lm-code-binder.github.io/)), instruction tuning and retrieval embeddings for LLM ([**Instructor Embeddings**](https://instructor-embedding.github.io/)), code generation and semantic parsing ([**Spider**](https://yale-lily.github.io/spider), [**DS-1000**](https://ds1000-code-gen.github.io/), [**Coder-Reviewer Reranking**](https://arxiv.org/abs/2211.16490), [**UnifiedSKG**](https://github.com/HKUNLP/UnifiedSKG)),and interactive dialog systems ([**ICL-DST**](https://github.com/Yushi-Hu/IC-DST), [**SParC**](https://yale-lily.github.io/sparc), [**CoSQL**](https://yale-lily.github.io/cosql), [**NL2Interface**](https://arxiv.org/abs/2209.08834)). More about our research can be found on our [project page](https://www.xlang.ai/project). \n\nWe are a dedicated research team profoundly invested and interested in XLang and language model agents, particularly those related to data and web/app agents. More XLang, code generation, LLM+tool use, and LLM+robotics paper collections can be found in our ACL tutorial on complex reasoning: [LLM+tool use](https://github.com/xlang-ai/xlang-paper-reading).\n\n## **XLang Agents: An Introduction**\n\nXLang Agents are language model agents developed by our team, aiming to utilize a range of tools to enhance their capabilities, serving as user-centric intelligent agents. Currently the XLang Agents supports three different agents focusing on different application scenarios, including:\n\n- **Data Agent**: This agent is skilled in data tools, allowing efficient data search, manipulation, and visualization. It excels in code execution for data-centric tasks.\n- **Plugins Agent**: With over 200 third-party plugins, this agent addresses diverse daily life needs, aiding in various tasks.\n- **Web Agent**: Utilizing a Chrome extension, this agent automates web navigation, streamlining browsing to find and access information.\n- **Robotic Agent**: comming soon\n    \n💡 We have make all three agents online, just visit 👉[**XLang Agents**](https://chat.xlang.ai) and feel free to explore! For more details about XLang Agents, you can also check the official documents in 👉[**XLang Docs**](https://docs.xlang.ai) !\n\n**Here are some interesting things XLang Agents can do!**\n\n---\n\n### Data Agent\n\n**Code generation + Data tools = Data Agent!**\n\nAfter selecting certain data tools, the agent can take your request and proactively take actions to fulfill your request. \n\nIn the following example, you will see how data agent help you search a dataset, draw an interactive line plot, and finally construct an ARIMA model to perform some prediction.\n\n\u003c!-- [https://www.youtube.com/watch?v=JabK4PiJJqs](https://www.youtube.com/watch?v=JabK4PiJJqs) --\u003e\n\u003ciframe src=\"https://www.youtube.com/embed/JabK4PiJJqs\" title=\"Data Agent Overview\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\n\n---\n\n### Plugins Agent\n\n**Unleash the power of hundreds of real-world applications through our intelligent Plugins system!**\n\nThe Agent uses a provided API YAML to intelligently determines the optimal timing and selection of plugins to invoke. Each plugin has been thoughtfully curated to fulfill various requirements across your everyday life situations.\n\nFor instance, when traveling to Toronto, it recommends attractions, handles currency conversion, provides weather updates, and suggests clothing, ensuring a hassle-free journey.\n\n\u003ciframe src=\"https://www.youtube.com/embed/UL7VEAQHYBE\" title=\"Plugins Agent Overview\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\n\n---\n\n### Web Agent\n\n**Effortlessly navigate the internet with the Web Agent, powering up your browsing experience.**\n\nThe Web Agent, utilizing a Chrome extension, automates website navigation to streamline browsing and enhance information retrieval. It simplifies the user's quest for pertinent details and desired resources. \n\nSpecifically in the following example, the agent extracts movie reviews from IMDb and assists in posting a thread on Twitter. Additionally, our interface facilitates multi-turn interactions, ensuring efficient task completion and enriched user engagement.\n\n\u003ciframe src=\"https://www.youtube.com/embed/yH31TXBfrKI\" title=\"Web Agent Overview\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\n\n---\n\nBy harnessing the power of large language models in conjunction with diverse tools, XLang Agents significantly expand the capabilities of conversational interfaces, offering intelligent assistance that revolves around the user. \n\n**Engage in a conversation with our XLang Agents to explore its wide-ranging capabilities further!**\n\n## **What's Next? The Future**\n\nOur aim is to build XLang, an open-source ecosystem and community for building and evaluating language model agents. This release will just be the beginning of our XLang open-source journey. In the following months, and beyond, we will be open-sourcing several significant projects, which will include all frameworks, models, demos, code, benchmarks, and more. We hope that in these particular times in NLP, we can enable more people, rather than just a few large companies or closed start-ups, to participate. We envision these initiatives as the starting point to establish a vivid LLM-powered agents, LLM + tool use, and language grounding community, encouraging more people to contribute, develop and perform exciting research based on our work.\n\n- Online demos of XLang Agents\n- Framework or toolkits, more sophisticated LangChain for building and evaluating langauge model agents\n- Agent demo frontend and backend repos for HCI/VIS + NLP research and developers\n- Pretraining actionable and agentic large language models (donation supports welcome!)\n- SOTA methods for code generation, general LLMs, and LLMs + tool use for building language model agents\n- ……\n\n## **Acknowledgements**\n\nWe would like to express our gratitude towards Google Research, Amazon AWS, and Salesforce Research. The gift funds and necessary computational resources generously provided by these awards have given us the capability and resources to implement this project. We also appreciate the invaluable advice we received throughout the process.\n\n**Personal Acknowledgements by [Tao](https://taoyds.github.io/):**\n\nI feel fortunate for the year I spent at UWNLP, which is one of the world's top institutions for NLP research. During this time, I observed the nascent shift towards LLM in NLP. I would like to extend my thanks to Noah Smith, Luke Zettlemoyer, and Mari Ostendorf. The idea of XLang came about from a suggestion Luke made during a meeting in his office.\n\nI would also like to pay tribute to my late Ph.D. advisor, Dragomir Radev. Without him, it's very possible that none of what we are starting today would exist.\n","title":"Introducing XLang: An Open-Source Framework for Building Language Model Agents via Executable Language Grounding","shortTitle":"XLANG Intro","date":"10 August 2023","author":"XLANG Lab","coverImage":"/blog/xlang_overview.png","previewContent":"Introducing XLang, an open-source platform that constructs language model agents through executable language grounding. Alongside this framework, we unveil demos of XLang Agents, encompassing Data, Plugins, and Web agents. Moving forward, we're set to open-source multiple substantial projects, encompassing frameworks, models, demos, code, benchmarks, and beyond.","onlineImage":"https://i.imgur.com/sPqu2t7.png","twitterLink":"https://twitter.com/XLangNLP/status/1689723514134446081"}]},"__N_SSG":true},"page":"/blog","query":{},"buildId":"d5O8OIQN-5BBLJ4NpJJOB","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>