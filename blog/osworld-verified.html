<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>XLANG Lab | <!-- -->Introducing OSWorld-Verified</title><link rel="apple-touch-icon" sizes="180x180" href="/favicon/black-on-white/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/black-on-white/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/black-on-white/favicon-16x16.png"/><link rel="manifest" href="/favicon/black-on-white/site.webmanifest"/><meta name="description" content="Introducing OSWorld-Verified"/><meta property="og:title" content="Introducing OSWorld-Verified"/><meta property="og:type" content="website"/><meta property="og:image" content="https://imgur.com/a/Jh7SS0Q"/><meta property="og:description" content="We&#x27;ve systematically addressed 300+ issues in OSWorld through a comprehensive refinement process. OSWorld-Verified delivers more reliable evaluation signals through improved infrastructure and enhanced task quality, providing the community with a more stable foundation for advancing computer use agent research."/><meta property="og:url" content="https://xlang.ai/blog/osworld-verified"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Introducing OSWorld-Verified"/><meta name="twitter:description" content="We&#x27;ve systematically addressed 300+ issues in OSWorld through a comprehensive refinement process. OSWorld-Verified delivers more reliable evaluation signals through improved infrastructure and enhanced task quality, providing the community with a more stable foundation for advancing computer use agent research."/><meta name="twitter:image" content="https://imgur.com/a/Jh7SS0Q"/><meta name="next-head-count" content="17"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/c7a1138c526c7fb2.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c7a1138c526c7fb2.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-4d525326a36e0ad2.js" defer=""></script><script src="/_next/static/chunks/pages/_app-43f67b4afd5f2ce7.js" defer=""></script><script src="/_next/static/chunks/845-76c3d46a37112c52.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-83a52c83a6b0e812.js" defer=""></script><script src="/_next/static/o1hdWyFQiG2ICuDjpFlrw/_buildManifest.js" defer=""></script><script src="/_next/static/o1hdWyFQiG2ICuDjpFlrw/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&display=swap">@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtr6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtZ6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCu170w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCuM70w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next"><div class="fixed top-0 left-0 w-full h-14 md:h-20 bg-white py-4 z-10 navbar-shadow"><div class="page-x-width w-full flex justify-between items-center"><div class="sm:hidden w-fit h-fit cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" class="text-[#0156AC]" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><line x1="4" y1="6" x2="20" y2="6"></line><line x1="4" y1="12" x2="20" y2="12"></line><line x1="4" y1="18" x2="20" y2="18"></line></svg></div><a href="/"><div class="flex gap-2 items-center cursor-pointer text-brand-dark"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/logo.svg"/><div>XLANG Lab</div></div></a><ul class="gap-8 text-md text-text-brand-dark hidden sm:flex"><li class="font-[500] hover:underline text-brand-dark"><a href="/">about</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/team">team</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/publications">publications</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/blog">blogs</a></li></ul><div class="flex gap-4 items-center"><ul class="hidden lg:flex gap-3"><li><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github-black.svg"/></a></li><li><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter-black.svg"/></a></li></ul><div class="max-sm:text-sm border border-brand-primary2 border-2 text-brand-primary2 font-[500] rounded-xl py-1 px-3 cursor-pointer"><a href="https://forms.gle/3Ki9ectMB5D31F8g8" target="_blank" rel="noopener noreferrer">join us</a></div></div></div></div><div class="relative w-full h-full"><div class="absolute top-0 left-0 max-sm:hidden -mt-8 z-[-1]"><img alt="Wave" loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/background/wave.svg"/><img alt="Wave 2" loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/background/wave2.svg"/></div><div class="pt-36 w-full min-h-screen bg-[#D9D9D9]/20"><div class="page-x-width"><div class="flex flex-col gap-8 mb-8"><div class="text-xs text-[#545454] font-[500] tracking-widest"><a href="/blog">Blog</a> / <!-- -->OSWorld-Verified</div><div class="text-[#0156AC] font-[500] text-3xl text-justify">Introducing OSWorld-Verified</div><div class="flex flex-wrap flex-col w-full"><div class="relative w-full font-[600] text-xs flex flex-wrap grid grid-cols-3 mb-4"><div class="flex flex-col justify-center items-center"><div class="text-[#666666] mb-2">Author</div><div>XLANG Lab</div></div><div class="flex flex-col justify-center items-center"><div class="text-[#666666] mb-2">Date</div><div>Jul 28, 2025</div></div><div class="flex flex-col justify-center items-center"><div class="text-[#666666] mb-2">Share</div><ul class="flex gap-4"><li class="cursor-pointer"><a href="https://join.slack.com/t/xlanggroup/shared_invite/zt-20zb8hxas-eKSGJrbzHiPmrADCDX3_rQ"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/slack-black.svg"/></a></li><li class="cursor-pointer"><a href="https://github.com/xlang-ai/OSWorld"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github-black.svg"/></a></li><li class="cursor-pointer"><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter-black.svg"/></a></li></ul></div></div><div class="relative w-full aspect-video rounded-lg"><img alt="Introducing OSWorld-Verified" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;object-position:center top;color:transparent" src="/blog/osworld-verified/overview.png"/></div></div></div><div class="tracking-wide leading-7 mb-24 mt-6"><p class="mb-4 text-sm leading-7 text-justify">In April 2024, OSWorld&#x27;s initial version <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[1]</span> was released for the first controllable environment to benchmark computer use agents. Over the past year and more, we have been delighted and pleasantly surprised to witness the benchmark and environment&#x27;s driving impact on the community, the emergence of multiple related works, new directions and testing initiatives from tech giants like Anthropic <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[2]</span><span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[3]</span> and OpenAI <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[4]</span>, and the birth of new startups <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[5]</span><span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[6]</span><span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[7]</span>, applications and possibilities.</p>
<p class="mb-4 text-sm leading-7 text-justify">Throughout these 15 months, we have continuously invested in supporting features like Docker, parallelization and developing system images for more platforms, while collaborating with the community to address issues on an ongoing basis.</p>
<p class="mb-4 text-sm leading-7 text-justify">After 15 months, we are announcing a major improvement and refinement initiative. We have collected, verified, validated, and fixed 300+ pieces of feedback, involving approximately two months of dedicated effort from a ~10-person team. We are now launching the OSWorld-Verified and refined OSWorld examples, providing more authentic signals for evaluation and learning based on this foundation.</p>
<p class="mb-4 text-sm leading-7 text-justify">What follows are our insights from this refinement process, re-evaluation results and current state analysis, a retrospective on computer-use agent evaluation over the past year, and our outlook for the future of CUA evaluation.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>TL;DR</strong>: We&#x27;ve systematically addressed 300+ issues in OSWorld through a comprehensive refinement process. OSWorld-Verified delivers more reliable evaluation signals through improved infrastructure (VMware/Docker â†’ AWS, 50x parallelization) and enhanced task quality (fixed web changes, ambiguity, evaluation robustness). The benchmark remains challenging with significant room for improvement toward human-level performance.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Key Updates</strong>:</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Infrastructure</strong>: Migrated to AWS cloud for massive parallelization (10+ hours â†’ minutes)</li>
<li><strong>Task Quality</strong>: Fixed 300+ issues including web structure changes, instruction ambiguity, and evaluation robustness</li>
<li><strong>Evaluation</strong>: Established public evaluation platform for verified apple-to-apple comparisons</li>
<li><strong>Performance</strong>: Current leading models show success primarily stems from extensive human trajectory data</li>
</ul>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Impact</strong>: OSWorld-Verified provides the community with a more stable, scalable foundation for advancing computer use agent research and development.</p>
<hr class="my-6"/>
<h2 class="text-xl font-[600] my-6 text-justify">Acknowledgement</h2>
<p class="mb-4 text-sm leading-7 text-justify">Special thanks to the following institutions that provided feedback and participated in the fixes (as well as institutions that provided feedback during the process): <a href="https://www.moonshot.ai/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">MoonShot AI</a>ï¼Œ<a href="https://www.hud.so/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Human Data</a>, <a href="https://openai.com/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">OpenAI</a>, <a href="https://seed-tars.com/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">ByteDance Seed TARS</a>, <a href="https://www.anthropic.com/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Anthropic</a>, <a href="https://www.simular.ai/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Simular</a>, <a href="https://sites.google.com/view/chaoh" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">HKU Data Intelligence Lab</a></p>
<p class="mb-4 text-sm leading-7 text-justify">Special thanks to the following students who participated in the specific fixes: <a href="https://yuanmengqi.github.io/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Mengqi Yuan</a>, <a href="https://zdy023.github.io/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Danyang Zhang</a>, <a href="https://thisisxxz.com/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Xinzhuang Xiong</a>,  <a href="https://scholar.google.com/citations?user=JPwg5MwAAAAJ&amp;hl=en" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Zhennan Shen</a>, <a href="https://github.com/adlsdztony" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Zilong Zhou</a>, Yanxu Chen, <a href="https://millank0817.github.io/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Jiaqi Deng</a>, <a href="https://tianbaoxie.com/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Tianbao Xie</a>, Junda Chen, <a href="https://chenjix.github.io/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Jixuan Chen</a>, <a href="https://www.linkedin.com/in/haoyuan-wu-240878291/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Haoyuan Wu</a>.</p>
<p class="mb-4 text-sm leading-7 text-justify">Special thanks to the following students who participated in running the re-evaluation: <a href="https://yuanmengqi.github.io/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Mengqi Yuan</a>, <a href="https://github.com/adlsdztony" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Zilong Zhou</a>, <a href="https://xinyuanwangcs.github.io/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Xinyuan Wang</a>, <a href="https://bowenbryanwang.github.io/" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Bowen Wang</a>.</p>
<hr class="my-6"/>
<h2 class="text-xl font-[600] my-6 text-justify">Why an Update?</h2>
<p class="mb-4 text-sm leading-7 text-justify">Although OSWorld&#x27;s early infrastructure using virtual machines and Docker theoretically already provided a completely realistic and almost reproducible environment for decentralized evaluation distribution, and we dedicated over 400 man-hours for four rounds of checks before the first release in April 2024, with continuous maintenance investing hundreds of additional hours to ensure quality, we still collected approximately 300 issues pointed out by several institutions.</p>
<p class="mb-4 text-sm leading-7 text-justify">This made us deeply realize a lesson: <strong>providing reliable rewards consumes more human resources than we imagined</strong>.</p>
<p class="mb-4 text-sm leading-7 text-justify">Next, we&#x27;ll discuss the uncontrollable factors we discovered in actual operations that potentially cause this benchmark&#x27;s signal to gradually weaken and become chaotic over time:</p>
<h3 class="text-lg font-[600] my-6 text-justify">Unexpected Uncontrollable Uncertainty in (Certain) Environments</h3>
<h4>Environment Instability and Changes</h4>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Anti-crawling mechanisms and CAPTCHAs</strong>: Google search, shopping websites showing &quot;blocked by the website as robot&quot;, verification challenges</li>
<li><strong>Network access restrictions</strong>: 403 IP blocking issues (Steam connection timeout, NBA.com geo-restrictions)</li>
<li><strong>Dynamic content changes</strong>: Website UI overhauls causing DOM structure changes
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li>e.g., Apple comparison page URL parameter changes, Budget.com introducing CAPTCHAs from some point</li>
<li>e.g., Car rental sites (Rentalcars.com) implementing lazy loading and encoding changes</li>
</ul>
</li>
<li><strong>Web page structure modifications</strong>: speedtest.net CSV export exists before while now be deleted, so the task is actually modified</li>
<li><strong>URL and content evolution</strong>: Chrome settings URLs changing, airline sites modifying search interfaces</li>
</ul>
<h4>Fragile Dependencies with Timing Issues</h4>
<p class="mb-4 text-sm leading-7 text-justify">e.g., Some tasks&#x27; config and postconfig rely on &quot;hardcoded&quot; operations, such as &quot;Copy the screenshot 1.png from the desktop to where my cursor is located.&quot; Our configuration uses <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">pyautogui.press(&quot;down&quot;, presses=8, interval=0.01)</code> to move the cursor to the 9th line, which requires LibreOffice Writer to be fully loaded with the document open and cursor blinking at the first line when executing this command. The previous fragile dependencies couldn&#x27;t guarantee sequential execution, causing initial setup issues in some tasks.</p>
<h3 class="text-lg font-[600] my-6 text-justify">Incompleteness of Initial Tasks Annotation</h3>
<h4>Task Ambiguity</h4>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Subjective interpretation requirements</strong>:</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li>e.g., &quot;Purple/Red/Green background&quot; having multiple valid interpretations (light purple, dark purple, etc.)</li>
<li>e.g., &quot;Fill all blank cells&quot; unclear whether referring to all empty cells or specific ranges</li>
</ul>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Unclear scope definitions</strong>:</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li>e.g., &quot;Resize image&quot; vs &quot;resize layer&quot; confusion in GIMP tasks</li>
<li>e.g., When a user says &#x27;Switch off Bluetooth&#x27; while Bluetooth is already unavailable (meaning the Bluetooth button cannot be turned on), agents can be confused about whether this should be considered a success or deemed infeasible</li>
</ul>
<h4>Multiple Solution Approaches</h4>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Alternative valid methods</strong>: Models finding different but correct paths to achieve goals</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li>e.g., Task instruction is to concatenate two .csv files without specifying how to handle header rows during concatenation, so we must consider every valid concatenation approach and award full points if any method works.</li>
</ul>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Creative problem-solving</strong>: Models using extensions or workarounds not anticipated in ground truth</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li>e.g., VS Code background customization via extensions when built-in options are insufficient, while we didn&#x27;t think about this option and marked the task as infeasible.</li>
<li>e.g., Multi-app workflows when single-app solutions are expected, while we didn&#x27;t think about this option and marked the task as infeasible.</li>
</ul>
<p class="mb-4 text-sm leading-7 text-justify"><strong>False negatives from limited ground truth</strong>:</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li>e.g., &quot;Change the first two paragraphs to double line spacing&quot; - the empty line between the two paragraphs can either be set to double spacing or left unchanged; both approaches should be considered correct.</li>
<li>e.g., Different but functionally equivalent spreadsheet formulas marked incorrect</li>
</ul>
<h3 class="text-lg font-[600] my-6 text-justify">Decentralized Evaluation Reduces Motivation to Contribute Error Discovery</h3>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Lack of motivation to provide feedback</strong>: After discovering problems, most people have no motivation to provide feedback to OSWorld for us to make corrections. This leads to gradual evolution where everyone has their own environment, or even modifies tasks for evaluation to achieve higher scores, making scores increasingly opaque and incomparable.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>No benefit from submitting modifications</strong>: We also see that people hold similar attitudes toward other benchmarks. This is an inherent problem with decentralized benchmarks.</p>
<p class="mb-4 text-sm leading-7 text-justify">In summary, to advance the development of this field, we hope everyone can achieve apple-to-apple comparisons. Therefore, we hope the community will actively participate in OSWorld-Verified public evaluation while conducting their own tests, running agents on a unified platform for verification. We have made improvements in both infrastructure and tasks.</p>
<hr class="my-6"/>
<h2 class="text-xl font-[600] my-6 text-justify">Infrastructure â€” From VMware to AWS: The Journey of CUA Environment Infrastructure</h2>
<p class="mb-4 text-sm leading-7 text-justify">The distribution methods for benchmark datasets (similarly, training data) have continuously evolved over time - from being shared through academic papers and word-of-mouth, to university-hosted servers (like Penn Treebank), to GitHub repositories, and now to platforms like Hugging Face datasets as the primary distribution medium.</p>
<p class="mb-4 text-sm leading-7 text-justify">Computer use agent development environments present unique complexities (after all, we&#x27;re enabling agents to operate on computers). Under these conditions, achieving apple-to-apple comparisons becomes increasingly challenging, leading to seemingly absurd situations where machine performance correlates with evaluation results.</p>
<p class="mb-4 text-sm leading-7 text-justify">We have been continuously searching for the technical optimal solution. Our initial attempt involved building controlled environments using VMware, where we distributed <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">.vmdk</code> files and <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">.vmx</code> configuration files, saving images during first-time execution on users&#x27; computers. However, this approach had significant drawbacks: despite being theoretically parallelizable, the software consumed substantial personal computer resources with excessive runtime. Additionally, VMware became increasingly closed and cumbersome to download after Broadcom&#x27;s acquisition.</p>
<p class="mb-4 text-sm leading-7 text-justify">Inspired by the dokur project series (https://github.com/dockur/windows), we integrated Docker containerization technology 2024 Summer, utilizing an open-source VMware-like service - QEMU - running within Docker containers to execute virtual machines. This approach enabled us to achieve multi-environment parallelization on a single server, running 8 or even 16 environments simultaneously for experiments, though still constrained by server performance. We also implemented AWS support during this period but didn&#x27;t pursue it extensively.</p>
<p class="mb-4 text-sm leading-7 text-justify">Later, WindowsAgentArena <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[8]</span> (whose leading authors founded the c/ua company) left a profound impression on us by leveraging cloud services for parallelization, compressing evaluation time from 10+ hours to just 20 minutes. We think it is the right direction, while enhancement can always be done. So we actually followed WindowsAgentArena&#x27;s approach by leveraging AWS as cloud services and extended this feature in OSWorld infrastructure, which enables us to run up to 50 environments simultaneously and shorten evaluation time to minutes while ensuring comparability across evaluations.</p>
<hr class="my-6"/>
<h2 class="text-xl font-[600] my-6 text-justify">Tasks â€” Practice for Repairing Task Signals Manually at Scale: Embracing Change and Ambiguity</h2>
<p class="mb-4 text-sm leading-7 text-justify">Over the past month we reviewed and manually fixed nearly 300 feedback items collected from every accessible community channelâ€”Moonshot AI, HUD, OpenAI, ByteDance, Anthropic, and Simular AI, etc. We&#x27;ve compiled the detailed checks and modifications into the following spreadsheet <a href="https://docs.google.com/spreadsheets/d/19GSOVCtYM7j3V84Zl5QiaeiEtgK_NIvqtDVXEoVb4U0/edit?gid=0#gid=0" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">OSWorld fix log</a>.</p>
<p class="mb-4 text-sm leading-7 text-justify">For tasks we identified as genuinely problematic, we primarily modified only the evaluators to minimize changes to the tasks themselves and maintain score continuity. We only adjusted task descriptions when absolutely necessary.</p>
<h3 class="text-lg font-[600] my-6 text-justify">Major Categories of Issues Addressed</h3>
<h4>Web Structure and Content Changes</h4>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Problem</strong>: Websites frequently change their HTML structure, CSS classes, and content, causing evaluation functions to fail or become unreliable.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Solutions Implemented</strong>:</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Updated HTML parsing functions</strong>: Modified <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">get_active_tab_html_parse</code> and similar functions to work with current website structures</li>
<li><strong>URL-based validation</strong>: Changed from HTML structure validation to URL field validation (e.g., modifying <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">get_rule_relativeTime</code> function)</li>
<li><strong>Fuzzy matching</strong>: Added <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">is_expected_active_tab_approximate</code> function for approximate URL comparisons</li>
<li><strong>Environment change marks</strong>: Added <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">possibility_of_env_change</code> field to flag volatile tasks</li>
</ul>
<h4>Anti-Crawling and Access Issues</h4>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Problem</strong>: Websites blocking automated access through CAPTCHA, IP restrictions, or bot detection.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Solutions Deployed</strong>:</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Proxy infrastructure</strong>: Added <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">proxy</code> field support for websites with aggressive anti-crawling</li>
<li><strong>Alternative website selection</strong>: For heavily protected sites (e.g., SeatGeek â†’ Ticketek, TripAdvisor proxy issues), switched to functionally equivalent alternatives</li>
<li><strong>Fixed IP allocation</strong>: Implemented single-IP assignment for tasks requiring human verification</li>
<li><strong>Simulated environments (WIP)</strong>: Created controlled web pages for tasks affected by external dependencies</li>
</ul>
<h4>Time-Sensitive and Booking Tasks</h4>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Problem</strong>: Tasks involving future dates, reservations, or time-sensitive content become invalid over time.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Solutions Implemented</strong>:</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Extended booking windows</strong>: Changed booking periods from 4 months to 8+ months for availability</li>
<li><strong>Updated target locations</strong>: Modified unavailable venues (e.g., Albion Basin â†’ Diamond for reservations)</li>
<li><strong>Instruction clarification</strong>: Added specific time qualifications and date ranges</li>
<li><strong>Dynamic date handling</strong>: Improved relative time calculations in evaluation functions</li>
</ul>
<h4>Instruction Ambiguity and Clarity</h4>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Problem</strong>: Vague or ambiguous instructions leading to multiple valid interpretations and evaluation inconsistencies.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Specific Improvements</strong>:</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>File format specifications</strong>: Added explicit format requirements (e.g., &quot;.png format using only GIMP&#x27;s built-in features&quot;)</li>
<li><strong>Path clarifications</strong>: Specified exact file paths and naming conventions</li>
<li><strong>Action sequences</strong>: Clarified step-by-step requirements (e.g., &quot;batch process all images&quot; vs &quot;adjust individually&quot;)</li>
<li><strong>Scope limitations</strong>: Added restrictions like &quot;without launching separate web browser&quot; or &quot;using only built-in features&quot;</li>
<li><strong>Quantitative specifications</strong>: Added precise measurements, percentages, and ranges</li>
<li><strong>Consider more options</strong>: Instead of modifying instruction. Add more options into gold solutions, and give full marks once one is satisfied</li>
</ul>
<h4>Evaluation Function Robustness</h4>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Problem</strong>: Overly strict or inadequate evaluation functions causing incorrect scoring.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Major Enhancements</strong>:</p>
<h5>File and Content Comparison</h5>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Fuzzy matching for documents</strong>: Added <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">compare_docx_files</code> with 0-1 scoring instead of binary</li>
<li><strong>Image similarity improvements</strong>: Implemented perceptual hashing algorithms for <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">compare_pdf_images</code> to ignore minor visual differences</li>
<li><strong>PDF content validation</strong>: Enhanced PDF comparison with margin and formatting tolerance</li>
<li><strong>EPUB accuracy calculation</strong>: Fixed evaluation logic using averaging instead of problematic multiplication</li>
</ul>
<h5>Spreadsheet and Data Tasks</h5>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Cell format tolerance</strong>: Ignored formatting differences that don&#x27;t affect content (e.g., &quot;$&quot; signs in currency fields)</li>
<li><strong>Range specifications</strong>: Added precise cell range definitions for operations</li>
<li><strong>Conditional formatting</strong>: Improved detection of styling changes vs content changes</li>
<li><strong>Formula vs value comparison</strong>: Enhanced logic to handle both calculated and manually entered data</li>
</ul>
<h5>GUI and Application Tasks</h5>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Color tolerance</strong>: Implemented acceptable color variation ranges for visual elements</li>
<li><strong>Font and formatting flexibility</strong>: Reduced strict requirements on default formatting while maintaining core functionality</li>
<li><strong>Unit conversion handling</strong>: Added proper handling for measurement units (cm, px, etc.)</li>
<li><strong>Case sensitivity management</strong>: Made appropriate distinctions between case-sensitive and case-insensitive comparisons</li>
</ul>
<h4>System and Environment Stability</h4>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Problem</strong>: Tasks failing due to system-level issues, timing problems, or environment inconsistencies.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Infrastructure Improvements</strong>:</p>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Blocking vs non-blocking operations</strong>: Converted <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">open_file</code> functions from fire-and-forget to synchronous blocking operations</li>
<li><strong>Sleep timing optimization</strong>: Iteratively adjusted inter-action delays for different machine types (<code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">t3.medium</code>, <code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">t3.large</code>)</li>
<li><strong>VM Image Optimization</strong>: We compressed the virtual machine images (from 50GB down to 25GB) and adjusted the appropriate IOPS ratios to address the slow first-boot performance caused by lazy loading during AWS AMI/EBS snapshot transfers (LibreOffice taking 1 minute to open on first launch was unacceptable), while also reducing costs</li>
<li><strong>Font installation</strong>: Install all fonts for LibreOffice Impress and Writer tasks to avoid formatting issues caused by missing font files.</li>
<li><strong>Audio/video handling</strong>: Added proper audio device configuration (<code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">-aout=dummy</code> for VLC) and format validation</li>
<li><strong>Hosting Location</strong>: We move all files (including init state files used to build the environment&#x27;s initial state and gold files for result validation) from Google Drive to Hugging Face. Using Google Drive for file hosting proved to be a poor choice ðŸ™ƒ- whenever download counts became too high, it would temporarily block all downloads for everyone, causing evaluation issues. We have migrated all files to Hugging Face datasets for smooth distribution</li>
</ul>
<h3 class="text-lg font-[600] my-6 text-justify">Takeaways for Future Benchmark Construction</h3>
<h4>What We Would Do Differently</h4>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Proactive monitoring</strong>: Implement webhook-like systems for continuous task health monitoring</li>
<li><strong>Simulated environments</strong>: Build more controlled environments for tasks prone to external changes</li>
<li><strong>Dual-track evaluation</strong>: Maintain both decentralized and centralized evaluation systems</li>
<li><strong>Resource planning</strong>: Budget significantly more time and human resources for ongoing maintenance. Even very daily tasks can contain significant amounts of possible approaches than we can expect at the beginning</li>
</ul>
<h4>Best Practices Established</h4>
<ul class="list-disc pl-4 text-sm leading-7 text-justify">
<li><strong>Minimal invasive changes</strong>: Prioritize evaluation fixes over task modifications</li>
<li><strong>Comprehensive documentation</strong>: Maintain detailed logs of all issues and resolutions</li>
<li><strong>Community integration</strong>: Establish clear channels for issue reporting and collaborative fixes</li>
<li><strong>Continuous improvement</strong>: Regular review cycles with stakeholder feedback integration</li>
</ul>
<p class="mb-4 text-sm leading-7 text-justify">This comprehensive repair effort has significantly improved the reliability and validity of the OSWorld benchmark while establishing processes for ongoing maintenance and improvement.</p>
<hr class="my-6"/>
<h2 class="text-xl font-[600] my-6 text-justify">Based on the Rerun Results, How Far Are We From Completing OSWorld?</h2>
<h3 class="text-lg font-[600] my-6 text-justify">Our Implementation</h3>
<p class="mb-4 text-sm leading-7 text-justify">For Agent implementation, we have added implementations of Operator, Claude, UI-TARS <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[9]</span>, Qwen2.5-VL <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[10]</span>, Seed-1.5-VL <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[11]</span>, OpenCUA <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[12]</span> and other models based on the existing agents implementation.
We have also integrated advanced agent implementations based on agentic frameworks, such as Agent S<span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[13]</span>, Jedi<span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[14]</span>, GTA1<span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[15]</span>, and others.
All agent implementations are placed under <a href="https://github.com/xlang-ai/OSWorld/tree/main/mm_agents" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">mm_agents folder of OSWorld repo</a>.</p>
<p class="mb-4 text-sm leading-7 text-justify">In the implementation, since many of the above models are not publicly available, for Operator and Claude models, we extensively referenced the implementation of Computer Use Agent Arena <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[16]</span>, and referenced some usage examples from these providers <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[17]</span><span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[18]</span><span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[19]</span>.
We also conducted repeated validation and calibration of agent performance through preliminary experiments for confirmation.</p>
<h3 class="text-lg font-[600] my-6 text-justify">Key Insights from OSWorld-Verified Results</h3>
<p class="mb-4 text-sm leading-7 text-justify">We ran experiments for each model under different step settings. Here are some takeaways:</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>OSWorld remains far from saturated with substantial headroom to human-level performance.</strong> Despite impressive gains, the benchmark still presents significant challenges. With human performance estimated at ~72% from our original study, even the best current systems are only reaching about 75% of human capability. The performance distribution shows clear tiers: agentic frameworks (45-53%), strong foundation models (35-44%), and specialized models (25-40%), with substantial gaps between each tier. This indicates that OSWorld continues to provide meaningful signal for model development, particularly in areas requiring complex multi-step reasoning, error recovery, and adaptation to interface changes.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>Agentic frameworks with reasoning models dominate the leaderboard.</strong> Agentic frameworks powered by reasoning models like o3 have achieved breakthrough performance. GTA1 w/ o3 reaches 53.1% and Jedi-7B w/ o3 achieves 51.0%, representing a remarkable 4.3x improvement over the original OSWorld best result of 12.24%. This demonstrates that sophisticated orchestration layers can dramatically amplify the capabilities of reasoning models, even when those models weren&#x27;t specifically trained for computer use tasks. Interestingly, highlighting the importance of computational resources in achieving strong performance.</p>
<p class="mb-4 text-sm leading-7 text-justify"><strong>General models&#x27; capability improvements and reasoning enhancements show exceptional progress in computer use capabilities.</strong> Among foundation models, Claude 4 Sonnet stands out with 43.9% performance, significantly outperforming other general-purpose models and even approaching specialized computer use models like UI-TARS (40.0%). o3&#x27;s performance varies drastically with step budget (9.1% to 23.0%), compared with 5% of GPT-4o, not to mention further integration with grounding model improvements. This all suggests that better pretraining and post-training bringing general scaling capabilities and reasoning abilities (even without any computer-use specific purpose) will potentially help improve computer use agents.</p>
<p class="mb-4 text-sm leading-7 text-justify">Based on these results, we updated the leaderboard by adding a verified section and setting it as the default display, while also adding links to computer agent arena scores. For future model submissions, we will continue to serve as a verification platform, consistently open-sourcing agent implementations, execution code, reproducible results, and generated trajectories to help the community gain further insights into current capability boundaries and continuously provide reliable evaluation signals.</p>
<hr class="my-6"/>
<h2 class="text-xl font-[600] my-6 text-justify">How to Use and Submit New Results on the Leaderboard</h2>
<p class="mb-4 text-sm leading-7 text-justify">You can always still use providers like VMware and Docker with the new task suites.</p>
<p class="mb-4 text-sm leading-7 text-justify">Meanwhile, we have provided a detailed guide on using our AWS-based <a href="https://github.com/xlang-ai/OSWorld/blob/main/PUBLIC_EVALUATION_GUIDELINE.md" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Public Evaluation platform</a>. You can set up and run your OSWorld-Verified tests on this more controllable platform.</p>
<p class="mb-4 text-sm leading-7 text-justify">If you want your results to be verified and displayed on the verified leaderboard, you need to schedule a meeting with us (current maintainers: tianbaoxiexxx@gmail.com, yuanmengqi732@gmail.com) to run your agent code on our side and have us report the results.</p>
<p class="mb-4 text-sm leading-7 text-justify">You need to upload and allow us to disclose your agent implementation under the OSWorld framework (you may choose not to expose your model API to the public), along with a report that allows the public to understand what&#x27;s happening behind the scenes.</p>
<p class="mb-4 text-sm leading-7 text-justify">Alternatively, if you are from a trusted institution, you can share your monitoring data and trajectories with us.</p>
<p class="mb-4 text-sm leading-7 text-justify">Please carefully follow the <a href="https://github.com/xlang-ai/OSWorld/blob/main/PUBLIC_EVALUATION_GUIDELINE.md" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify">Public Evaluation Guideline</a> to get results.</p>
<hr class="my-6"/>
<h2 class="text-xl font-[600] my-6 text-justify">Key Insights for the Community: Next Steps Forward</h2>
<h3 class="text-lg font-[600] my-6 text-justify">Scaling Computer Use Data Collection and Training</h3>
<p class="mb-4 text-sm leading-7 text-justify">Our analysis of UI-TARS and OpenCUA, currently the most successful models with publicly available technical details, reveals that their success stems primarily from extensive human computer use trajectory data. The existing publicly available trajectory datasets remain insufficient in scale, predominantly focusing on Mobile and Web Agent domains with limited action spaces, indicating that research in this area is still in its early stages <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[17]</span><span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[18]</span><span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[19]</span>.</p>
<p class="mb-4 text-sm leading-7 text-justify">There is substantial room for exploration in data collection methodologies (including collection tools, annotation frameworks, and potentially viable commercial models), synthetic data generation approaches, efficient utilization strategies, and optimal integration practices across different training phases. Current evidence suggests that diverse, high-quality computer use data, combined with existing model architectures, is sufficient to enable models to develop robust computer use capabilities. Further scaling could involve collecting large volumes of unlabeled trajectories for pretraining phases, while providing more effective signals during post-training to enhance instruction-following capabilities in computer use scenarios.</p>
<h3 class="text-lg font-[600] my-6 text-justify">Scaling Post-Training Signal Enhancement</h3>
<p class="mb-4 text-sm leading-7 text-justify">Jason Wei&#x27;s blog post on verification asymmetry and its implications for reinforcement learning <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[20]</span> provides valuable insights that align with OSWorld&#x27;s approach to leveraging verification asymmetry for signal generation. For broader-scope computer use domains, we encompass both test case verification (similar to SWE-bench) and verification capabilities found in competition mathematics and open-domain QA (such as AIME and GAIA). We collectively term this &quot;verifiable code&quot; or &quot;(dense) reward code,&quot; which can often be automatically generated <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[21]</span><span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[22]</span>.</p>
<p class="mb-4 text-sm leading-7 text-justify">Current models lack robust capabilities for processing trajectories, particularly multimodal ones, making correctness assessment challenging (though human evaluation of computer use actions remains relatively straightforward). If humans establish appropriate scaffolding with well-designed verification code, combined with human-generated operational results for additional validation support, the difficulty of assessment could be significantly reduced or even eliminated. However, building such scaffolding requires domain expertise, and iterating through multiple problem-solving approaches demands practical experience. Our goal should be to advance models to a position where they can autonomously construct this scaffoldingâ€”a process requiring iteration but representing our intended direction.</p>
<p class="mb-4 text-sm leading-7 text-justify">We propose leveraging the gap in all domains where max(human verification capability, model verification capability) &gt; model generation capability to reinforce or filter synthetically generated data for capability enhancement. This process can be substantially automated or semi-automated <span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[23]</span><span class="text-sm font-medium text-blue-600 bg-blue-50 px-1 py-0.5 rounded">[24]</span>, with existing academic research offering significant opportunities for industrial-scale implementation. Additionally, for existing verifiable rewards, we can build upon current infrastructure to consider process-based approaches for denser reward provision, thereby improving learning efficiency for suitable tasks where critical milestones can be clearly defined.</p>
<h3 class="text-lg font-[600] my-6 text-justify">More Realistic and Controllable Evaluation</h3>
<p class="mb-4 text-sm leading-7 text-justify">In OSWorld&#x27;s first iteration, we focused on defining task frameworks for computer use agents, standardizing action states, and establishing evaluation environment infrastructure. Through engineering practice, we explored virtualization-based environment construction and backend-driven action execution methodologies. Due to time and resource constraints, we selected commonly used software applications and avoided over-annotating tasks requiring extended completion times, deep professional software expertise, video understanding components, real-time intensive operations, and asynchronous completion scenarios.</p>
<p class="mb-4 text-sm leading-7 text-justify">Our team is actively working toward releasing an updated version before the end of this fall, addressing these limitations and expanding the evaluation scope.</p>
<p class="mb-4 text-sm leading-7 text-justify">Looking forward, we envision building the future of digital intelligent agents, providing novel interfaces that liberate human productivity and transform how we interact with computing environments. The convergence of scaled trajectory data, enhanced verification mechanisms, and more comprehensive evaluation frameworks will be instrumental in realizing this vision of autonomous computer use capabilities.</p>
<p class="mb-4 text-sm leading-7 text-justify">Thanks for getting this far.</p>
<hr class="my-6"/>
<h2 class="text-xl font-[600] my-6 text-justify">References</h2>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[1] Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., ... &amp; Yu, T. (2024). Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37, 52040-52094.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[2] Anthopic, P. B. C. (2024, October). Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku. https://www.anthropic.com/news/3-5-models-and-computer-use</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[3] Anthropic, C. (2025). 3.7 sonnet and claude code. https://www.anthropic.com/news/claude-3-7-sonnet</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[4] OpenAI Operator Team, Computer-Using Agent: Powering Operator with Computer-Using Agent, a universal interface for AI to interact with the digital world. https://openai.com/index/computer-using-agent/</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[5] Get started with C/ua, https://www.trycua.com/</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[6] The evaluation platform for computer use agents, https://www.hud.so/</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[7] A computer for your AI, https://scrapybara.com/</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[8] Bonatti, R., Zhao, D., Bonacci, F., Dupont, D., Abdali, S., Li, Y., ... &amp; Hui, Z. (2024). Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[9] Qin, Y., Ye, Y., Fang, J., Wang, H., Liang, S., Tian, S., ... &amp; Shi, G. (2025). Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[10] Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., ... &amp; Lin, J. (2025). Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[11] Guo, D., Wu, F., Zhu, F., Leng, F., Shi, G., Chen, H., ... &amp; Wang, W. (2025). Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[12] Wang, X., Wang, B., Lu, D., Yang, J., Xie, T., Wang, J., Deng, J., Guo, X., Xu, Y., Wu, C. H., Shen, Z., Li, Z., Li, R., Li, X., Chen, J., Zheng, B., Li, P., Lei, F., Cao, R., Fu, Y., Shin, D., Shin, M., Hu, J., Wang, Y., Chen, J., Ye, Y., Zhang, D., Wang, Y., Wang, H., Yang, D., Zhong, V., Charles, Y., Yang, Z., &amp; Yu, T. (2025). OpenCUA: Open foundations for computer-use agents. arXiv preprint. https://opencua.xlang.ai/</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[13] Agashe, S., Wong, K., Tu, V., Yang, J., Li, A., &amp; Wang, X. E. (2025). Agent s2: A compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[14] Xie, T., Deng, J., Li, X., Yang, J., Wu, H., Chen, J., ... &amp; Xiong, C. (2025). Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis. arXiv preprint arXiv:2505.13227.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[15] Yang, Y., Li, D., Dai, Y., Yang, Y., Luo, Z., Zhao, Z., ... &amp; Li, J. (2025). GTA1: GUI Test-time Scaling Agent. arXiv preprint arXiv:2507.05791.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[16] Wang, B., Wang, X., Deng, J., Zhong, V., Yu, T., Xie, T., Li, R., Zhang, Y., Li, G., Toh, J. H., Wang, Z., Zhang, Y., Su, Y., &amp; Yang, D. (2025). Computer Agent Arena: An open-source online platform evaluating computer agents in real computer environments. XLANG Lab. https://arena.xlang.ai/</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[17] OpenAI CUA Sample App: OpenAI. (2025). OpenAI CUA sample app [Computer software]. GitHub. https://github.com/openai/openai-cua-sample-app</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[18] OpenAI Computer Use Documentation: OpenAI. (2025). Computer use. In OpenAI Platform Documentation. https://platform.openai.com/docs/guides/tools-computer-use</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[19] Anthropic Computer Use Demo: Anthropic. (2025). Computer use demo [Computer software]. GitHub. https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[20] Xu, Y., Wang, Z., Wang, J., Lu, D., Xie, T., Saha, A., ... &amp; Xiong, C. (2024). Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[21] Xu, Y., Lu, D., Shen, Z., Wang, J., Wang, Z., Mao, Y., ... &amp; Yu, T. (2024). Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. arXiv preprint arXiv:2412.09605.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[22] Xie, J., Xu, D., Zhao, X., &amp; Song, D. (2025). AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents. arXiv preprint arXiv:2506.14205.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[23] Jason Wei, Asymmetry of verification and verifier&#x27;s law, 2025, https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[24] Xie, T., Zhao, S., Wu, C. H., Liu, Y., Luo, Q., Zhong, V., ... &amp; Yu, T. (2023). Text2reward: Reward shaping with language models for reinforcement learning. arXiv preprint arXiv:2309.11489.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[25] Ma, Y. J., Liang, W., Wang, G., Huang, D. A., Bastani, O., Jayaraman, D., ... &amp; Anandkumar, A. (2023). Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[26] Zhong, R., Snell, C., Klein, D., &amp; Eisner, J. (2022). Non-programmers can label programs indirectly via active examples: A case study with text-to-SQL. arXiv preprint arXiv:2205.12422.</p>
<p class="mb-4 text-sm leading-7 text-justify"><a href="" target="_blank" class="underline cursor-pointer hover:text-brand-primary2 text-justify"></a>[27] Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., ... &amp; Wu, J. (2023). Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390.</p>
<hr class="my-6"/>
<h2 class="text-xl font-[600] my-6 text-justify">Citation</h2>
<p class="mb-4 text-sm leading-7 text-justify">If you think this blog post and the content involved are helpful to you, please cite:</p>
<pre class="bg-gray-100 p-4 rounded-lg text-sm font-mono whitespace-pre-wrap overflow-x-auto my-4 leading-relaxed"><code class="bg-gray-100 text-red-600 px-1 py-0.5 rounded text-sm font-mono">@article{osworld_verified,
      title={Introducing OSWorld-Verified}, 
      author={Tianbao Xie and Mengqi Yuan and Danyang Zhang and Xinzhuang Xiong and Zhennan Shen and Zilong Zhou  and Xinyuan Wang and Yanxu Chen and Jiaqi Deng and Junda Chen and Bowen Wang and Haoyuan Wu and Jixuan Chen and Junli Wang and Dunjie Lu and Hao Hu and Tao Yu},
      year={2025},
}
</code></pre></div></div></div></div><div class="w-full bg-brand-offBlack p-4"><div class="page-x-width flex justify-center sm:justify-between flex-wrap gap-4"><div class="flex gap-2 items-center"><a href="/"><div class="relative"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/logo-white.svg"/></div></a><div class="text-white text-xs w-fit">Â© Copyright 2023 XLANG Lab. All right reserved.</div></div><nav><ul class="text-white flex gap-6"><li class="cursor-pointer"><a href="https://discord.com/invite/4Gnw7eTEZR"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/discord.svg"/></a></li><li class="cursor-pointer"><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github.svg"/></a></li><li class="cursor-pointer"><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter.svg"/></a></li></ul></nav></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"osworld-verified","content":"\nIn April 2024, OSWorld's initial version [[1]](#ref1) was released for the first controllable environment to benchmark computer use agents. Over the past year and more, we have been delighted and pleasantly surprised to witness the benchmark and environment's driving impact on the community, the emergence of multiple related works, new directions and testing initiatives from tech giants like Anthropic [[2]](#ref2)[[3]](#ref3) and OpenAI [[4]](#ref4), and the birth of new startups [[5]](#ref5)[[6]](#ref6)[[7]](#ref7), applications and possibilities.\n\nThroughout these 15 months, we have continuously invested in supporting features like Docker, parallelization and developing system images for more platforms, while collaborating with the community to address issues on an ongoing basis. \n\nAfter 15 months, we are announcing a major improvement and refinement initiative. We have collected, verified, validated, and fixed 300+ pieces of feedback, involving approximately two months of dedicated effort from a ~10-person team. We are now launching the OSWorld-Verified and refined OSWorld examples, providing more authentic signals for evaluation and learning based on this foundation.\n\nWhat follows are our insights from this refinement process, re-evaluation results and current state analysis, a retrospective on computer-use agent evaluation over the past year, and our outlook for the future of CUA evaluation.\n\n**TL;DR**: We've systematically addressed 300+ issues in OSWorld through a comprehensive refinement process. OSWorld-Verified delivers more reliable evaluation signals through improved infrastructure (VMware/Docker â†’ AWS, 50x parallelization) and enhanced task quality (fixed web changes, ambiguity, evaluation robustness). The benchmark remains challenging with significant room for improvement toward human-level performance.\n\n**Key Updates**:\n- **Infrastructure**: Migrated to AWS cloud for massive parallelization (10+ hours â†’ minutes)\n- **Task Quality**: Fixed 300+ issues including web structure changes, instruction ambiguity, and evaluation robustness\n- **Evaluation**: Established public evaluation platform for verified apple-to-apple comparisons\n- **Performance**: Current leading models show success primarily stems from extensive human trajectory data\n\n**Impact**: OSWorld-Verified provides the community with a more stable, scalable foundation for advancing computer use agent research and development.\n\n\u003chr class=\"solid\"\u003e\n\n## Acknowledgement\n\nSpecial thanks to the following institutions that provided feedback and participated in the fixes (as well as institutions that provided feedback during the process): [MoonShot AI](https://www.moonshot.ai/)ï¼Œ[Human Data](https://www.hud.so/), [OpenAI](https://openai.com/), [ByteDance Seed TARS](https://seed-tars.com/), [Anthropic](https://www.anthropic.com/), [Simular](https://www.simular.ai/), [HKU Data Intelligence Lab](https://sites.google.com/view/chaoh)\n\nSpecial thanks to the following students who participated in the specific fixes: [Mengqi Yuan](https://yuanmengqi.github.io/), [Danyang Zhang](https://zdy023.github.io/), [Xinzhuang Xiong](https://thisisxxz.com/),  [Zhennan Shen](https://scholar.google.com/citations?user=JPwg5MwAAAAJ\u0026hl=en), [Zilong Zhou](https://github.com/adlsdztony), Yanxu Chen, [Jiaqi Deng](https://millank0817.github.io/), [Tianbao Xie](https://tianbaoxie.com/), Junda Chen, [Jixuan Chen](https://chenjix.github.io/), [Haoyuan Wu](https://www.linkedin.com/in/haoyuan-wu-240878291/).\n\nSpecial thanks to the following students who participated in running the re-evaluation: [Mengqi Yuan](https://yuanmengqi.github.io/), [Zilong Zhou](https://github.com/adlsdztony), [Xinyuan Wang](https://xinyuanwangcs.github.io/), [Bowen Wang](https://bowenbryanwang.github.io/).\n\n\u003chr class=\"solid\"\u003e\n\n## Why an Update?\n\nAlthough OSWorld's early infrastructure using virtual machines and Docker theoretically already provided a completely realistic and almost reproducible environment for decentralized evaluation distribution, and we dedicated over 400 man-hours for four rounds of checks before the first release in April 2024, with continuous maintenance investing hundreds of additional hours to ensure quality, we still collected approximately 300 issues pointed out by several institutions. \n\nThis made us deeply realize a lesson: **providing reliable rewards consumes more human resources than we imagined**. \n\nNext, we'll discuss the uncontrollable factors we discovered in actual operations that potentially cause this benchmark's signal to gradually weaken and become chaotic over time:\n\n### Unexpected Uncontrollable Uncertainty in (Certain) Environments\n\n#### Environment Instability and Changes\n\n- **Anti-crawling mechanisms and CAPTCHAs**: Google search, shopping websites showing \"blocked by the website as robot\", verification challenges\n- **Network access restrictions**: 403 IP blocking issues (Steam connection timeout, NBA.com geo-restrictions)\n- **Dynamic content changes**: Website UI overhauls causing DOM structure changes\n  - e.g., Apple comparison page URL parameter changes, Budget.com introducing CAPTCHAs from some point\n  - e.g., Car rental sites (Rentalcars.com) implementing lazy loading and encoding changes\n- **Web page structure modifications**: speedtest.net CSV export exists before while now be deleted, so the task is actually modified\n- **URL and content evolution**: Chrome settings URLs changing, airline sites modifying search interfaces\n\n#### Fragile Dependencies with Timing Issues\n\ne.g., Some tasks' config and postconfig rely on \"hardcoded\" operations, such as \"Copy the screenshot 1.png from the desktop to where my cursor is located.\" Our configuration uses `pyautogui.press(\"down\", presses=8, interval=0.01)` to move the cursor to the 9th line, which requires LibreOffice Writer to be fully loaded with the document open and cursor blinking at the first line when executing this command. The previous fragile dependencies couldn't guarantee sequential execution, causing initial setup issues in some tasks.\n\n### Incompleteness of Initial Tasks Annotation\n\n#### Task Ambiguity\n\n**Subjective interpretation requirements**:\n- e.g., \"Purple/Red/Green background\" having multiple valid interpretations (light purple, dark purple, etc.)\n- e.g., \"Fill all blank cells\" unclear whether referring to all empty cells or specific ranges\n\n**Unclear scope definitions**:\n- e.g., \"Resize image\" vs \"resize layer\" confusion in GIMP tasks\n- e.g., When a user says 'Switch off Bluetooth' while Bluetooth is already unavailable (meaning the Bluetooth button cannot be turned on), agents can be confused about whether this should be considered a success or deemed infeasible\n\n#### Multiple Solution Approaches\n\n**Alternative valid methods**: Models finding different but correct paths to achieve goals\n- e.g., Task instruction is to concatenate two .csv files without specifying how to handle header rows during concatenation, so we must consider every valid concatenation approach and award full points if any method works.\n\n**Creative problem-solving**: Models using extensions or workarounds not anticipated in ground truth\n- e.g., VS Code background customization via extensions when built-in options are insufficient, while we didn't think about this option and marked the task as infeasible.\n- e.g., Multi-app workflows when single-app solutions are expected, while we didn't think about this option and marked the task as infeasible.\n\n**False negatives from limited ground truth**:\n- e.g., \"Change the first two paragraphs to double line spacing\" - the empty line between the two paragraphs can either be set to double spacing or left unchanged; both approaches should be considered correct.\n- e.g., Different but functionally equivalent spreadsheet formulas marked incorrect\n\n### Decentralized Evaluation Reduces Motivation to Contribute Error Discovery\n\n**Lack of motivation to provide feedback**: After discovering problems, most people have no motivation to provide feedback to OSWorld for us to make corrections. This leads to gradual evolution where everyone has their own environment, or even modifies tasks for evaluation to achieve higher scores, making scores increasingly opaque and incomparable.\n\n**No benefit from submitting modifications**: We also see that people hold similar attitudes toward other benchmarks. This is an inherent problem with decentralized benchmarks.\n\nIn summary, to advance the development of this field, we hope everyone can achieve apple-to-apple comparisons. Therefore, we hope the community will actively participate in OSWorld-Verified public evaluation while conducting their own tests, running agents on a unified platform for verification. We have made improvements in both infrastructure and tasks.\n\n\u003chr class=\"solid\"\u003e\n\n## Infrastructure â€” From VMware to AWS: The Journey of CUA Environment Infrastructure\n\nThe distribution methods for benchmark datasets (similarly, training data) have continuously evolved over time - from being shared through academic papers and word-of-mouth, to university-hosted servers (like Penn Treebank), to GitHub repositories, and now to platforms like Hugging Face datasets as the primary distribution medium. \n\nComputer use agent development environments present unique complexities (after all, we're enabling agents to operate on computers). Under these conditions, achieving apple-to-apple comparisons becomes increasingly challenging, leading to seemingly absurd situations where machine performance correlates with evaluation results.\n\nWe have been continuously searching for the technical optimal solution. Our initial attempt involved building controlled environments using VMware, where we distributed `.vmdk` files and `.vmx` configuration files, saving images during first-time execution on users' computers. However, this approach had significant drawbacks: despite being theoretically parallelizable, the software consumed substantial personal computer resources with excessive runtime. Additionally, VMware became increasingly closed and cumbersome to download after Broadcom's acquisition.\n\nInspired by the dokur project series (https://github.com/dockur/windows), we integrated Docker containerization technology 2024 Summer, utilizing an open-source VMware-like service - QEMU - running within Docker containers to execute virtual machines. This approach enabled us to achieve multi-environment parallelization on a single server, running 8 or even 16 environments simultaneously for experiments, though still constrained by server performance. We also implemented AWS support during this period but didn't pursue it extensively.\n\nLater, WindowsAgentArena [[8]](#ref8) (whose leading authors founded the c/ua company) left a profound impression on us by leveraging cloud services for parallelization, compressing evaluation time from 10+ hours to just 20 minutes. We think it is the right direction, while enhancement can always be done. So we actually followed WindowsAgentArena's approach by leveraging AWS as cloud services and extended this feature in OSWorld infrastructure, which enables us to run up to 50 environments simultaneously and shorten evaluation time to minutes while ensuring comparability across evaluations.\n\n\u003chr class=\"solid\"\u003e\n\n## Tasks â€” Practice for Repairing Task Signals Manually at Scale: Embracing Change and Ambiguity\n\nOver the past month we reviewed and manually fixed nearly 300 feedback items collected from every accessible community channelâ€”Moonshot AI, HUD, OpenAI, ByteDance, Anthropic, and Simular AI, etc. We've compiled the detailed checks and modifications into the following spreadsheet [OSWorld fix log](https://docs.google.com/spreadsheets/d/19GSOVCtYM7j3V84Zl5QiaeiEtgK_NIvqtDVXEoVb4U0/edit?gid=0#gid=0).\n\nFor tasks we identified as genuinely problematic, we primarily modified only the evaluators to minimize changes to the tasks themselves and maintain score continuity. We only adjusted task descriptions when absolutely necessary.\n\n### Major Categories of Issues Addressed\n\n#### Web Structure and Content Changes\n\n**Problem**: Websites frequently change their HTML structure, CSS classes, and content, causing evaluation functions to fail or become unreliable.\n\n**Solutions Implemented**:\n- **Updated HTML parsing functions**: Modified `get_active_tab_html_parse` and similar functions to work with current website structures\n- **URL-based validation**: Changed from HTML structure validation to URL field validation (e.g., modifying `get_rule_relativeTime` function)\n- **Fuzzy matching**: Added `is_expected_active_tab_approximate` function for approximate URL comparisons\n- **Environment change marks**: Added `possibility_of_env_change` field to flag volatile tasks\n\n#### Anti-Crawling and Access Issues\n\n**Problem**: Websites blocking automated access through CAPTCHA, IP restrictions, or bot detection.\n\n**Solutions Deployed**:\n- **Proxy infrastructure**: Added `proxy` field support for websites with aggressive anti-crawling\n- **Alternative website selection**: For heavily protected sites (e.g., SeatGeek â†’ Ticketek, TripAdvisor proxy issues), switched to functionally equivalent alternatives\n- **Fixed IP allocation**: Implemented single-IP assignment for tasks requiring human verification\n- **Simulated environments (WIP)**: Created controlled web pages for tasks affected by external dependencies\n\n#### Time-Sensitive and Booking Tasks\n\n**Problem**: Tasks involving future dates, reservations, or time-sensitive content become invalid over time.\n\n**Solutions Implemented**:\n- **Extended booking windows**: Changed booking periods from 4 months to 8+ months for availability\n- **Updated target locations**: Modified unavailable venues (e.g., Albion Basin â†’ Diamond for reservations)\n- **Instruction clarification**: Added specific time qualifications and date ranges\n- **Dynamic date handling**: Improved relative time calculations in evaluation functions\n\n#### Instruction Ambiguity and Clarity\n\n**Problem**: Vague or ambiguous instructions leading to multiple valid interpretations and evaluation inconsistencies.\n\n**Specific Improvements**:\n- **File format specifications**: Added explicit format requirements (e.g., \".png format using only GIMP's built-in features\")\n- **Path clarifications**: Specified exact file paths and naming conventions\n- **Action sequences**: Clarified step-by-step requirements (e.g., \"batch process all images\" vs \"adjust individually\")\n- **Scope limitations**: Added restrictions like \"without launching separate web browser\" or \"using only built-in features\"\n- **Quantitative specifications**: Added precise measurements, percentages, and ranges\n- **Consider more options**: Instead of modifying instruction. Add more options into gold solutions, and give full marks once one is satisfied\n\n#### Evaluation Function Robustness\n\n**Problem**: Overly strict or inadequate evaluation functions causing incorrect scoring.\n\n**Major Enhancements**:\n\n##### File and Content Comparison\n- **Fuzzy matching for documents**: Added `compare_docx_files` with 0-1 scoring instead of binary\n- **Image similarity improvements**: Implemented perceptual hashing algorithms for `compare_pdf_images` to ignore minor visual differences\n- **PDF content validation**: Enhanced PDF comparison with margin and formatting tolerance\n- **EPUB accuracy calculation**: Fixed evaluation logic using averaging instead of problematic multiplication\n\n##### Spreadsheet and Data Tasks\n- **Cell format tolerance**: Ignored formatting differences that don't affect content (e.g., \"$\" signs in currency fields)\n- **Range specifications**: Added precise cell range definitions for operations\n- **Conditional formatting**: Improved detection of styling changes vs content changes\n- **Formula vs value comparison**: Enhanced logic to handle both calculated and manually entered data\n\n##### GUI and Application Tasks\n- **Color tolerance**: Implemented acceptable color variation ranges for visual elements\n- **Font and formatting flexibility**: Reduced strict requirements on default formatting while maintaining core functionality\n- **Unit conversion handling**: Added proper handling for measurement units (cm, px, etc.)\n- **Case sensitivity management**: Made appropriate distinctions between case-sensitive and case-insensitive comparisons\n\n#### System and Environment Stability\n\n**Problem**: Tasks failing due to system-level issues, timing problems, or environment inconsistencies.\n\n**Infrastructure Improvements**:\n- **Blocking vs non-blocking operations**: Converted `open_file` functions from fire-and-forget to synchronous blocking operations\n- **Sleep timing optimization**: Iteratively adjusted inter-action delays for different machine types (`t3.medium`, `t3.large`)\n- **VM Image Optimization**: We compressed the virtual machine images (from 50GB down to 25GB) and adjusted the appropriate IOPS ratios to address the slow first-boot performance caused by lazy loading during AWS AMI/EBS snapshot transfers (LibreOffice taking 1 minute to open on first launch was unacceptable), while also reducing costs\n- **Font installation**: Install all fonts for LibreOffice Impress and Writer tasks to avoid formatting issues caused by missing font files.\n- **Audio/video handling**: Added proper audio device configuration (`-aout=dummy` for VLC) and format validation\n- **Hosting Location**: We move all files (including init state files used to build the environment's initial state and gold files for result validation) from Google Drive to Hugging Face. Using Google Drive for file hosting proved to be a poor choice ðŸ™ƒ- whenever download counts became too high, it would temporarily block all downloads for everyone, causing evaluation issues. We have migrated all files to Hugging Face datasets for smooth distribution\n\n### Takeaways for Future Benchmark Construction\n\n#### What We Would Do Differently\n- **Proactive monitoring**: Implement webhook-like systems for continuous task health monitoring\n- **Simulated environments**: Build more controlled environments for tasks prone to external changes\n- **Dual-track evaluation**: Maintain both decentralized and centralized evaluation systems\n- **Resource planning**: Budget significantly more time and human resources for ongoing maintenance. Even very daily tasks can contain significant amounts of possible approaches than we can expect at the beginning\n\n#### Best Practices Established\n- **Minimal invasive changes**: Prioritize evaluation fixes over task modifications\n- **Comprehensive documentation**: Maintain detailed logs of all issues and resolutions\n- **Community integration**: Establish clear channels for issue reporting and collaborative fixes\n- **Continuous improvement**: Regular review cycles with stakeholder feedback integration\n\nThis comprehensive repair effort has significantly improved the reliability and validity of the OSWorld benchmark while establishing processes for ongoing maintenance and improvement.\n\n\u003chr class=\"solid\"\u003e\n\n## Based on the Rerun Results, How Far Are We From Completing OSWorld?\n\n### Our Implementation\n\nFor Agent implementation, we have added implementations of Operator, Claude, UI-TARS [[9]](#ref9), Qwen2.5-VL [[10]](#ref10), Seed-1.5-VL [[11]](#ref11), OpenCUA [[12]](#ref12) and other models based on the existing agents implementation. \nWe have also integrated advanced agent implementations based on agentic frameworks, such as Agent S[[13]](#ref13), Jedi[[14]](#ref14), GTA1[[15]](#ref15), and others.\nAll agent implementations are placed under [mm_agents folder of OSWorld repo](https://github.com/xlang-ai/OSWorld/tree/main/mm_agents).\n\nIn the implementation, since many of the above models are not publicly available, for Operator and Claude models, we extensively referenced the implementation of Computer Use Agent Arena [[16]](#ref16), and referenced some usage examples from these providers [[17]](#ref17)[[18]](#ref18)[[19]](#ref19). \nWe also conducted repeated validation and calibration of agent performance through preliminary experiments for confirmation.\n\n### Key Insights from OSWorld-Verified Results\n\nWe ran experiments for each model under different step settings. Here are some takeaways:\n\n**OSWorld remains far from saturated with substantial headroom to human-level performance.** Despite impressive gains, the benchmark still presents significant challenges. With human performance estimated at ~72% from our original study, even the best current systems are only reaching about 75% of human capability. The performance distribution shows clear tiers: agentic frameworks (45-53%), strong foundation models (35-44%), and specialized models (25-40%), with substantial gaps between each tier. This indicates that OSWorld continues to provide meaningful signal for model development, particularly in areas requiring complex multi-step reasoning, error recovery, and adaptation to interface changes.\n\n**Agentic frameworks with reasoning models dominate the leaderboard.** Agentic frameworks powered by reasoning models like o3 have achieved breakthrough performance. GTA1 w/ o3 reaches 53.1% and Jedi-7B w/ o3 achieves 51.0%, representing a remarkable 4.3x improvement over the original OSWorld best result of 12.24%. This demonstrates that sophisticated orchestration layers can dramatically amplify the capabilities of reasoning models, even when those models weren't specifically trained for computer use tasks. Interestingly, highlighting the importance of computational resources in achieving strong performance.\n\n**General models' capability improvements and reasoning enhancements show exceptional progress in computer use capabilities.** Among foundation models, Claude 4 Sonnet stands out with 43.9% performance, significantly outperforming other general-purpose models and even approaching specialized computer use models like UI-TARS (40.0%). o3's performance varies drastically with step budget (9.1% to 23.0%), compared with 5% of GPT-4o, not to mention further integration with grounding model improvements. This all suggests that better pretraining and post-training bringing general scaling capabilities and reasoning abilities (even without any computer-use specific purpose) will potentially help improve computer use agents.\n\n\n\nBased on these results, we updated the leaderboard by adding a verified section and setting it as the default display, while also adding links to computer agent arena scores. For future model submissions, we will continue to serve as a verification platform, consistently open-sourcing agent implementations, execution code, reproducible results, and generated trajectories to help the community gain further insights into current capability boundaries and continuously provide reliable evaluation signals.\n\n\u003chr class=\"solid\"\u003e\n\n## How to Use and Submit New Results on the Leaderboard\n\nYou can always still use providers like VMware and Docker with the new task suites.\n\nMeanwhile, we have provided a detailed guide on using our AWS-based [Public Evaluation platform](https://github.com/xlang-ai/OSWorld/blob/main/PUBLIC_EVALUATION_GUIDELINE.md). You can set up and run your OSWorld-Verified tests on this more controllable platform. \n\nIf you want your results to be verified and displayed on the verified leaderboard, you need to schedule a meeting with us (current maintainers: tianbaoxiexxx@gmail.com, yuanmengqi732@gmail.com) to run your agent code on our side and have us report the results. \n\nYou need to upload and allow us to disclose your agent implementation under the OSWorld framework (you may choose not to expose your model API to the public), along with a report that allows the public to understand what's happening behind the scenes.\n\nAlternatively, if you are from a trusted institution, you can share your monitoring data and trajectories with us. \n\nPlease carefully follow the [Public Evaluation Guideline](https://github.com/xlang-ai/OSWorld/blob/main/PUBLIC_EVALUATION_GUIDELINE.md) to get results.\n\n\u003chr class=\"solid\"\u003e\n\n## Key Insights for the Community: Next Steps Forward\n\n### Scaling Computer Use Data Collection and Training\n\nOur analysis of UI-TARS and OpenCUA, currently the most successful models with publicly available technical details, reveals that their success stems primarily from extensive human computer use trajectory data. The existing publicly available trajectory datasets remain insufficient in scale, predominantly focusing on Mobile and Web Agent domains with limited action spaces, indicating that research in this area is still in its early stages [[17]](#ref17)[[18]](#ref18)[[19]](#ref19).\n\nThere is substantial room for exploration in data collection methodologies (including collection tools, annotation frameworks, and potentially viable commercial models), synthetic data generation approaches, efficient utilization strategies, and optimal integration practices across different training phases. Current evidence suggests that diverse, high-quality computer use data, combined with existing model architectures, is sufficient to enable models to develop robust computer use capabilities. Further scaling could involve collecting large volumes of unlabeled trajectories for pretraining phases, while providing more effective signals during post-training to enhance instruction-following capabilities in computer use scenarios.\n\n### Scaling Post-Training Signal Enhancement\n\nJason Wei's blog post on verification asymmetry and its implications for reinforcement learning [[20]](#ref20) provides valuable insights that align with OSWorld's approach to leveraging verification asymmetry for signal generation. For broader-scope computer use domains, we encompass both test case verification (similar to SWE-bench) and verification capabilities found in competition mathematics and open-domain QA (such as AIME and GAIA). We collectively term this \"verifiable code\" or \"(dense) reward code,\" which can often be automatically generated [[21]](#ref21)[[22]](#ref22).\n\nCurrent models lack robust capabilities for processing trajectories, particularly multimodal ones, making correctness assessment challenging (though human evaluation of computer use actions remains relatively straightforward). If humans establish appropriate scaffolding with well-designed verification code, combined with human-generated operational results for additional validation support, the difficulty of assessment could be significantly reduced or even eliminated. However, building such scaffolding requires domain expertise, and iterating through multiple problem-solving approaches demands practical experience. Our goal should be to advance models to a position where they can autonomously construct this scaffoldingâ€”a process requiring iteration but representing our intended direction.\n\nWe propose leveraging the gap in all domains where max(human verification capability, model verification capability) \u003e model generation capability to reinforce or filter synthetically generated data for capability enhancement. This process can be substantially automated or semi-automated [[23]](#ref23)[[24]](#ref24), with existing academic research offering significant opportunities for industrial-scale implementation. Additionally, for existing verifiable rewards, we can build upon current infrastructure to consider process-based approaches for denser reward provision, thereby improving learning efficiency for suitable tasks where critical milestones can be clearly defined.\n\n### More Realistic and Controllable Evaluation\n\nIn OSWorld's first iteration, we focused on defining task frameworks for computer use agents, standardizing action states, and establishing evaluation environment infrastructure. Through engineering practice, we explored virtualization-based environment construction and backend-driven action execution methodologies. Due to time and resource constraints, we selected commonly used software applications and avoided over-annotating tasks requiring extended completion times, deep professional software expertise, video understanding components, real-time intensive operations, and asynchronous completion scenarios.\n\nOur team is actively working toward releasing an updated version before the end of this fall, addressing these limitations and expanding the evaluation scope.\n\nLooking forward, we envision building the future of digital intelligent agents, providing novel interfaces that liberate human productivity and transform how we interact with computing environments. The convergence of scaled trajectory data, enhanced verification mechanisms, and more comprehensive evaluation frameworks will be instrumental in realizing this vision of autonomous computer use capabilities.\n\nThanks for getting this far.\n\n\u003chr class=\"solid\"\u003e\n\n## References\n\n\u003ca id=\"ref1\"\u003e\u003c/a\u003e[1] Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., ... \u0026 Yu, T. (2024). Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37, 52040-52094.\n\n\u003ca id=\"ref2\"\u003e\u003c/a\u003e[2] Anthopic, P. B. C. (2024, October). Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku. https://www.anthropic.com/news/3-5-models-and-computer-use\n\n\u003ca id=\"ref3\"\u003e\u003c/a\u003e[3] Anthropic, C. (2025). 3.7 sonnet and claude code. https://www.anthropic.com/news/claude-3-7-sonnet\n\n\u003ca id=\"ref4\"\u003e\u003c/a\u003e[4] OpenAI Operator Team, Computer-Using Agent: Powering Operator with Computer-Using Agent, a universal interface for AI to interact with the digital world. https://openai.com/index/computer-using-agent/\n\n\u003ca id=\"ref5\"\u003e\u003c/a\u003e[5] Get started with C/ua, https://www.trycua.com/\n\n\u003ca id=\"ref6\"\u003e\u003c/a\u003e[6] The evaluation platform for computer use agents, https://www.hud.so/\n\n\u003ca id=\"ref7\"\u003e\u003c/a\u003e[7] A computer for your AI, https://scrapybara.com/\n\n\u003ca id=\"ref8\"\u003e\u003c/a\u003e[8] Bonatti, R., Zhao, D., Bonacci, F., Dupont, D., Abdali, S., Li, Y., ... \u0026 Hui, Z. (2024). Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264.\n\n\u003ca id=\"ref9\"\u003e\u003c/a\u003e[9] Qin, Y., Ye, Y., Fang, J., Wang, H., Liang, S., Tian, S., ... \u0026 Shi, G. (2025). Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326.\n\n\u003ca id=\"ref10\"\u003e\u003c/a\u003e[10] Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., ... \u0026 Lin, J. (2025). Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923.\n\n\u003ca id=\"ref11\"\u003e\u003c/a\u003e[11] Guo, D., Wu, F., Zhu, F., Leng, F., Shi, G., Chen, H., ... \u0026 Wang, W. (2025). Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062.\n\n\u003ca id=\"ref12\"\u003e\u003c/a\u003e[12] Wang, X., Wang, B., Lu, D., Yang, J., Xie, T., Wang, J., Deng, J., Guo, X., Xu, Y., Wu, C. H., Shen, Z., Li, Z., Li, R., Li, X., Chen, J., Zheng, B., Li, P., Lei, F., Cao, R., Fu, Y., Shin, D., Shin, M., Hu, J., Wang, Y., Chen, J., Ye, Y., Zhang, D., Wang, Y., Wang, H., Yang, D., Zhong, V., Charles, Y., Yang, Z., \u0026 Yu, T. (2025). OpenCUA: Open foundations for computer-use agents. arXiv preprint. https://opencua.xlang.ai/\n\n\u003ca id=\"ref13\"\u003e\u003c/a\u003e[13] Agashe, S., Wong, K., Tu, V., Yang, J., Li, A., \u0026 Wang, X. E. (2025). Agent s2: A compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906.\n\n\u003ca id=\"ref14\"\u003e\u003c/a\u003e[14] Xie, T., Deng, J., Li, X., Yang, J., Wu, H., Chen, J., ... \u0026 Xiong, C. (2025). Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis. arXiv preprint arXiv:2505.13227.\n\n\u003ca id=\"ref15\"\u003e\u003c/a\u003e[15] Yang, Y., Li, D., Dai, Y., Yang, Y., Luo, Z., Zhao, Z., ... \u0026 Li, J. (2025). GTA1: GUI Test-time Scaling Agent. arXiv preprint arXiv:2507.05791.\n\n\u003ca id=\"ref16\"\u003e\u003c/a\u003e[16] Wang, B., Wang, X., Deng, J., Zhong, V., Yu, T., Xie, T., Li, R., Zhang, Y., Li, G., Toh, J. H., Wang, Z., Zhang, Y., Su, Y., \u0026 Yang, D. (2025). Computer Agent Arena: An open-source online platform evaluating computer agents in real computer environments. XLANG Lab. https://arena.xlang.ai/\n\n\u003ca id=\"ref17\"\u003e\u003c/a\u003e[17] OpenAI CUA Sample App: OpenAI. (2025). OpenAI CUA sample app [Computer software]. GitHub. https://github.com/openai/openai-cua-sample-app\n\n\u003ca id=\"ref18\"\u003e\u003c/a\u003e[18] OpenAI Computer Use Documentation: OpenAI. (2025). Computer use. In OpenAI Platform Documentation. https://platform.openai.com/docs/guides/tools-computer-use\n\n\u003ca id=\"ref19\"\u003e\u003c/a\u003e[19] Anthropic Computer Use Demo: Anthropic. (2025). Computer use demo [Computer software]. GitHub. https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo\n\n\u003ca id=\"ref20\"\u003e\u003c/a\u003e[20] Xu, Y., Wang, Z., Wang, J., Lu, D., Xie, T., Saha, A., ... \u0026 Xiong, C. (2024). Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454.\n\n\u003ca id=\"ref21\"\u003e\u003c/a\u003e[21] Xu, Y., Lu, D., Shen, Z., Wang, J., Wang, Z., Mao, Y., ... \u0026 Yu, T. (2024). Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. arXiv preprint arXiv:2412.09605.\n\n\u003ca id=\"ref22\"\u003e\u003c/a\u003e[22] Xie, J., Xu, D., Zhao, X., \u0026 Song, D. (2025). AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents. arXiv preprint arXiv:2506.14205.\n\n\u003ca id=\"ref23\"\u003e\u003c/a\u003e[23] Jason Wei, Asymmetry of verification and verifier's law, 2025, https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law\n\n\u003ca id=\"ref24\"\u003e\u003c/a\u003e[24] Xie, T., Zhao, S., Wu, C. H., Liu, Y., Luo, Q., Zhong, V., ... \u0026 Yu, T. (2023). Text2reward: Reward shaping with language models for reinforcement learning. arXiv preprint arXiv:2309.11489.\n\n\u003ca id=\"ref25\"\u003e\u003c/a\u003e[25] Ma, Y. J., Liang, W., Wang, G., Huang, D. A., Bastani, O., Jayaraman, D., ... \u0026 Anandkumar, A. (2023). Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931.\n\n\u003ca id=\"ref26\"\u003e\u003c/a\u003e[26] Zhong, R., Snell, C., Klein, D., \u0026 Eisner, J. (2022). Non-programmers can label programs indirectly via active examples: A case study with text-to-SQL. arXiv preprint arXiv:2205.12422.\n\n\u003ca id=\"ref27\"\u003e\u003c/a\u003e[27] Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., ... \u0026 Wu, J. (2023). Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390.\n\n\u003chr class=\"solid\"\u003e\n\n## Citation\n\nIf you think this blog post and the content involved are helpful to you, please cite:\n```\n@article{osworld_verified,\n      title={Introducing OSWorld-Verified}, \n      author={Tianbao Xie and Mengqi Yuan and Danyang Zhang and Xinzhuang Xiong and Zhennan Shen and Zilong Zhou  and Xinyuan Wang and Yanxu Chen and Jiaqi Deng and Junda Chen and Bowen Wang and Haoyuan Wu and Jixuan Chen and Junli Wang and Dunjie Lu and Hao Hu and Tao Yu},\n      year={2025},\n}\n```\n\n\n\n\n","title":"Introducing OSWorld-Verified","shortTitle":"OSWorld-Verified","date":"28 July 2025","author":"XLANG Lab","coverImage":"/blog/osworld-verified/overview.png","previewContent":"We've systematically addressed 300+ issues in OSWorld through a comprehensive refinement process. OSWorld-Verified delivers more reliable evaluation signals through improved infrastructure and enhanced task quality, providing the community with a more stable foundation for advancing computer use agent research.","onlineImage":"https://imgur.com/a/Jh7SS0Q","githubLink":"https://github.com/xlang-ai/OSWorld"}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"osworld-verified"},"buildId":"o1hdWyFQiG2ICuDjpFlrw","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>