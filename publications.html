<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>XLANG Lab | Research</title><link rel="apple-touch-icon" sizes="180x180" href="/favicon/white-on-green/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/white-on-green/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/white-on-green/favicon-16x16.png"/><link rel="manifest" href="/favicon/white-on-green/site.webmanifest"/><meta name="next-head-count" content="7"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/c7a1138c526c7fb2.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c7a1138c526c7fb2.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-4d525326a36e0ad2.js" defer=""></script><script src="/_next/static/chunks/pages/_app-43f67b4afd5f2ce7.js" defer=""></script><script src="/_next/static/chunks/pages/publications-52b30223f9d7a405.js" defer=""></script><script src="/_next/static/a0SHYVUQO49DdImcU_m8p/_buildManifest.js" defer=""></script><script src="/_next/static/a0SHYVUQO49DdImcU_m8p/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&display=swap">@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtr6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtZ6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCu170w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCuM70w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v30/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next"><div class="fixed top-0 left-0 w-full h-14 md:h-20 bg-white py-4 z-10 navbar-shadow"><div class="page-x-width w-full flex justify-between items-center"><div class="sm:hidden w-fit h-fit cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" class="text-[#0156AC]" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><line x1="4" y1="6" x2="20" y2="6"></line><line x1="4" y1="12" x2="20" y2="12"></line><line x1="4" y1="18" x2="20" y2="18"></line></svg></div><a href="/"><div class="flex gap-2 items-center cursor-pointer text-brand-dark"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/logo.svg"/><div>XLANG Lab</div></div></a><ul class="gap-8 text-md text-text-brand-dark hidden sm:flex"><li class="font-[500] hover:underline text-brand-dark"><a href="/">about</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/team">team</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/publications">publications</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/blog">blogs</a></li></ul><div class="flex gap-4 items-center"><ul class="hidden lg:flex gap-3"><li><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github-black.svg"/></a></li><li><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter-black.svg"/></a></li></ul><div class="max-sm:text-sm border border-brand-primary2 border-2 text-brand-primary2 font-[500] rounded-xl py-1 px-3 cursor-pointer"><a href="https://forms.gle/3Ki9ectMB5D31F8g8" target="_blank" rel="noopener noreferrer">join us</a></div></div></div></div><div class="relative w-full h-full"><div class="absolute top-0 left-0 max-sm:hidden -mt-8 z-[-1]"><img alt="Wave" loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/background/wave.svg"/><img alt="Wave 2" loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/background/wave2.svg"/></div><div class="w-full pt-20 sm:pt-36 pb-10 bg-[#D9D9D9]/20"><div class="page-x-width flex flex-col gap-8 sm:gap-6"><div><h1 class="text-2xl mb-6">Research</h1><p class="leading-7">Our research focuses on building grounded AI systems that enable users to interact through natural language with digital and physical environments. We develop AI agents that translate language and perception into executable code and actions, empowering people to perform data science, control computers, and collaborate with robots. Our work spans three core areas: code generation for data science, grounding language in the digital world, and grounding language in the physical world.</p></div><div><h1 class="text-2xl font-[500] mb-6">Papers</h1><div class="flex gap-3 flex-wrap"><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">Code Generation for Data Science</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">Grounding Language in the Digital World</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">Grounding Language in the Physical World</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">Others : RAG, Tool use, Efficient LLM, Data synthesis etc.</div></div><div class="flex flex-col mt-10"><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/osworld.png"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu</p><p class="text-md font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2404.07972" target="_blank">paper</a><a href="https://github.com/xlang-ai/OSWorld" target="_blank">code</a><a href="https://twitter.com/TianbaoX/status/1778781521253667267" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="ARKS: Active Retrieval in Knowledge Soup for Code Generation" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/ark.jpeg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">ARKS: Active Retrieval in Knowledge Soup for Code Generation</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu</p><p class="text-md font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2402.12317" target="_blank">paper</a><a href="https://github.com/xlang-ai/arks" target="_blank">code</a><a href="https://twitter.com/hongjin_su/status/1759978005525643466" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="Generative Representational Instruction Tuning" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/grit.jpeg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">Generative Representational Instruction Tuning</h1><p class="text-[#727272] text-xs font-[500]">Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela</p><p class="text-md font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2402.09906" target="_blank">paper</a><a href="https://github.com/ContextualAI/gritlm" target="_blank">code</a><a href="https://twitter.com/Muennighoff/status/1758307967802224770" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="OpenAgents: An Open Platform for Language Agents in the Wild" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/openagents.png"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">OpenAgents: An Open Platform for Language Agents in the Wild</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie*, Fan Zhou*, Zhoujun Cheng*, Peng Shi*, Luoxuan Weng*, Yitao Liu*, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu</p><p class="text-md font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2310.10634" target="_blank">paper</a><a href="https://github.com/xlang-ai/OpenAgents" target="_blank">code</a><a href="https://www.xlang.ai/blog/xlang-intro" target="_blank">page</a><a href="https://twitter.com/ChengZhoujun/status/1714343204148113860" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="Lemur: Harmonizing Natural Language and Code for Language Agents" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/lemur.png"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">Lemur: Harmonizing Natural Language and Code for Language Agents</h1><p class="text-[#727272] text-xs font-[500]">Yiheng Xu*, Hongjin Su*, Chen Xing*, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu</p><p class="text-md font-[500]">ICLR 2024 Spotlight</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2310.06830" target="_blank">paper</a><a href="https://github.com/OpenLemur/lemur" target="_blank">code</a><a href="https://www.xlang.ai/blog/openlemur" target="_blank">page</a><a href="https://twitter.com/yihengxu_/status/1712537543688990940" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning
" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/text2reward.png"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning
</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie*, Siheng Zhao*, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</p><p class="text-md font-[500]">ICLR 2024 Spotlight</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2309.11489" target="_blank">paper</a><a href="https://github.com/xlang-ai/text2reward" target="_blank">code</a><a href="https://text-to-reward.github.io/" target="_blank">page</a><a href="https://twitter.com/arankomatsuzaki/status/1706311844829487153" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="Instructor Embeddings: One Embedder, Any Task: Instruction-Finetuned Text Embeddings" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/instructor.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">Instructor Embeddings: One Embedder, Any Task: Instruction-Finetuned Text Embeddings</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su*, Weijia Shi*, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu</p><p class="text-md font-[500]">ACL 2023 Findings</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2212.09741" target="_blank">paper</a><a href="https://github.com/HKUNLP/instructor-embedding" target="_blank">code</a><a href="https://instructor-embedding.github.io/" target="_blank">page</a><a href="https://twitter.com/WeijiaShi2/status/1605307966109863936" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/ds-1000.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation</h1><p class="text-[#727272] text-xs font-[500]">Yuhang Lai*, Chengxi Li*, Yiming Wang*, Tianyi Zhang*, Ruiqi Zhong*, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, Tao Yu</p><p class="text-md font-[500]">ICML 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2211.11501" target="_blank">paper</a><a href="https://github.com/HKUNLP/DS-1000" target="_blank">code</a><a href="https://ds1000-code-gen.github.io/" target="_blank">page</a><a href="https://twitter.com/taoyds/status/1595086401309388801" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="Coder Reviewer Reranking for Code Generation" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/code-review.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">Coder Reviewer Reranking for Code Generation</h1><p class="text-[#727272] text-xs font-[500]">Tianyi Zhang, Tao Yu, Tatsunori B. Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, Sida I. Wang</p><p class="text-md font-[500]">ICML 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2211.16490" target="_blank">paper</a><a href="https://github.com/facebookresearch/coder_reviewer_reranking" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="Compositional Exemplars for In-context Learning" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/ce-icl.png"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">Compositional Exemplars for In-context Learning</h1><p class="text-[#727272] text-xs font-[500]">Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong.</p><p class="text-md font-[500]">ICML 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2302.05698" target="_blank">paper</a><a href="https://github.com/HKUNLP/icl-ceil" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="Binder: Binding Language Models in Symbolic Languages" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/binder.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">Binder: Binding Language Models in Symbolic Languages</h1><p class="text-[#727272] text-xs font-[500]">Zhoujun Cheng*, Tianbao Xie*, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu</p><p class="text-md font-[500]">ICLR 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2210.02875" target="_blank">paper</a><a href="https://github.com/HKUNLP/binder" target="_blank">code</a><a href="https://lm-code-binder.github.io/" target="_blank">page</a><a href="https://twitter.com/taoyds/status/1580987466974396417" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="Selective Annotation Makes Language Models Better Few-Shot Learners" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/selective-annotation.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">Selective Annotation Makes Language Models Better Few-Shot Learners</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu</p><p class="text-md font-[500]">ICLR 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2209.01975" target="_blank">paper</a><a href="https://github.com/HKUNLP/icl-selective-annotation" target="_blank">code</a><a href="https://twitter.com/jungokasai/status/1568302230490730497" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/unifiedskg.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie*, Chen Henry Wu*, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu.</p><p class="text-md font-[500]">EMNLP 2022</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2201.05966" target="_blank">paper</a><a href="https://github.com/HKUNLP/UnifiedSKG" target="_blank">code</a><a href="https://unifiedskg.com/" target="_blank">page</a><a href="https://twitter.com/taoyds/status/1487177399875690500" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="ZeroGen: Efficient Zero-shot Learning via Dataset Generation" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/zerogen.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">ZeroGen: Efficient Zero-shot Learning via Dataset Generation</h1><p class="text-[#727272] text-xs font-[500]">Jiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong</p><p class="text-md font-[500]">EMNLP 2022</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2202.07922" target="_blank">paper</a><a href="https://github.com/jiacheng-ye/ZeroGen" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="In-Context Learning for Few-Shot Dialogue State Tracking" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/ic-dst.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">In-Context Learning for Few-Shot Dialogue State Tracking</h1><p class="text-[#727272] text-xs font-[500]">Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, Mari Ostendorf</p><p class="text-md font-[500]">EMNLP Findings 2022</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2203.08568" target="_blank">paper</a><a href="https://github.com/Yushi-Hu/IC-DST" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/nl2interface.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries</h1><p class="text-[#727272] text-xs font-[500]">Yiru Chen, Ryan Li, Austin Mac, Tianbao Xie, Tao Yu, Eugene Wu</p><p class="text-md font-[500]">IEEE Visualization Conference NLVIZ Workshop 2022</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2209.08834" target="_blank">paper</a><a href="https://github.com/learnedinterfaces/PI2" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/grappa.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing</h1><p class="text-[#727272] text-xs font-[500]">Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Richard Socher, Caiming Xiong</p><p class="text-md font-[500]">ICLR 2021</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2009.13845" target="_blank">paper</a><a href="https://github.com/taoyds/grappa" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="Semantic Evaluation for Text-to-SQL with Distilled Test Suites" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/test-suite.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">Semantic Evaluation for Text-to-SQL with Distilled Test Suites</h1><p class="text-[#727272] text-xs font-[500]">Ruiqi Zhong, Tao Yu, Dan Klein</p><p class="text-md font-[500]">EMNLP 2020</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/2010.02840" target="_blank">paper</a><a href="https://github.com/taoyds/test-suite-sql-eval" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/cosql.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases</h1><p class="text-[#727272] text-xs font-[500]">Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, Dragomir Radev</p><p class="text-md font-[500]">EMNLP 2019</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/1909.05378" target="_blank">paper</a><a href="https://github.com/taoyds/cosql" target="_blank">code</a><a href="https://yale-lily.github.io/cosql" target="_blank">page</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="SParC: Cross-Domain Semantic Parsing in Context" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/sparc.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">SParC: Cross-Domain Semantic Parsing in Context</h1><p class="text-[#727272] text-xs font-[500]">Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, Dragomir Radev</p><p class="text-md font-[500]">ACL 2018</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/1906.02285" target="_blank">paper</a><a href="https://github.com/taoyds/sparc" target="_blank">code</a><a href="https://yale-lily.github.io/sparc" target="_blank">page</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="grid grid-cols-7 gap-4"><div class="col-span-2 relative aspect-video rounded-lg overflow-hidden shadow-xl"><img alt="Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task" loading="lazy" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/research/spider.jpg"/></div><div class="col-span-5 flex flex-col"><h1 class="text-lg font-[500]">Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task</h1><p class="text-[#727272] text-xs font-[500]">Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev</p><p class="text-md font-[500]">EMNLP 2018</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs mt-auto"><a href="https://arxiv.org/abs/1809.08887" target="_blank">paper</a><a href="https://github.com/taoyds/spider" target="_blank">code</a><a href="https://yale-lily.github.io/spider" target="_blank">page</a></div></div></div></div></div></div></div></div></div><div class="w-full bg-brand-offBlack p-4"><div class="page-x-width flex justify-center sm:justify-between flex-wrap gap-4"><div class="flex gap-2 items-center"><a href="/"><div class="relative"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/logo-white.svg"/></div></a><div class="text-white text-xs w-fit">© Copyright 2023 XLANG Lab. All right reserved.</div></div><nav><ul class="text-white flex gap-6"><li class="cursor-pointer"><a href="https://discord.com/invite/4Gnw7eTEZR"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/discord.svg"/></a></li><li class="cursor-pointer"><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github.svg"/></a></li><li class="cursor-pointer"><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter.svg"/></a></li></ul></nav></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"papers":[{"category":["PoweredAgents","InteractiveSystems","ToolUse"],"title":"OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments","authors":"Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu","publication":"Preprint","paperLink":"https://arxiv.org/abs/2404.07972","codeLink":"https://github.com/xlang-ai/OSWorld","dataLink":"https://github.com/xlang-ai/OSWorld/tree/main/evaluation_examples","blogLink":"","twitterLink":"https://twitter.com/TianbaoX/status/1778781521253667267","image":"/research/osworld.png"},{"category":["Grounding","EfficientLLMs"],"title":"ARKS: Active Retrieval in Knowledge Soup for Code Generation","authors":"Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu","publication":"Preprint","paperLink":"https://arxiv.org/abs/2402.12317","codeLink":"https://github.com/xlang-ai/arks","dataLink":"https://huggingface.co/datasets/xlangai/arks_data","blogLink":"","twitterLink":"https://twitter.com/hongjin_su/status/1759978005525643466","image":"/research/ark.jpeg"},{"category":["EfficientLLMs"],"title":"Generative Representational Instruction Tuning","authors":"Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela","publication":"Preprint","paperLink":"https://arxiv.org/abs/2402.09906","codeLink":"https://github.com/ContextualAI/gritlm","dataLink":"","blogLink":"","twitterLink":"https://twitter.com/Muennighoff/status/1758307967802224770","image":"/research/grit.jpeg"},{"category":["PoweredAgents","InteractiveSystems"],"title":"OpenAgents: An Open Platform for Language Agents in the Wild","authors":"Tianbao Xie*, Fan Zhou*, Zhoujun Cheng*, Peng Shi*, Luoxuan Weng*, Yitao Liu*, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu","publication":"Preprint","paperLink":"https://arxiv.org/abs/2310.10634","codeLink":"https://github.com/xlang-ai/OpenAgents","dataLink":"https://github.com/xlang-ai/OpenAgents","blogLink":"https://www.xlang.ai/blog/xlang-intro","twitterLink":"https://twitter.com/ChengZhoujun/status/1714343204148113860","image":"/research/openagents.png"},{"category":["PoweredAgents","Grounding","ToolUse"],"title":"Lemur: Harmonizing Natural Language and Code for Language Agents","authors":"Yiheng Xu*, Hongjin Su*, Chen Xing*, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu","publication":"ICLR 2024 Spotlight","paperLink":"https://arxiv.org/abs/2310.06830","codeLink":"https://github.com/OpenLemur/lemur","dataLink":"https://huggingface.co/OpenLemur","blogLink":"https://www.xlang.ai/blog/openlemur","twitterLink":"https://twitter.com/yihengxu_/status/1712537543688990940","image":"/research/lemur.png","huggingfaceModel":["OpenLemur/lemur-70b-chat-v1","OpenLemur/lemur-70b-v1"]},{"category":["Grounding","Robotics"],"title":"Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning\n","authors":"Tianbao Xie*, Siheng Zhao*, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu","publication":"ICLR 2024 Spotlight","paperLink":"https://arxiv.org/abs/2309.11489","codeLink":"https://github.com/xlang-ai/text2reward","dataLink":"https://github.com/xlang-ai/text2reward","blogLink":"https://text-to-reward.github.io/","twitterLink":"https://twitter.com/arankomatsuzaki/status/1706311844829487153","image":"/research/text2reward.png"},{"category":["EfficientLLMs"],"title":"Instructor Embeddings: One Embedder, Any Task: Instruction-Finetuned Text Embeddings","authors":"Hongjin Su*, Weijia Shi*, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu","publication":"ACL 2023 Findings","paperLink":"https://arxiv.org/abs/2212.09741","codeLink":"https://github.com/HKUNLP/instructor-embedding","dataLink":"https://drive.google.com/file/d/1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52/view","blogLink":"https://instructor-embedding.github.io/","twitterLink":"https://twitter.com/WeijiaShi2/status/1605307966109863936","image":"/research/instructor.jpg","huggingfaceModel":["hkunlp/instructor-large","hkunlp/instructor-base","hkunlp/instructor-xl"]},{"category":["Grounding"],"title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation","authors":"Yuhang Lai*, Chengxi Li*, Yiming Wang*, Tianyi Zhang*, Ruiqi Zhong*, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, Tao Yu","publication":"ICML 2023","paperLink":"https://arxiv.org/abs/2211.11501","codeLink":"https://github.com/HKUNLP/DS-1000","dataLink":"https://github.com/HKUNLP/DS-1000/tree/main/ds1000_example","blogLink":"https://ds1000-code-gen.github.io/","twitterLink":"https://twitter.com/taoyds/status/1595086401309388801","image":"/research/ds-1000.jpg"},{"category":["Grounding"],"title":"Coder Reviewer Reranking for Code Generation","authors":"Tianyi Zhang, Tao Yu, Tatsunori B. Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, Sida I. Wang","publication":"ICML 2023","paperLink":"https://arxiv.org/abs/2211.16490","codeLink":"https://github.com/facebookresearch/coder_reviewer_reranking","dataLink":"","blogLink":"","twitterLink":"","image":"/research/code-review.jpg"},{"category":["EfficientLLMs"],"title":"Compositional Exemplars for In-context Learning","authors":"Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong.","publication":"ICML 2023","paperLink":"https://arxiv.org/abs/2302.05698","codeLink":"https://github.com/HKUNLP/icl-ceil","dataLink":"","blogLink":"","twitterLink":"","image":"/research/ce-icl.png"},{"category":["Grounding","ToolUse"],"title":"Binder: Binding Language Models in Symbolic Languages","authors":"Zhoujun Cheng*, Tianbao Xie*, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu","publication":"ICLR 2023","paperLink":"https://arxiv.org/abs/2210.02875","codeLink":"https://github.com/HKUNLP/binder","dataLink":"https://github.com/HKUNLP/Binder/tree/main/datasets","blogLink":"https://lm-code-binder.github.io/","twitterLink":"https://twitter.com/taoyds/status/1580987466974396417","image":"/research/binder.jpg"},{"category":["EfficientLLMs"],"title":"Selective Annotation Makes Language Models Better Few-Shot Learners","authors":"Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu","publication":"ICLR 2023","paperLink":"https://arxiv.org/abs/2209.01975","codeLink":"https://github.com/HKUNLP/icl-selective-annotation","dataLink":"https://github.com/HKUNLP/icl-selective-annotation/tree/main/data","twitterLink":"https://twitter.com/jungokasai/status/1568302230490730497","image":"/research/selective-annotation.jpg"},{"category":["Grounding","EfficientLLMs"],"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models","authors":"Tianbao Xie*, Chen Henry Wu*, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu.","publication":"EMNLP 2022","paperLink":"https://arxiv.org/abs/2201.05966","codeLink":"https://github.com/HKUNLP/UnifiedSKG","dataLink":"https://unifiedskg.com/benchmarks/","blogLink":"https://unifiedskg.com/","twitterLink":"https://twitter.com/taoyds/status/1487177399875690500","image":"/research/unifiedskg.jpg"},{"category":["EfficientLLMs"],"title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation","authors":"Jiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong","publication":"EMNLP 2022","paperLink":"https://arxiv.org/abs/2202.07922","codeLink":"https://github.com/jiacheng-ye/ZeroGen","dataLink":"","blogLink":"","twitterLink":"","image":"/research/zerogen.jpg"},{"category":["EfficientLLMs","InteractiveSystems"],"title":"In-Context Learning for Few-Shot Dialogue State Tracking","authors":"Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, Mari Ostendorf","publication":"EMNLP Findings 2022","paperLink":"https://arxiv.org/abs/2203.08568","codeLink":"https://github.com/Yushi-Hu/IC-DST","dataLink":"","blogLink":"","twitterLink":"","image":"/research/ic-dst.jpg"},{"category":["Grounding","InteractiveSystems"],"title":"NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries","authors":"Yiru Chen, Ryan Li, Austin Mac, Tianbao Xie, Tao Yu, Eugene Wu","publication":"IEEE Visualization Conference NLVIZ Workshop 2022","paperLink":"https://arxiv.org/abs/2209.08834","codeLink":"https://github.com/learnedinterfaces/PI2","dataLink":"","blogLink":"","twitterLink":"","image":"/research/nl2interface.jpg"},{"category":["Grounding"],"title":"GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing","authors":"Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Richard Socher, Caiming Xiong","publication":"ICLR 2021","paperLink":"https://arxiv.org/abs/2009.13845","codeLink":"https://github.com/taoyds/grappa","dataLink":"","blogLink":"","twitterLink":"","image":"/research/grappa.jpg"},{"category":["Grounding"],"title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suites","authors":"Ruiqi Zhong, Tao Yu, Dan Klein","publication":"EMNLP 2020","paperLink":"https://arxiv.org/abs/2010.02840","codeLink":"https://github.com/taoyds/test-suite-sql-eval","dataLink":"","blogLink":"","twitterLink":"","image":"/research/test-suite.jpg"},{"category":["Grounding","InteractiveSystems"],"title":"CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases","authors":"Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, Dragomir Radev","publication":"EMNLP 2019","paperLink":"https://arxiv.org/abs/1909.05378","codeLink":"https://github.com/taoyds/cosql","dataLink":"https://drive.google.com/uc?export=download\u0026id=1Y3ydpFiQQ3FC0bzdfy3groV95O_f1nXF","blogLink":"https://yale-lily.github.io/cosql","twitterLink":"","image":"/research/cosql.jpg"},{"category":["Grounding"],"title":"SParC: Cross-Domain Semantic Parsing in Context","authors":"Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, Dragomir Radev","publication":"ACL 2018","paperLink":"https://arxiv.org/abs/1906.02285","codeLink":"https://github.com/taoyds/sparc","dataLink":"https://drive.google.com/uc?export=download\u0026id=1Uu7NMHTR1tdQw1t7bAuM7OPU4LElVKfg","blogLink":"https://yale-lily.github.io/sparc","twitterLink":"","image":"/research/sparc.jpg"},{"category":["Grounding"],"title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task","authors":"Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev","publication":"EMNLP 2018","paperLink":"https://arxiv.org/abs/1809.08887","codeLink":"https://github.com/taoyds/spider","dataLink":"https://drive.google.com/uc?export=download\u0026id=1TqleXec_OykOYFREKKtschzY29dUcVAQ","blogLink":"https://yale-lily.github.io/spider","twitterLink":"","image":"/research/spider.jpg"}],"talks":[{"title":"Building Natural Language Interfaces through Grounding Language Models into Executable Actions","startDate":"2023-04-01","endDate":"2023-05-31","desc":"Columbia NLP seminar, Cornell DB seminar, Microsoft Research Asia","link":""},{"title":"Building Natural Language Interfaces with Large Language Models","startDate":"2022-11-01","endDate":"2022-11-30","desc":"Amazon AWS","link":"https://docs.google.com/presentation/d/1jsfb6foc3XCcfO9mVC4IkCoG9mQ0N3adlISj86Mauig/edit?usp=sharing\u0026resourcekey=0-9oawun4RgUnSh_UWJ64U9g"},{"title":"Few-shot In-context Learning with Large Language Models","startDate":"2022-06-01","endDate":"2022-06-30","desc":"AllState Tech Talks","link":""},{"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models","startDate":"2022-02-01","endDate":"2022-02-28","desc":"ServiceNow Research (Prev. ElementAI)","link":"https://docs.google.com/presentation/d/1A2fSbpHXEVP_NVyeiy8TymHnibb91_OZmuk83qffq-Q/edit?usp=sharing"},{"title":"SCoRe: Pre-Training for Context Representation in Conversational Semantic Parsing","startDate":"2021-04-01","endDate":"2021-04-30","desc":"Google Research","link":"https://docs.google.com/presentation/d/11S7y57PRSlq_ccQxprdtko1MVgQN--L8yH8H9UPfsfE/edit?usp=sharing"},{"title":"Learning to Build Conversational Natural Language Interfaces","startDate":"2021-01-01","endDate":"2021-03-31","desc":"The University of Hong Kong (CS), The National University of Singapore (CS), The University of Wisconsin-Madison (CS), Simon Fraser University (CS), The University of Minnesota Twin Cities (CSE)","link":"https://docs.google.com/presentation/d/1ZyuMKRoXgY9dBmiJeYT8n_3sgFY6aw_EaMbnqZtAt8Y/edit?usp=sharing"},{"title":"SParC: Cross-Domain Semantic Parsing in Context","startDate":"2019-09-01","endDate":"2019-09-30","desc":"Microsoft Research AI Breakthroughs Workshop, Redmond","link":"https://yale-lily.github.io/sparc"}]},"__N_SSG":true},"page":"/publications","query":{},"buildId":"a0SHYVUQO49DdImcU_m8p","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>