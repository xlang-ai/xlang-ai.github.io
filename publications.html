<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>XLANG Lab | Research</title><link rel="apple-touch-icon" sizes="180x180" href="/favicon/white-on-green/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/white-on-green/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/white-on-green/favicon-16x16.png"/><link rel="manifest" href="/favicon/white-on-green/site.webmanifest"/><meta name="next-head-count" content="7"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/d6f692ad1e46a8a8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d6f692ad1e46a8a8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-4d525326a36e0ad2.js" defer=""></script><script src="/_next/static/chunks/pages/_app-43f67b4afd5f2ce7.js" defer=""></script><script src="/_next/static/chunks/pages/publications-cd216b9e0600a6f5.js" defer=""></script><script src="/_next/static/AMbiQiO6HRLFcaOWDE66z/_buildManifest.js" defer=""></script><script src="/_next/static/AMbiQiO6HRLFcaOWDE66z/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&display=swap">@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtr6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtZ6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCu170w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCuM70w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next"><div class="fixed top-0 left-0 w-full h-14 md:h-20 bg-white py-4 z-10 navbar-shadow"><div class="page-x-width w-full flex justify-between items-center"><div class="sm:hidden w-fit h-fit cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" class="text-[#0156AC]" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><line x1="4" y1="6" x2="20" y2="6"></line><line x1="4" y1="12" x2="20" y2="12"></line><line x1="4" y1="18" x2="20" y2="18"></line></svg></div><a href="/"><div class="flex gap-2 items-center cursor-pointer text-brand-dark"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/logo.svg"/><div>XLANG Lab</div></div></a><ul class="gap-8 text-md text-text-brand-dark hidden sm:flex"><li class="font-[500] hover:underline text-brand-dark"><a href="/">about</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/team">team</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/publications">publications</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/blog">blogs</a></li></ul><div class="flex gap-4 items-center"><ul class="hidden lg:flex gap-3"><li><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github-black.svg"/></a></li><li><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter-black.svg"/></a></li></ul><div class="max-sm:text-sm border border-brand-primary2 border-2 text-brand-primary2 font-[500] rounded-xl py-1 px-3 cursor-pointer"><a href="https://forms.gle/3Ki9ectMB5D31F8g8" target="_blank" rel="noopener noreferrer">join us</a></div></div></div></div><div class="relative w-full h-full"><div class="absolute top-0 left-0 max-sm:hidden -mt-8 z-[-1]"><img alt="Wave" loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/background/wave.svg"/><img alt="Wave 2" loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/background/wave2.svg"/></div><div class="w-full pt-20 sm:pt-36 pb-10 bg-[#D9D9D9]/20"><div class="page-x-width flex flex-col gap-8 sm:gap-6"><div><h1 class="text-2xl mb-6">Research</h1><p class="leading-7">Our research focuses on building grounded AI systems that enable users to interact through natural language with digital and physical environments. We develop AI agents that translate language and perception into executable code and actions, empowering people to perform data science, control computers, and collaborate with robots. Our work spans three core areas: code generation for data science, grounding language in the digital world, and grounding language in the physical world.</p></div><div><h1 class="text-2xl font-[500] mb-6">Papers</h1><div class="flex gap-3 flex-wrap"><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">Code Generation for Data Science</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">Grounding Language in the Digital World</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">Grounding Language in the Physical World</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">Others : RAG, Tool use, Efficient LLM, Data synthesis etc.</div></div><div class="flex flex-col mt-10"><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/osworld.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu</p><p class="text-md font-[500]">NeurIPS 2024</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2404.07972" target="_blank">paper</a><a href="https://github.com/xlang-ai/OSWorld" target="_blank">code</a><a href="https://os-world.github.io/" target="_blank">page</a><a href="https://twitter.com/TianbaoX/status/1778781521253667267" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/agenttrek.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials</h1><p class="text-[#727272] text-xs font-[500]">Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, Tao Yu</p><p class="text-md font-[500]">ICLR 2025</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2412.09605" target="_blank">paper</a><a href="https://agenttrek.github.io/" target="_blank">page</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/aguvis.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction</h1><p class="text-[#727272] text-xs font-[500]">Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, Caiming Xiong</p><p class="text-md font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2412.04454" target="_blank">paper</a><a href="https://github.com/xlang-ai/aguvis" target="_blank">code</a><a href="https://aguvis-project.github.io/" target="_blank">page</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/learn_by_interact.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, Sercan Ö. Arık</p><p class="text-md font-[500]">ICLR 2025</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2501.10893" target="_blank">paper</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/spider2.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows</h1><p class="text-[#727272] text-xs font-[500]">Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, Victor Zhong, Caiming Xiong, Ruoxi Sun, Qian Liu, Sida Wang, Tao Yu</p><p class="text-md font-[500]">ICLR 2025</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://www.arxiv.org/abs/2411.07763" target="_blank">paper</a><a href="https://github.com/xlang-ai/Spider2" target="_blank">code</a><a href="https://spider2-sql.github.io/" target="_blank">page</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Attacking Vision-Language Computer Agents via Pop-ups" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/attack.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Attacking Vision-Language Computer Agents via Pop-ups</h1><p class="text-[#727272] text-xs font-[500]">Yanzhe Zhang, Tao Yu, Diyi Yang</p><p class="text-md font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2411.02391" target="_blank">paper</a><a href="https://github.com/SALT-NLP/PopupAttack" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/bright.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, Tao Yu</p><p class="text-md font-[500]">ICLR 2025</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2407.12883" target="_blank">paper</a><a href="https://github.com/xlang-ai/BRIGHT" target="_blank">code</a><a href="https://brightbenchmark.github.io/" target="_blank">page</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Generative Representational Instruction Tuning" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/grit.jpeg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Generative Representational Instruction Tuning</h1><p class="text-[#727272] text-xs font-[500]">Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela</p><p class="text-md font-[500]">ICLR 2025</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2402.09906" target="_blank">paper</a><a href="https://github.com/ContextualAI/gritlm" target="_blank">code</a><a href="https://twitter.com/Muennighoff/status/1758307967802224770" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/spider2v.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?</h1><p class="text-[#727272] text-xs font-[500]">Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu</p><p class="text-md font-[500]">NeurIPS 2024, Spotlight</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2407.10956" target="_blank">paper</a><a href="https://github.com/xlang-ai/Spider2-V" target="_blank">code</a><a href="https://spider2-v.github.io/" target="_blank">page</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="EvoR: Evolving Retrieval for Code Generation" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/ark.jpeg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">EvoR: Evolving Retrieval for Code Generation</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu</p><p class="text-md font-[500]">EMNLP Findings 2024</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2307.07047" target="_blank">paper</a><a href="https://github.com/xlang-ai/EVOR" target="_blank">code</a><a href="https://arks-codegen.github.io/" target="_blank">page</a><a href="https://twitter.com/hongjin_su/status/1759978005525643466" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="OS-Copilot: Towards Generalist Computer Agents with Self-Improvement" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/os_copilot.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">OS-Copilot: Towards Generalist Computer Agents with Self-Improvement</h1><p class="text-[#727272] text-xs font-[500]">Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong</p><p class="text-md font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2402.07456" target="_blank">paper</a><a href="https://github.com/OS-Copilot/OS-Copilot" target="_blank">code</a><a href="https://os-copilot.github.io/" target="_blank">page</a><a href="https://twitter.com/zywu_hku/status/1758014688779244002" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="OpenAgents: An Open Platform for Language Agents in the Wild" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/openagents.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">OpenAgents: An Open Platform for Language Agents in the Wild</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie*, Fan Zhou*, Zhoujun Cheng*, Peng Shi*, Luoxuan Weng*, Yitao Liu*, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu</p><p class="text-md font-[500]">COLM 2024</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2310.10634" target="_blank">paper</a><a href="https://github.com/xlang-ai/OpenAgents" target="_blank">code</a><a href="https://www.xlang.ai/blog/xlang-intro" target="_blank">page</a><a href="https://twitter.com/ChengZhoujun/status/1714343204148113860" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Does Collaborative Human-LM Dialogue Generation Help Information Extraction from Human Dialogues?" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/does.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Does Collaborative Human-LM Dialogue Generation Help Information Extraction from Human Dialogues?</h1><p class="text-[#727272] text-xs font-[500]">Bo-Ru Lu, Nikita Haduong, Chia-Hsuan Lee, Zeqiu Wu, Hao Cheng, Paul Koester, Jean Utke, Tao Yu, Noah A. Smith, Mari Ostendorf</p><p class="text-md font-[500]">COLM 2024</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2307.07047" target="_blank">paper</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Lemur: Harmonizing Natural Language and Code for Language Agents" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/lemur.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Lemur: Harmonizing Natural Language and Code for Language Agents</h1><p class="text-[#727272] text-xs font-[500]">Yiheng Xu*, Hongjin Su*, Chen Xing*, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu</p><p class="text-md font-[500]">ICLR 2024, Spotlight</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2310.06830" target="_blank">paper</a><a href="https://github.com/OpenLemur/lemur" target="_blank">code</a><a href="https://www.xlang.ai/blog/openlemur" target="_blank">page</a><a href="https://twitter.com/yihengxu_/status/1712537543688990940" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning
" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/text2reward.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning
</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie*, Siheng Zhao*, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</p><p class="text-md font-[500]">ICLR 2024, Spotlight</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2309.11489" target="_blank">paper</a><a href="https://github.com/xlang-ai/text2reward" target="_blank">code</a><a href="https://text-to-reward.github.io/" target="_blank">page</a><a href="https://twitter.com/arankomatsuzaki/status/1706311844829487153" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Instructor Embeddings: One Embedder, Any Task: Instruction-Finetuned Text Embeddings" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/instructor.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Instructor Embeddings: One Embedder, Any Task: Instruction-Finetuned Text Embeddings</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su*, Weijia Shi*, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu</p><p class="text-md font-[500]">ACL 2023 Findings</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2212.09741" target="_blank">paper</a><a href="https://github.com/HKUNLP/instructor-embedding" target="_blank">code</a><a href="https://instructor-embedding.github.io/" target="_blank">page</a><a href="https://twitter.com/WeijiaShi2/status/1605307966109863936" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/ds-1000.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation</h1><p class="text-[#727272] text-xs font-[500]">Yuhang Lai*, Chengxi Li*, Yiming Wang*, Tianyi Zhang*, Ruiqi Zhong*, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, Tao Yu</p><p class="text-md font-[500]">ICML 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2211.11501" target="_blank">paper</a><a href="https://github.com/HKUNLP/DS-1000" target="_blank">code</a><a href="https://ds1000-code-gen.github.io/" target="_blank">page</a><a href="https://twitter.com/taoyds/status/1595086401309388801" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Coder Reviewer Reranking for Code Generation" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/code-review.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Coder Reviewer Reranking for Code Generation</h1><p class="text-[#727272] text-xs font-[500]">Tianyi Zhang, Tao Yu, Tatsunori B. Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, Sida I. Wang</p><p class="text-md font-[500]">ICML 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2211.16490" target="_blank">paper</a><a href="https://github.com/facebookresearch/coder_reviewer_reranking" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Compositional Exemplars for In-context Learning" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/ce-icl.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Compositional Exemplars for In-context Learning</h1><p class="text-[#727272] text-xs font-[500]">Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong.</p><p class="text-md font-[500]">ICML 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2302.05698" target="_blank">paper</a><a href="https://github.com/HKUNLP/icl-ceil" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Batch Prompting: Efficient Inference with Large Language Model APIs" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/batch.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Batch Prompting: Efficient Inference with Large Language Model APIs</h1><p class="text-[#727272] text-xs font-[500]">Zhoujun Cheng, Jungo Kasai, Tao Yu</p><p class="text-md font-[500]">EMNLP 2023 Industry Track</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2301.08721" target="_blank">paper</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Binder: Binding Language Models in Symbolic Languages" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/binder.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Binder: Binding Language Models in Symbolic Languages</h1><p class="text-[#727272] text-xs font-[500]">Zhoujun Cheng*, Tianbao Xie*, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu</p><p class="text-md font-[500]">ICLR 2023, Spotlight</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2210.02875" target="_blank">paper</a><a href="https://github.com/HKUNLP/binder" target="_blank">code</a><a href="https://lm-code-binder.github.io/" target="_blank">page</a><a href="https://twitter.com/taoyds/status/1580987466974396417" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Selective Annotation Makes Language Models Better Few-Shot Learners" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/selective-annotation.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Selective Annotation Makes Language Models Better Few-Shot Learners</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu</p><p class="text-md font-[500]">ICLR 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2209.01975" target="_blank">paper</a><a href="https://github.com/HKUNLP/icl-selective-annotation" target="_blank">code</a><a href="https://twitter.com/jungokasai/status/1568302230490730497" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/unifiedskg.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie*, Chen Henry Wu*, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu.</p><p class="text-md font-[500]">EMNLP 2022, Oral</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2201.05966" target="_blank">paper</a><a href="https://github.com/HKUNLP/UnifiedSKG" target="_blank">code</a><a href="https://unifiedskg.com/" target="_blank">page</a><a href="https://twitter.com/taoyds/status/1487177399875690500" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="ZeroGen: Efficient Zero-shot Learning via Dataset Generation" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/zerogen.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">ZeroGen: Efficient Zero-shot Learning via Dataset Generation</h1><p class="text-[#727272] text-xs font-[500]">Jiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong</p><p class="text-md font-[500]">EMNLP 2022</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2202.07922" target="_blank">paper</a><a href="https://github.com/jiacheng-ye/ZeroGen" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="In-Context Learning for Few-Shot Dialogue State Tracking" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/ic-dst.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">In-Context Learning for Few-Shot Dialogue State Tracking</h1><p class="text-[#727272] text-xs font-[500]">Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, Mari Ostendorf</p><p class="text-md font-[500]">EMNLP Findings 2022</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2203.08568" target="_blank">paper</a><a href="https://github.com/Yushi-Hu/IC-DST" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/research/spider.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task</h1><p class="text-[#727272] text-xs font-[500]">Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev</p><p class="text-md font-[500]">EMNLP 2018</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/1809.08887" target="_blank">paper</a><a href="https://github.com/taoyds/spider" target="_blank">code</a><a href="https://yale-lily.github.io/spider" target="_blank">page</a></div></div></div></div></div></div></div></div></div><div class="w-full bg-brand-offBlack p-4"><div class="page-x-width flex justify-center sm:justify-between flex-wrap gap-4"><div class="flex gap-2 items-center"><a href="/"><div class="relative"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/logo-white.svg"/></div></a><div class="text-white text-xs w-fit">© Copyright 2023 XLANG Lab. All right reserved.</div></div><nav><ul class="text-white flex gap-6"><li class="cursor-pointer"><a href="https://discord.com/invite/4Gnw7eTEZR"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/discord.svg"/></a></li><li class="cursor-pointer"><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/github.svg"/></a></li><li class="cursor-pointer"><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/icons/twitter.svg"/></a></li></ul></nav></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"papers":[{"category":["DigitalAIAgents"],"title":"OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments","authors":"Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu","publication":"NeurIPS 2024","paperLink":"https://arxiv.org/abs/2404.07972","codeLink":"https://github.com/xlang-ai/OSWorld","dataLink":"https://github.com/xlang-ai/OSWorld/tree/main/evaluation_examples","blogLink":"https://os-world.github.io/","twitterLink":"https://twitter.com/TianbaoX/status/1778781521253667267","image":"/research/osworld.png"},{"category":["DigitalAIAgents","Others"],"title":"AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials","authors":"Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, Tao Yu","publication":"ICLR 2025","paperLink":"https://arxiv.org/abs/2412.09605","codeLink":"","dataLink":"","blogLink":"https://agenttrek.github.io/","twitterLink":"","image":"/research/agenttrek.jpg"},{"category":["DigitalAIAgents"],"title":"Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction","authors":"Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, Caiming Xiong","publication":"Preprint","paperLink":"https://arxiv.org/abs/2412.04454","codeLink":"https://github.com/xlang-ai/aguvis","dataLink":"","blogLink":"https://aguvis-project.github.io/","twitterLink":"","image":"/research/aguvis.jpg"},{"category":["DigitalAIAgents"],"title":"Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments","authors":"Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, Sercan Ö. Arık","publication":"ICLR 2025","paperLink":"https://arxiv.org/abs/2501.10893","codeLink":"","dataLink":"","blogLink":"","twitterLink":"","image":"/research/learn_by_interact.png"},{"category":["CodeGeneration","DigitalAIAgents"],"title":"Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows","authors":"Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, Victor Zhong, Caiming Xiong, Ruoxi Sun, Qian Liu, Sida Wang, Tao Yu","publication":"ICLR 2025","paperLink":"https://www.arxiv.org/abs/2411.07763","codeLink":"https://github.com/xlang-ai/Spider2","dataLink":"","blogLink":"https://spider2-sql.github.io/","twitterLink":"","image":"/research/spider2.jpg"},{"category":["DigitalAIAgents"],"title":"Attacking Vision-Language Computer Agents via Pop-ups","authors":"Yanzhe Zhang, Tao Yu, Diyi Yang","publication":"Preprint","paperLink":"https://arxiv.org/abs/2411.02391","codeLink":"https://github.com/SALT-NLP/PopupAttack","dataLink":"","blogLink":"","twitterLink":"","image":"/research/attack.jpg"},{"category":["Others"],"title":"BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval","authors":"Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, Tao Yu","publication":"ICLR 2025","paperLink":"https://arxiv.org/abs/2407.12883","codeLink":"https://github.com/xlang-ai/BRIGHT","dataLink":"","blogLink":"https://brightbenchmark.github.io/","twitterLink":"","image":"/research/bright.jpg"},{"category":["Others"],"title":"Generative Representational Instruction Tuning","authors":"Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela","publication":"ICLR 2025","paperLink":"https://arxiv.org/abs/2402.09906","codeLink":"https://github.com/ContextualAI/gritlm","dataLink":"","blogLink":"","twitterLink":"https://twitter.com/Muennighoff/status/1758307967802224770","image":"/research/grit.jpeg"},{"category":["CodeGeneration","DigitalAIAgents"],"title":"Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?","authors":"Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu","publication":"NeurIPS 2024, Spotlight","paperLink":"https://arxiv.org/abs/2407.10956","codeLink":"https://github.com/xlang-ai/Spider2-V","dataLink":"","blogLink":"https://spider2-v.github.io/","twitterLink":"","image":"/research/spider2v.jpg"},{"category":["CodeGeneration"],"title":"EvoR: Evolving Retrieval for Code Generation","authors":"Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu","publication":"EMNLP Findings 2024","paperLink":"https://arxiv.org/abs/2307.07047","codeLink":"https://github.com/xlang-ai/EVOR","dataLink":"https://huggingface.co/datasets/xlangai/arks_data","blogLink":"https://arks-codegen.github.io/","twitterLink":"https://twitter.com/hongjin_su/status/1759978005525643466","image":"/research/ark.jpeg"},{"category":["DigitalAIAgents"],"title":"OS-Copilot: Towards Generalist Computer Agents with Self-Improvement","authors":"Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong","publication":"Preprint","paperLink":"https://arxiv.org/abs/2402.07456","codeLink":"https://github.com/OS-Copilot/OS-Copilot","dataLink":"","blogLink":"https://os-copilot.github.io/","twitterLink":"https://twitter.com/zywu_hku/status/1758014688779244002","image":"/research/os_copilot.png"},{"category":["DigitalAIAgents"],"title":"OpenAgents: An Open Platform for Language Agents in the Wild","authors":"Tianbao Xie*, Fan Zhou*, Zhoujun Cheng*, Peng Shi*, Luoxuan Weng*, Yitao Liu*, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu","publication":"COLM 2024","paperLink":"https://arxiv.org/abs/2310.10634","codeLink":"https://github.com/xlang-ai/OpenAgents","dataLink":"https://github.com/xlang-ai/OpenAgents","blogLink":"https://www.xlang.ai/blog/xlang-intro","twitterLink":"https://twitter.com/ChengZhoujun/status/1714343204148113860","image":"/research/openagents.png"},{"category":["Others"],"title":"Does Collaborative Human-LM Dialogue Generation Help Information Extraction from Human Dialogues?","authors":"Bo-Ru Lu, Nikita Haduong, Chia-Hsuan Lee, Zeqiu Wu, Hao Cheng, Paul Koester, Jean Utke, Tao Yu, Noah A. Smith, Mari Ostendorf","publication":"COLM 2024","paperLink":"https://arxiv.org/abs/2307.07047","codeLink":"","dataLink":"","blogLink":"","twitterLink":"","image":"/research/does.png"},{"category":["PhysicalAIAgents","DigitalAIAgents","CodeGeneration"],"title":"Lemur: Harmonizing Natural Language and Code for Language Agents","authors":"Yiheng Xu*, Hongjin Su*, Chen Xing*, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu","publication":"ICLR 2024, Spotlight","paperLink":"https://arxiv.org/abs/2310.06830","codeLink":"https://github.com/OpenLemur/lemur","dataLink":"https://huggingface.co/OpenLemur","blogLink":"https://www.xlang.ai/blog/openlemur","twitterLink":"https://twitter.com/yihengxu_/status/1712537543688990940","image":"/research/lemur.png","huggingfaceModel":["OpenLemur/lemur-70b-chat-v1","OpenLemur/lemur-70b-v1"]},{"category":["PhysicalAIAgents"],"title":"Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning\n","authors":"Tianbao Xie*, Siheng Zhao*, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu","publication":"ICLR 2024, Spotlight","paperLink":"https://arxiv.org/abs/2309.11489","codeLink":"https://github.com/xlang-ai/text2reward","dataLink":"https://github.com/xlang-ai/text2reward","blogLink":"https://text-to-reward.github.io/","twitterLink":"https://twitter.com/arankomatsuzaki/status/1706311844829487153","image":"/research/text2reward.png"},{"category":["DigitalAIAgents","Others"],"title":"Instructor Embeddings: One Embedder, Any Task: Instruction-Finetuned Text Embeddings","authors":"Hongjin Su*, Weijia Shi*, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu","publication":"ACL 2023 Findings","paperLink":"https://arxiv.org/abs/2212.09741","codeLink":"https://github.com/HKUNLP/instructor-embedding","dataLink":"https://drive.google.com/file/d/1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52/view","blogLink":"https://instructor-embedding.github.io/","twitterLink":"https://twitter.com/WeijiaShi2/status/1605307966109863936","image":"/research/instructor.jpg","huggingfaceModel":["hkunlp/instructor-large","hkunlp/instructor-base","hkunlp/instructor-xl"]},{"category":["CodeGeneration"],"title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation","authors":"Yuhang Lai*, Chengxi Li*, Yiming Wang*, Tianyi Zhang*, Ruiqi Zhong*, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, Tao Yu","publication":"ICML 2023","paperLink":"https://arxiv.org/abs/2211.11501","codeLink":"https://github.com/HKUNLP/DS-1000","dataLink":"https://github.com/HKUNLP/DS-1000/tree/main/ds1000_example","blogLink":"https://ds1000-code-gen.github.io/","twitterLink":"https://twitter.com/taoyds/status/1595086401309388801","image":"/research/ds-1000.jpg"},{"category":["CodeGeneration"],"title":"Coder Reviewer Reranking for Code Generation","authors":"Tianyi Zhang, Tao Yu, Tatsunori B. Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, Sida I. Wang","publication":"ICML 2023","paperLink":"https://arxiv.org/abs/2211.16490","codeLink":"https://github.com/facebookresearch/coder_reviewer_reranking","dataLink":"","blogLink":"","twitterLink":"","image":"/research/code-review.jpg"},{"category":["Others"],"title":"Compositional Exemplars for In-context Learning","authors":"Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong.","publication":"ICML 2023","paperLink":"https://arxiv.org/abs/2302.05698","codeLink":"https://github.com/HKUNLP/icl-ceil","dataLink":"","blogLink":"","twitterLink":"","image":"/research/ce-icl.png"},{"category":["Others"],"title":"Batch Prompting: Efficient Inference with Large Language Model APIs","authors":"Zhoujun Cheng, Jungo Kasai, Tao Yu","publication":"EMNLP 2023 Industry Track","paperLink":"https://arxiv.org/abs/2301.08721","codeLink":"","dataLink":"","blogLink":"","twitterLink":"","image":"/research/batch.png"},{"category":["CodeGeneration","DigitalAIAgents","Others"],"title":"Binder: Binding Language Models in Symbolic Languages","authors":"Zhoujun Cheng*, Tianbao Xie*, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu","publication":"ICLR 2023, Spotlight","paperLink":"https://arxiv.org/abs/2210.02875","codeLink":"https://github.com/HKUNLP/binder","dataLink":"https://github.com/HKUNLP/Binder/tree/main/datasets","blogLink":"https://lm-code-binder.github.io/","twitterLink":"https://twitter.com/taoyds/status/1580987466974396417","image":"/research/binder.jpg"},{"category":["Others"],"title":"Selective Annotation Makes Language Models Better Few-Shot Learners","authors":"Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu","publication":"ICLR 2023","paperLink":"https://arxiv.org/abs/2209.01975","codeLink":"https://github.com/HKUNLP/icl-selective-annotation","dataLink":"https://github.com/HKUNLP/icl-selective-annotation/tree/main/data","twitterLink":"https://twitter.com/jungokasai/status/1568302230490730497","image":"/research/selective-annotation.jpg"},{"category":["CodeGeneration"],"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models","authors":"Tianbao Xie*, Chen Henry Wu*, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu.","publication":"EMNLP 2022, Oral","paperLink":"https://arxiv.org/abs/2201.05966","codeLink":"https://github.com/HKUNLP/UnifiedSKG","dataLink":"https://unifiedskg.com/benchmarks/","blogLink":"https://unifiedskg.com/","twitterLink":"https://twitter.com/taoyds/status/1487177399875690500","image":"/research/unifiedskg.jpg"},{"category":["Others"],"title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation","authors":"Jiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong","publication":"EMNLP 2022","paperLink":"https://arxiv.org/abs/2202.07922","codeLink":"https://github.com/jiacheng-ye/ZeroGen","dataLink":"","blogLink":"","twitterLink":"","image":"/research/zerogen.jpg"},{"category":["Others"],"title":"In-Context Learning for Few-Shot Dialogue State Tracking","authors":"Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, Mari Ostendorf","publication":"EMNLP Findings 2022","paperLink":"https://arxiv.org/abs/2203.08568","codeLink":"https://github.com/Yushi-Hu/IC-DST","dataLink":"","blogLink":"","twitterLink":"","image":"/research/ic-dst.jpg"},{"category":["CodeGeneration"],"title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task","authors":"Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev","publication":"EMNLP 2018","paperLink":"https://arxiv.org/abs/1809.08887","codeLink":"https://github.com/taoyds/spider","dataLink":"https://drive.google.com/uc?export=download\u0026id=1TqleXec_OykOYFREKKtschzY29dUcVAQ","blogLink":"https://yale-lily.github.io/spider","twitterLink":"","image":"/research/spider.jpg"}],"talks":[{"title":"Building Natural Language Interfaces through Grounding Language Models into Executable Actions","startDate":"2023-04-01","endDate":"2023-05-31","desc":"Columbia NLP seminar, Cornell DB seminar, Microsoft Research Asia","link":""},{"title":"Building Natural Language Interfaces with Large Language Models","startDate":"2022-11-01","endDate":"2022-11-30","desc":"Amazon AWS","link":"https://docs.google.com/presentation/d/1jsfb6foc3XCcfO9mVC4IkCoG9mQ0N3adlISj86Mauig/edit?usp=sharing\u0026resourcekey=0-9oawun4RgUnSh_UWJ64U9g"},{"title":"Few-shot In-context Learning with Large Language Models","startDate":"2022-06-01","endDate":"2022-06-30","desc":"AllState Tech Talks","link":""},{"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models","startDate":"2022-02-01","endDate":"2022-02-28","desc":"ServiceNow Research (Prev. ElementAI)","link":"https://docs.google.com/presentation/d/1A2fSbpHXEVP_NVyeiy8TymHnibb91_OZmuk83qffq-Q/edit?usp=sharing"},{"title":"SCoRe: Pre-Training for Context Representation in Conversational Semantic Parsing","startDate":"2021-04-01","endDate":"2021-04-30","desc":"Google Research","link":"https://docs.google.com/presentation/d/11S7y57PRSlq_ccQxprdtko1MVgQN--L8yH8H9UPfsfE/edit?usp=sharing"},{"title":"Learning to Build Conversational Natural Language Interfaces","startDate":"2021-01-01","endDate":"2021-03-31","desc":"The University of Hong Kong (CS), The National University of Singapore (CS), The University of Wisconsin-Madison (CS), Simon Fraser University (CS), The University of Minnesota Twin Cities (CSE)","link":"https://docs.google.com/presentation/d/1ZyuMKRoXgY9dBmiJeYT8n_3sgFY6aw_EaMbnqZtAt8Y/edit?usp=sharing"},{"title":"SParC: Cross-Domain Semantic Parsing in Context","startDate":"2019-09-01","endDate":"2019-09-30","desc":"Microsoft Research AI Breakthroughs Workshop, Redmond","link":"https://yale-lily.github.io/sparc"}]},"__N_SSG":true},"page":"/publications","query":{},"buildId":"AMbiQiO6HRLFcaOWDE66z","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>