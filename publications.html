<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>XLANG Lab | Research</title><link rel="apple-touch-icon" sizes="180x180" href="/favicon/white-on-green/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/white-on-green/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/white-on-green/favicon-16x16.png"/><link rel="manifest" href="/favicon/white-on-green/site.webmanifest"/><meta name="next-head-count" content="7"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/xlang-website/_next/static/css/9d2dcaadb6682cc7.css" as="style"/><link rel="stylesheet" href="/xlang-website/_next/static/css/9d2dcaadb6682cc7.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/xlang-website/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/xlang-website/_next/static/chunks/webpack-4c0b4781cf71d9f3.js" defer=""></script><script src="/xlang-website/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/xlang-website/_next/static/chunks/main-7831bc1dbacb25ec.js" defer=""></script><script src="/xlang-website/_next/static/chunks/pages/_app-d454913aba06aa6e.js" defer=""></script><script src="/xlang-website/_next/static/chunks/pages/publications-d765043a0cdb3ef4.js" defer=""></script><script src="/xlang-website/_next/static/JN0lV5S2ZcPB9wuKwc4_J/_buildManifest.js" defer=""></script><script src="/xlang-website/_next/static/JN0lV5S2ZcPB9wuKwc4_J/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&display=swap">@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtr6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtZ6Ew9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCu170w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCuM70w9.woff) format('woff')}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WRhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459W1hyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WZhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WdhyyTh89ZNpQ.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Montserrat';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/montserrat/v29/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next"><div class="fixed top-0 left-0 w-full h-14 md:h-20 bg-white py-4 z-10 navbar-shadow"><div class="page-x-width w-full flex justify-between items-center"><div class="sm:hidden w-fit h-fit cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" class="text-[#0156AC]" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><line x1="4" y1="6" x2="20" y2="6"></line><line x1="4" y1="12" x2="20" y2="12"></line><line x1="4" y1="18" x2="20" y2="18"></line></svg></div><a href="/xlang-website"><div class="flex gap-2 items-center cursor-pointer text-brand-dark"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/xlang-website/icons/logo.svg"/><div>XLANG Lab</div></div></a><ul class="gap-8 text-md text-text-brand-dark hidden sm:flex"><li class="font-[500] hover:underline text-brand-dark"><a href="/xlang-website">about</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/xlang-website/team">team</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/xlang-website/publications">publications</a></li><li class="font-[500] hover:underline text-brand-dark"><a href="/xlang-website/blog">blogs</a></li></ul><div class="flex gap-4 items-center"><ul class="hidden lg:flex gap-3"><li><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/xlang-website/icons/github-black.svg"/></a></li><li><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/xlang-website/icons/twitter-black.svg"/></a></li></ul><div class="max-sm:text-sm border border-brand-primary2 border-2 text-brand-primary2 font-[500] rounded-xl py-1 px-3 cursor-pointer"><a href="https://forms.gle/3Ki9ectMB5D31F8g8" target="_blank" rel="noopener noreferrer">join us</a></div></div></div></div><div class="relative w-full h-full"><div class="absolute top-0 left-0 max-sm:hidden -mt-8 z-[-1]"><img loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/xlang-website/background/wave.svg"/><img loading="lazy" width="2000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="/xlang-website/background/wave2.svg"/></div><div class="w-full pt-20 sm:pt-36 pb-10 bg-[#D9D9D9]/20"><div class="page-x-width flex flex-col gap-8 sm:gap-6"><div><h1 class="text-2xl mb-6">Research</h1><p class="leading-7">At the XLANG Lab, our research centers on constructing language model agents that convert language instructions into executable actions within real-world contexts. This encompasses databases (data agent), web applications (plugins/web agent), physical world interactions (robotic agent), and involves techniques like LLM + tool utilization, code generation, semantic parsing, interactive systems, and beyond.</p></div><div><h1 class="text-2xl font-[500] mb-6">Papers</h1><div class="flex gap-3 flex-wrap"><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">code generation and semantic parsing</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">LLM + tool use</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">LLM-powered agents</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">efficient and generalizable LLMs</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">dialog and interactive systems</div><div class="border border-[1.5px] border-[#7A7A7A] text-[#7A7A7A] rounded-lg py-1 px-3 text-xs cursor-pointer">LLM + Robotics</div></div><div class="flex flex-col mt-10"><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/bright.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, Tao Yu</p><p class="italic text-xs font-[500]">Preprint 2024</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2407.12883" target="_blank">paper</a><a href="https://github.com/xlang-ai/BRIGHT" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/spider2-v.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?</h1><p class="text-[#727272] text-xs font-[500]">Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu</p><p class="italic text-xs font-[500]">NeurIPS 2024, Spotlight</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2407.10956" target="_blank">paper</a><a href="https://github.com/xlang-ai/Spider2-V" target="_blank">code</a><a href="https://spider2-v.github.io/" target="_blank">page</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Attacking Vision-Language Computer Agents via Pop-ups" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/attack.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Attacking Vision-Language Computer Agents via Pop-ups</h1><p class="text-[#727272] text-xs font-[500]">Yanzhe Zhang, Tao Yu, Diyi Yang</p><p class="italic text-xs font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2411.02391" target="_blank">paper</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/spider2.0.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows</h1><p class="text-[#727272] text-xs font-[500]">Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, Victor Zhong, Caiming Xiong, Ruoxi Sun, Qian Liu, Sida Wang, Tao Yu</p><p class="italic text-xs font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://www.arxiv.org/abs/2411.07763" target="_blank">paper</a><a href="https://github.com/xlang-ai/Spider2" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/osworld.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu</p><p class="italic text-xs font-[500]">NeurIPS 2024</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2404.07972" target="_blank">paper</a><a href="https://github.com/xlang-ai/OSWorld" target="_blank">code</a><a href="https://twitter.com/TianbaoX/status/1778781521253667267" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="EvoR: Evolving Retrieval for Code Generation" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/ark.jpeg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">EvoR: Evolving Retrieval for Code Generation</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu</p><p class="italic text-xs font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2307.07047" target="_blank">paper</a><a href="https://github.com/xlang-ai/EVOR" target="_blank">code</a><a href="https://arks-codegen.github.io/" target="_blank">page</a><a href="https://twitter.com/hongjin_su/status/1759978005525643466" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Generative Representational Instruction Tuning" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/grit.jpeg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Generative Representational Instruction Tuning</h1><p class="text-[#727272] text-xs font-[500]">Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela</p><p class="italic text-xs font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2402.09906" target="_blank">paper</a><a href="https://github.com/ContextualAI/gritlm" target="_blank">code</a><a href="https://twitter.com/Muennighoff/status/1758307967802224770" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="OS-Copilot: Towards Generalist Computer Agents with Self-Improvement" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/os-copilot.jpeg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">OS-Copilot: Towards Generalist Computer Agents with Self-Improvement</h1><p class="text-[#727272] text-xs font-[500]">Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong</p><p class="italic text-xs font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2402.07456" target="_blank">paper</a><a href="https://github.com/OS-Copilot/OS-Copilot" target="_blank">code</a><a href="https://twitter.com/zywu_hku/status/1758014688779244002" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="OpenAgents: An Open Platform for Language Agents in the Wild" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/openagents.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">OpenAgents: An Open Platform for Language Agents in the Wild</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie*, Fan Zhou*, Zhoujun Cheng*, Peng Shi*, Luoxuan Weng*, Yitao Liu*, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu</p><p class="italic text-xs font-[500]">Preprint</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2310.10634" target="_blank">paper</a><a href="https://github.com/xlang-ai/OpenAgents" target="_blank">code</a><a href="https://www.xlang.ai/blog/xlang-intro" target="_blank">page</a><a href="https://twitter.com/ChengZhoujun/status/1714343204148113860" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Lemur: Harmonizing Natural Language and Code for Language Agents" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/lemur.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Lemur: Harmonizing Natural Language and Code for Language Agents</h1><p class="text-[#727272] text-xs font-[500]">Yiheng Xu*, Hongjin Su*, Chen Xing*, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu</p><p class="italic text-xs font-[500]">ICLR 2024 Spotlight</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2310.06830" target="_blank">paper</a><a href="https://github.com/OpenLemur/lemur" target="_blank">code</a><a href="https://www.xlang.ai/blog/openlemur" target="_blank">page</a><a href="https://twitter.com/yihengxu_/status/1712537543688990940" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning
" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/text2reward.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning
</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie*, Siheng Zhao*, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</p><p class="italic text-xs font-[500]">ICLR 2024 Spotlight</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2309.11489" target="_blank">paper</a><a href="https://github.com/xlang-ai/text2reward" target="_blank">code</a><a href="https://text-to-reward.github.io/" target="_blank">page</a><a href="https://twitter.com/arankomatsuzaki/status/1706311844829487153" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Instructor Embeddings: One Embedder, Any Task: Instruction-Finetuned Text Embeddings" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/instructor.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Instructor Embeddings: One Embedder, Any Task: Instruction-Finetuned Text Embeddings</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su*, Weijia Shi*, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu</p><p class="italic text-xs font-[500]">ACL 2023 Findings</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2212.09741" target="_blank">paper</a><a href="https://github.com/HKUNLP/instructor-embedding" target="_blank">code</a><a href="https://instructor-embedding.github.io/" target="_blank">page</a><a href="https://twitter.com/WeijiaShi2/status/1605307966109863936" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/ds-1000.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation</h1><p class="text-[#727272] text-xs font-[500]">Yuhang Lai*, Chengxi Li*, Yiming Wang*, Tianyi Zhang*, Ruiqi Zhong*, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, Tao Yu</p><p class="italic text-xs font-[500]">ICML 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2211.11501" target="_blank">paper</a><a href="https://github.com/HKUNLP/DS-1000" target="_blank">code</a><a href="https://ds1000-code-gen.github.io/" target="_blank">page</a><a href="https://twitter.com/taoyds/status/1595086401309388801" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Coder Reviewer Reranking for Code Generation" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/code-review.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Coder Reviewer Reranking for Code Generation</h1><p class="text-[#727272] text-xs font-[500]">Tianyi Zhang, Tao Yu, Tatsunori B. Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, Sida I. Wang</p><p class="italic text-xs font-[500]">ICML 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2211.16490" target="_blank">paper</a><a href="https://github.com/facebookresearch/coder_reviewer_reranking" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Compositional Exemplars for In-context Learning" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/ce-icl.png"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Compositional Exemplars for In-context Learning</h1><p class="text-[#727272] text-xs font-[500]">Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong.</p><p class="italic text-xs font-[500]">ICML 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2302.05698" target="_blank">paper</a><a href="https://github.com/HKUNLP/icl-ceil" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Binder: Binding Language Models in Symbolic Languages" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/binder.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Binder: Binding Language Models in Symbolic Languages</h1><p class="text-[#727272] text-xs font-[500]">Zhoujun Cheng*, Tianbao Xie*, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu</p><p class="italic text-xs font-[500]">ICLR 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2210.02875" target="_blank">paper</a><a href="https://github.com/HKUNLP/binder" target="_blank">code</a><a href="https://lm-code-binder.github.io/" target="_blank">page</a><a href="https://twitter.com/taoyds/status/1580987466974396417" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Selective Annotation Makes Language Models Better Few-Shot Learners" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/selective-annotation.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Selective Annotation Makes Language Models Better Few-Shot Learners</h1><p class="text-[#727272] text-xs font-[500]">Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu</p><p class="italic text-xs font-[500]">ICLR 2023</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2209.01975" target="_blank">paper</a><a href="https://github.com/HKUNLP/icl-selective-annotation" target="_blank">code</a><a href="https://twitter.com/jungokasai/status/1568302230490730497" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/unifiedskg.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</h1><p class="text-[#727272] text-xs font-[500]">Tianbao Xie*, Chen Henry Wu*, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu.</p><p class="italic text-xs font-[500]">EMNLP 2022</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2201.05966" target="_blank">paper</a><a href="https://github.com/HKUNLP/UnifiedSKG" target="_blank">code</a><a href="https://unifiedskg.com/" target="_blank">page</a><a href="https://twitter.com/taoyds/status/1487177399875690500" target="_blank">twitter</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="ZeroGen: Efficient Zero-shot Learning via Dataset Generation" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/zerogen.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">ZeroGen: Efficient Zero-shot Learning via Dataset Generation</h1><p class="text-[#727272] text-xs font-[500]">Jiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong</p><p class="italic text-xs font-[500]">EMNLP 2022</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2202.07922" target="_blank">paper</a><a href="https://github.com/jiacheng-ye/ZeroGen" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="In-Context Learning for Few-Shot Dialogue State Tracking" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/ic-dst.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">In-Context Learning for Few-Shot Dialogue State Tracking</h1><p class="text-[#727272] text-xs font-[500]">Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, Mari Ostendorf</p><p class="italic text-xs font-[500]">EMNLP Findings 2022</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2203.08568" target="_blank">paper</a><a href="https://github.com/Yushi-Hu/IC-DST" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/nl2interface.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries</h1><p class="text-[#727272] text-xs font-[500]">Yiru Chen, Ryan Li, Austin Mac, Tianbao Xie, Tao Yu, Eugene Wu</p><p class="italic text-xs font-[500]">IEEE Visualization Conference NLVIZ Workshop 2022</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2209.08834" target="_blank">paper</a><a href="https://github.com/learnedinterfaces/PI2" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/grappa.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing</h1><p class="text-[#727272] text-xs font-[500]">Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Richard Socher, Caiming Xiong</p><p class="italic text-xs font-[500]">ICLR 2021</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2009.13845" target="_blank">paper</a><a href="https://github.com/taoyds/grappa" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Semantic Evaluation for Text-to-SQL with Distilled Test Suites" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/test-suite.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Semantic Evaluation for Text-to-SQL with Distilled Test Suites</h1><p class="text-[#727272] text-xs font-[500]">Ruiqi Zhong, Tao Yu, Dan Klein</p><p class="italic text-xs font-[500]">EMNLP 2020</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/2010.02840" target="_blank">paper</a><a href="https://github.com/taoyds/test-suite-sql-eval" target="_blank">code</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/cosql.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases</h1><p class="text-[#727272] text-xs font-[500]">Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, Dragomir Radev</p><p class="italic text-xs font-[500]">EMNLP 2019</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/1909.05378" target="_blank">paper</a><a href="https://github.com/taoyds/cosql" target="_blank">code</a><a href="https://yale-lily.github.io/cosql" target="_blank">page</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="SParC: Cross-Domain Semantic Parsing in Context" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/sparc.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">SParC: Cross-Domain Semantic Parsing in Context</h1><p class="text-[#727272] text-xs font-[500]">Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, Dragomir Radev</p><p class="italic text-xs font-[500]">ACL 2018</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/1906.02285" target="_blank">paper</a><a href="https://github.com/taoyds/sparc" target="_blank">code</a><a href="https://yale-lily.github.io/sparc" target="_blank">page</a></div></div></div></div><div class="border-t border-b border-black/30 py-6"><div class="sm:flex gap-4"><div class="relative min-w-[180px] max-sm:h-48 h-32 rounded-lg overflow-hidden my-auto shadow-xl"><img alt="Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:center;color:transparent" src="/xlang-website/research/spider.jpg"/></div><div class="flex flex-col w-full"><h1 class="text-lg font-[500]">Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task</h1><p class="text-[#727272] text-xs font-[500]">Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev</p><p class="italic text-xs font-[500]">EMNLP 2018</p><div class="flex justify-end items-center w-full gap-3 font-[500] text-xs"><a href="https://arxiv.org/abs/1809.08887" target="_blank">paper</a><a href="https://github.com/taoyds/spider" target="_blank">code</a><a href="https://yale-lily.github.io/spider" target="_blank">page</a></div></div></div></div></div></div></div></div></div><div class="w-full bg-brand-offBlack p-4"><div class="page-x-width flex justify-center sm:justify-between flex-wrap gap-4"><div class="flex gap-2 items-center"><a href="/xlang-website"><div class="relative"><img alt="Xlang" loading="lazy" width="30" height="30" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/xlang-website/icons/logo-white.svg"/></div></a><div class="text-white text-xs w-fit">Â© Copyright 2023 XLANG Lab. All right reserved.</div></div><nav><ul class="text-white flex gap-6"><li class="cursor-pointer"><a href="https://discord.com/invite/4Gnw7eTEZR"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/xlang-website/icons/discord.svg"/></a></li><li class="cursor-pointer"><a href="https://github.com/xlang-ai"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/xlang-website/icons/github.svg"/></a></li><li class="cursor-pointer"><a href="https://twitter.com/XLangNLP"><img alt="Xlang" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-md" style="color:transparent" src="/xlang-website/icons/twitter.svg"/></a></li></ul></nav></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"papers":[{"category":["Grounding"],"title":"BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval","authors":"Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, Tao Yu","publication":"Preprint 2024","paperLink":"https://arxiv.org/abs/2407.12883","codeLink":"https://github.com/xlang-ai/BRIGHT","dataLink":"","blogLink":"","twitterLink":"","image":"/research/bright.jpg"},{"category":["Grounding"],"title":"Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?","authors":"Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu","publication":"NeurIPS 2024, Spotlight","paperLink":"https://arxiv.org/abs/2407.10956","codeLink":"https://github.com/xlang-ai/Spider2-V","dataLink":"","blogLink":"https://spider2-v.github.io/","twitterLink":"","image":"/research/spider2-v.jpg"},{"category":[],"title":"Attacking Vision-Language Computer Agents via Pop-ups","authors":"Yanzhe Zhang, Tao Yu, Diyi Yang","publication":"Preprint","paperLink":"https://arxiv.org/abs/2411.02391","codeLink":"","dataLink":"","blogLink":"","twitterLink":"","image":"/research/attack.jpg"},{"category":["Grounding"],"title":"Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows","authors":"Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, Victor Zhong, Caiming Xiong, Ruoxi Sun, Qian Liu, Sida Wang, Tao Yu","publication":"Preprint","paperLink":"https://www.arxiv.org/abs/2411.07763","codeLink":"https://github.com/xlang-ai/Spider2","dataLink":"","blogLink":"","twitterLink":"","image":"/research/spider2.0.jpg"},{"category":["PoweredAgents","InteractiveSystems","ToolUse"],"title":"OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments","authors":"Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu","publication":"NeurIPS 2024","paperLink":"https://arxiv.org/abs/2404.07972","codeLink":"https://github.com/xlang-ai/OSWorld","dataLink":"https://github.com/xlang-ai/OSWorld/tree/main/evaluation_examples","blogLink":"","twitterLink":"https://twitter.com/TianbaoX/status/1778781521253667267","image":"/research/osworld.png"},{"category":["Grounding","EfficientLLMs"],"title":"EvoR: Evolving Retrieval for Code Generation","authors":"Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu","publication":"Preprint","paperLink":"https://arxiv.org/abs/2307.07047","codeLink":"https://github.com/xlang-ai/EVOR","dataLink":"https://huggingface.co/datasets/xlangai/arks_data","blogLink":"https://arks-codegen.github.io/","twitterLink":"https://twitter.com/hongjin_su/status/1759978005525643466","image":"/research/ark.jpeg"},{"category":["EfficientLLMs"],"title":"Generative Representational Instruction Tuning","authors":"Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela","publication":"Preprint","paperLink":"https://arxiv.org/abs/2402.09906","codeLink":"https://github.com/ContextualAI/gritlm","dataLink":"","blogLink":"","twitterLink":"https://twitter.com/Muennighoff/status/1758307967802224770","image":"/research/grit.jpeg"},{"category":["PoweredAgents","InteractiveSystems","ToolUse"],"title":"OS-Copilot: Towards Generalist Computer Agents with Self-Improvement","authors":"Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong","publication":"Preprint","paperLink":"https://arxiv.org/abs/2402.07456","codeLink":"https://github.com/OS-Copilot/OS-Copilot","dataLink":"","blogLink":"","twitterLink":"https://twitter.com/zywu_hku/status/1758014688779244002","image":"/research/os-copilot.jpeg"},{"category":["PoweredAgents","InteractiveSystems"],"title":"OpenAgents: An Open Platform for Language Agents in the Wild","authors":"Tianbao Xie*, Fan Zhou*, Zhoujun Cheng*, Peng Shi*, Luoxuan Weng*, Yitao Liu*, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu","publication":"Preprint","paperLink":"https://arxiv.org/abs/2310.10634","codeLink":"https://github.com/xlang-ai/OpenAgents","dataLink":"https://github.com/xlang-ai/OpenAgents","blogLink":"https://www.xlang.ai/blog/xlang-intro","twitterLink":"https://twitter.com/ChengZhoujun/status/1714343204148113860","image":"/research/openagents.png"},{"category":["PoweredAgents","Grounding","ToolUse"],"title":"Lemur: Harmonizing Natural Language and Code for Language Agents","authors":"Yiheng Xu*, Hongjin Su*, Chen Xing*, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu","publication":"ICLR 2024 Spotlight","paperLink":"https://arxiv.org/abs/2310.06830","codeLink":"https://github.com/OpenLemur/lemur","dataLink":"https://huggingface.co/OpenLemur","blogLink":"https://www.xlang.ai/blog/openlemur","twitterLink":"https://twitter.com/yihengxu_/status/1712537543688990940","image":"/research/lemur.png","huggingfaceModel":["OpenLemur/lemur-70b-chat-v1","OpenLemur/lemur-70b-v1"]},{"category":["Grounding","Robotics"],"title":"Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning\n","authors":"Tianbao Xie*, Siheng Zhao*, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu","publication":"ICLR 2024 Spotlight","paperLink":"https://arxiv.org/abs/2309.11489","codeLink":"https://github.com/xlang-ai/text2reward","dataLink":"https://github.com/xlang-ai/text2reward","blogLink":"https://text-to-reward.github.io/","twitterLink":"https://twitter.com/arankomatsuzaki/status/1706311844829487153","image":"/research/text2reward.png"},{"category":["EfficientLLMs"],"title":"Instructor Embeddings: One Embedder, Any Task: Instruction-Finetuned Text Embeddings","authors":"Hongjin Su*, Weijia Shi*, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu","publication":"ACL 2023 Findings","paperLink":"https://arxiv.org/abs/2212.09741","codeLink":"https://github.com/HKUNLP/instructor-embedding","dataLink":"https://drive.google.com/file/d/1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52/view","blogLink":"https://instructor-embedding.github.io/","twitterLink":"https://twitter.com/WeijiaShi2/status/1605307966109863936","image":"/research/instructor.jpg","huggingfaceModel":["hkunlp/instructor-large","hkunlp/instructor-base","hkunlp/instructor-xl"]},{"category":["Grounding"],"title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation","authors":"Yuhang Lai*, Chengxi Li*, Yiming Wang*, Tianyi Zhang*, Ruiqi Zhong*, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, Tao Yu","publication":"ICML 2023","paperLink":"https://arxiv.org/abs/2211.11501","codeLink":"https://github.com/HKUNLP/DS-1000","dataLink":"https://github.com/HKUNLP/DS-1000/tree/main/ds1000_example","blogLink":"https://ds1000-code-gen.github.io/","twitterLink":"https://twitter.com/taoyds/status/1595086401309388801","image":"/research/ds-1000.jpg"},{"category":["Grounding"],"title":"Coder Reviewer Reranking for Code Generation","authors":"Tianyi Zhang, Tao Yu, Tatsunori B. Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, Sida I. Wang","publication":"ICML 2023","paperLink":"https://arxiv.org/abs/2211.16490","codeLink":"https://github.com/facebookresearch/coder_reviewer_reranking","dataLink":"","blogLink":"","twitterLink":"","image":"/research/code-review.jpg"},{"category":["EfficientLLMs"],"title":"Compositional Exemplars for In-context Learning","authors":"Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong.","publication":"ICML 2023","paperLink":"https://arxiv.org/abs/2302.05698","codeLink":"https://github.com/HKUNLP/icl-ceil","dataLink":"","blogLink":"","twitterLink":"","image":"/research/ce-icl.png"},{"category":["Grounding","ToolUse"],"title":"Binder: Binding Language Models in Symbolic Languages","authors":"Zhoujun Cheng*, Tianbao Xie*, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu","publication":"ICLR 2023","paperLink":"https://arxiv.org/abs/2210.02875","codeLink":"https://github.com/HKUNLP/binder","dataLink":"https://github.com/HKUNLP/Binder/tree/main/datasets","blogLink":"https://lm-code-binder.github.io/","twitterLink":"https://twitter.com/taoyds/status/1580987466974396417","image":"/research/binder.jpg"},{"category":["EfficientLLMs"],"title":"Selective Annotation Makes Language Models Better Few-Shot Learners","authors":"Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu","publication":"ICLR 2023","paperLink":"https://arxiv.org/abs/2209.01975","codeLink":"https://github.com/HKUNLP/icl-selective-annotation","dataLink":"https://github.com/HKUNLP/icl-selective-annotation/tree/main/data","twitterLink":"https://twitter.com/jungokasai/status/1568302230490730497","image":"/research/selective-annotation.jpg"},{"category":["Grounding","EfficientLLMs"],"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models","authors":"Tianbao Xie*, Chen Henry Wu*, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu.","publication":"EMNLP 2022","paperLink":"https://arxiv.org/abs/2201.05966","codeLink":"https://github.com/HKUNLP/UnifiedSKG","dataLink":"https://unifiedskg.com/benchmarks/","blogLink":"https://unifiedskg.com/","twitterLink":"https://twitter.com/taoyds/status/1487177399875690500","image":"/research/unifiedskg.jpg"},{"category":["EfficientLLMs"],"title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation","authors":"Jiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong","publication":"EMNLP 2022","paperLink":"https://arxiv.org/abs/2202.07922","codeLink":"https://github.com/jiacheng-ye/ZeroGen","dataLink":"","blogLink":"","twitterLink":"","image":"/research/zerogen.jpg"},{"category":["EfficientLLMs","InteractiveSystems"],"title":"In-Context Learning for Few-Shot Dialogue State Tracking","authors":"Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, Mari Ostendorf","publication":"EMNLP Findings 2022","paperLink":"https://arxiv.org/abs/2203.08568","codeLink":"https://github.com/Yushi-Hu/IC-DST","dataLink":"","blogLink":"","twitterLink":"","image":"/research/ic-dst.jpg"},{"category":["Grounding","InteractiveSystems"],"title":"NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries","authors":"Yiru Chen, Ryan Li, Austin Mac, Tianbao Xie, Tao Yu, Eugene Wu","publication":"IEEE Visualization Conference NLVIZ Workshop 2022","paperLink":"https://arxiv.org/abs/2209.08834","codeLink":"https://github.com/learnedinterfaces/PI2","dataLink":"","blogLink":"","twitterLink":"","image":"/research/nl2interface.jpg"},{"category":["Grounding"],"title":"GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing","authors":"Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Richard Socher, Caiming Xiong","publication":"ICLR 2021","paperLink":"https://arxiv.org/abs/2009.13845","codeLink":"https://github.com/taoyds/grappa","dataLink":"","blogLink":"","twitterLink":"","image":"/research/grappa.jpg"},{"category":["Grounding"],"title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suites","authors":"Ruiqi Zhong, Tao Yu, Dan Klein","publication":"EMNLP 2020","paperLink":"https://arxiv.org/abs/2010.02840","codeLink":"https://github.com/taoyds/test-suite-sql-eval","dataLink":"","blogLink":"","twitterLink":"","image":"/research/test-suite.jpg"},{"category":["Grounding","InteractiveSystems"],"title":"CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases","authors":"Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, Dragomir Radev","publication":"EMNLP 2019","paperLink":"https://arxiv.org/abs/1909.05378","codeLink":"https://github.com/taoyds/cosql","dataLink":"https://drive.google.com/uc?export=download\u0026id=1Y3ydpFiQQ3FC0bzdfy3groV95O_f1nXF","blogLink":"https://yale-lily.github.io/cosql","twitterLink":"","image":"/research/cosql.jpg"},{"category":["Grounding"],"title":"SParC: Cross-Domain Semantic Parsing in Context","authors":"Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, Dragomir Radev","publication":"ACL 2018","paperLink":"https://arxiv.org/abs/1906.02285","codeLink":"https://github.com/taoyds/sparc","dataLink":"https://drive.google.com/uc?export=download\u0026id=1Uu7NMHTR1tdQw1t7bAuM7OPU4LElVKfg","blogLink":"https://yale-lily.github.io/sparc","twitterLink":"","image":"/research/sparc.jpg"},{"category":["Grounding"],"title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task","authors":"Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev","publication":"EMNLP 2018","paperLink":"https://arxiv.org/abs/1809.08887","codeLink":"https://github.com/taoyds/spider","dataLink":"https://drive.google.com/uc?export=download\u0026id=1TqleXec_OykOYFREKKtschzY29dUcVAQ","blogLink":"https://yale-lily.github.io/spider","twitterLink":"","image":"/research/spider.jpg"}],"talks":[{"title":"Building Natural Language Interfaces through Grounding Language Models into Executable Actions","startDate":"2023-04-01","endDate":"2023-05-31","desc":"Columbia NLP seminar, Cornell DB seminar, Microsoft Research Asia","link":""},{"title":"Building Natural Language Interfaces with Large Language Models","startDate":"2022-11-01","endDate":"2022-11-30","desc":"Amazon AWS","link":"https://docs.google.com/presentation/d/1jsfb6foc3XCcfO9mVC4IkCoG9mQ0N3adlISj86Mauig/edit?usp=sharing\u0026resourcekey=0-9oawun4RgUnSh_UWJ64U9g"},{"title":"Few-shot In-context Learning with Large Language Models","startDate":"2022-06-01","endDate":"2022-06-30","desc":"AllState Tech Talks","link":""},{"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models","startDate":"2022-02-01","endDate":"2022-02-28","desc":"ServiceNow Research (Prev. ElementAI)","link":"https://docs.google.com/presentation/d/1A2fSbpHXEVP_NVyeiy8TymHnibb91_OZmuk83qffq-Q/edit?usp=sharing"},{"title":"SCoRe: Pre-Training for Context Representation in Conversational Semantic Parsing","startDate":"2021-04-01","endDate":"2021-04-30","desc":"Google Research","link":"https://docs.google.com/presentation/d/11S7y57PRSlq_ccQxprdtko1MVgQN--L8yH8H9UPfsfE/edit?usp=sharing"},{"title":"Learning to Build Conversational Natural Language Interfaces","startDate":"2021-01-01","endDate":"2021-03-31","desc":"The University of Hong Kong (CS), The National University of Singapore (CS), The University of Wisconsin-Madison (CS), Simon Fraser University (CS), The University of Minnesota Twin Cities (CSE)","link":"https://docs.google.com/presentation/d/1ZyuMKRoXgY9dBmiJeYT8n_3sgFY6aw_EaMbnqZtAt8Y/edit?usp=sharing"},{"title":"SParC: Cross-Domain Semantic Parsing in Context","startDate":"2019-09-01","endDate":"2019-09-30","desc":"Microsoft Research AI Breakthroughs Workshop, Redmond","link":"https://yale-lily.github.io/sparc"}]},"__N_SSG":true},"page":"/publications","query":{},"buildId":"JN0lV5S2ZcPB9wuKwc4_J","assetPrefix":"/xlang-website","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>